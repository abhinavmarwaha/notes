{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":"python","separator":"[\\s\\-\\.]+"},"docs":[{"location":"index.html","text":"Abhinav's Notes \u00b6 My Second Brain.","title":"Home"},{"location":"index.html#abhinavs-notes","text":"My Second Brain.","title":"Abhinav's Notes"},{"location":"android.html","text":"Magisk Platform tools ndk Adreno updated drivers Soundfx remover Audio modification lib Busybox Detach Hidemyapplist Props config Shamiko Sound enhancement xperia Hosts Twrp retention Gms doze Xmlpack Storage isolation Lsposed Devoptshide Gravitybox Pixelify photos Xprivacylua Apps Afwall+ Sdmaid Frenko kernel manager Accubattery Termux Userland Vmos Fde.ai Naptime Network utils Pcapdroid Device info hw Opengl extensions viewer Adaway Warden Virustotal Theming Kwgt","title":"Android"},{"location":"emacs.html","text":"Emacs \u00b6 C-x -> Ctrl + X M-x -> Alt + X ( modifier keys ) C-x, C-g -> close session C-g -> close command C-v -> scroll downwrd M-v -> scroll upwrd C-l -> move screen around cursor Previous line, C-p : : Backward, C-b ..... cursor .... Forward, C-f : : Next line, C-n Just replace c with M for word C-a -> start of line C-e -> end of line M-a -> start of sentence M-e -> end of sentence M + < (shift + ,) -> start of file M + > (shift + .) -> start of file C-u -> add numeric argument M-89 -> add numeric argument Windows \u00b6 (screen is divied into windows) C-x 1 -> delete other windows Editing \u00b6 is electric C-d -> killed forward character M-d -> word C-k -> from curor to end killed M-k -> killed line C- -> highlight C-w -> killed highlighted kill -> can be inserted (yanking) delete -> can not be inserted but can be undoed C-y -> yanking M-y -> change yanked C-/ -> undo Files \u00b6 visiting file -> finding it. C-x C-f -> wisit file C-x C-s -> save file Buffers \u00b6 Bottom line -> mini buffer C-x C-b -> list buffers C-x b -> type buffer name to visit C-x s -> save some buffers EXtend commands \u00b6 C-x M-x -> name of command C-z -> exit emacs temporarily enter -> %emacs% or fg M-x recover-this-file to get autosaved file M-x fundamental-mode -> switch to major mode Tab \u00b6 to complete command Mode Line \u00b6 C-h m -> documentation of current mode Searching \u00b6 C-s -> forward C-r -> reverse -> move backwards C-s -> terminate search Windows \u00b6 C-x 2 -> window split C-M-v -> scroll the bottom window C-x o -> scroll to other window Frames \u00b6 collection of windows, menu, scroll bar etc C-x 5 2 -> new frame C-x 5 0 -> removes selected frame Recursive editing level \u00b6 [(fundamental)] -> exit Getting help \u00b6 C-h ? F1 M-x help C-h c -> show help about command C-h k -> show more help about command C-h f C-h v C-h a C-h i Dired \u00b6 Completion \u00b6 Org-mode \u00b6","title":"Emacs"},{"location":"emacs.html#emacs","text":"C-x -> Ctrl + X M-x -> Alt + X ( modifier keys ) C-x, C-g -> close session C-g -> close command C-v -> scroll downwrd M-v -> scroll upwrd C-l -> move screen around cursor Previous line, C-p : : Backward, C-b ..... cursor .... Forward, C-f : : Next line, C-n Just replace c with M for word C-a -> start of line C-e -> end of line M-a -> start of sentence M-e -> end of sentence M + < (shift + ,) -> start of file M + > (shift + .) -> start of file C-u -> add numeric argument M-89 -> add numeric argument","title":"Emacs"},{"location":"emacs.html#windows","text":"(screen is divied into windows) C-x 1 -> delete other windows","title":"Windows"},{"location":"emacs.html#editing","text":"is electric C-d -> killed forward character M-d -> word C-k -> from curor to end killed M-k -> killed line C- -> highlight C-w -> killed highlighted kill -> can be inserted (yanking) delete -> can not be inserted but can be undoed C-y -> yanking M-y -> change yanked C-/ -> undo","title":"Editing"},{"location":"emacs.html#files","text":"visiting file -> finding it. C-x C-f -> wisit file C-x C-s -> save file","title":"Files"},{"location":"emacs.html#buffers","text":"Bottom line -> mini buffer C-x C-b -> list buffers C-x b -> type buffer name to visit C-x s -> save some buffers","title":"Buffers"},{"location":"emacs.html#extend-commands","text":"C-x M-x -> name of command C-z -> exit emacs temporarily enter -> %emacs% or fg M-x recover-this-file to get autosaved file M-x fundamental-mode -> switch to major mode","title":"EXtend commands"},{"location":"emacs.html#tab","text":"to complete command","title":"Tab"},{"location":"emacs.html#mode-line","text":"C-h m -> documentation of current mode","title":"Mode Line"},{"location":"emacs.html#searching","text":"C-s -> forward C-r -> reverse -> move backwards C-s -> terminate search","title":"Searching"},{"location":"emacs.html#windows_1","text":"C-x 2 -> window split C-M-v -> scroll the bottom window C-x o -> scroll to other window","title":"Windows"},{"location":"emacs.html#frames","text":"collection of windows, menu, scroll bar etc C-x 5 2 -> new frame C-x 5 0 -> removes selected frame","title":"Frames"},{"location":"emacs.html#recursive-editing-level","text":"[(fundamental)] -> exit","title":"Recursive editing level"},{"location":"emacs.html#getting-help","text":"C-h ? F1 M-x help C-h c -> show help about command C-h k -> show more help about command C-h f C-h v C-h a C-h i","title":"Getting help"},{"location":"emacs.html#dired","text":"","title":"Dired"},{"location":"emacs.html#completion","text":"","title":"Completion"},{"location":"emacs.html#org-mode","text":"","title":"Org-mode"},{"location":"google-fu.html","text":"Google Fu \u00b6 Techniques \u00b6 social media -> @twitter price -> $400 hashtags -> #throwbackthursday Exclude words -> jaguar speed -car exact match -> \"tallest building\". range of numbers -> camera $50..$100. Combine searches -> marathon OR race. Operators \u00b6 operator:search_term intitle allintitle inurl allinurl filetype site link inanchor cache info related phonebook rphonebook bphonebook author group msgid insubject stocks define Filters \u00b6 Publish date Verbatim: Search for exact words or phrases. Dictionary: Find definitions, synonyms, or images. Personal Nearby Recipes Applications Patents Images \u00b6 Size Color Type Time Usage rights Videos \u00b6 Duration Time Quality Closed captioned Source Places \u00b6 Your past visits Rating Cuisine Price Hours Books \u00b6 Search: limited preview or full view, Google eBooks only Content Language Title Author Publisher Subject Publication date ISBN ISSN Further Readings \u00b6 google-dorks","title":"Google Fu"},{"location":"google-fu.html#google-fu","text":"","title":"Google Fu"},{"location":"google-fu.html#techniques","text":"social media -> @twitter price -> $400 hashtags -> #throwbackthursday Exclude words -> jaguar speed -car exact match -> \"tallest building\". range of numbers -> camera $50..$100. Combine searches -> marathon OR race.","title":"Techniques"},{"location":"google-fu.html#operators","text":"operator:search_term intitle allintitle inurl allinurl filetype site link inanchor cache info related phonebook rphonebook bphonebook author group msgid insubject stocks define","title":"Operators"},{"location":"google-fu.html#filters","text":"Publish date Verbatim: Search for exact words or phrases. Dictionary: Find definitions, synonyms, or images. Personal Nearby Recipes Applications Patents","title":"Filters"},{"location":"google-fu.html#images","text":"Size Color Type Time Usage rights","title":"Images"},{"location":"google-fu.html#videos","text":"Duration Time Quality Closed captioned Source","title":"Videos"},{"location":"google-fu.html#places","text":"Your past visits Rating Cuisine Price Hours","title":"Places"},{"location":"google-fu.html#books","text":"Search: limited preview or full view, Google eBooks only Content Language Title Author Publisher Subject Publication date ISBN ISSN","title":"Books"},{"location":"google-fu.html#further-readings","text":"google-dorks","title":"Further Readings"},{"location":"no-code.html","text":"No Code / Low Code \u00b6 App Building \u00b6 ReTool Honeycode Appsheet Webflow Elementor Debuild Front-page / docs \u00b6 Notion Carrd Automations \u00b6 Airtable Zapier","title":"No Code / Low Code"},{"location":"no-code.html#no-code-low-code","text":"","title":"No Code / Low Code"},{"location":"no-code.html#app-building","text":"ReTool Honeycode Appsheet Webflow Elementor Debuild","title":"App Building"},{"location":"no-code.html#front-page-docs","text":"Notion Carrd","title":"Front-page / docs"},{"location":"no-code.html#automations","text":"Airtable Zapier","title":"Automations"},{"location":"setup.html","text":"","title":"Setup"},{"location":"web.html","text":"Web \u00b6 Internet \u00b6 Enables inter-computer connectivity. WWW \u00b6 app over internet to share information. Web 1.0 \u00b6 Read-only web Static Search information and read it Goal: To present products to potential customer (brochure) Web 2.0 \u00b6 Interactive Dynamic Read-write-publish Twitter, Facebook, YouTube Web 3.0 \u00b6 Intelligent filtering, Recommendation systems, Artificial intelligence, auto-tagging, etc Blockchain","title":"Web"},{"location":"web.html#web","text":"","title":"Web"},{"location":"web.html#internet","text":"Enables inter-computer connectivity.","title":"Internet"},{"location":"web.html#www","text":"app over internet to share information.","title":"WWW"},{"location":"web.html#web-10","text":"Read-only web Static Search information and read it Goal: To present products to potential customer (brochure)","title":"Web 1.0"},{"location":"web.html#web-20","text":"Interactive Dynamic Read-write-publish Twitter, Facebook, YouTube","title":"Web 2.0"},{"location":"web.html#web-30","text":"Intelligent filtering, Recommendation systems, Artificial intelligence, auto-tagging, etc Blockchain","title":"Web 3.0"},{"location":"ai/agents.html","text":"Agents \u00b6 perceiving its environment through sensors acting upon that environment through actuators = architecture + program Agent Function \u00b6 [f: P* -> A] Rationality \u00b6 expected to maximize its performance measure. Rationality is distinct from omniscience (all-knowing with infinite knowledge) Types \u00b6 Table Driven \u00b6 function TABLE-DRIVEN-AGENT(percept) returns an action persistent: perepts: a sequence, initially empty table: a table of actions, indexed by percept sequences, initially fully specified append perept to the end of percepts action-LoOKUP(percepts, table) Simple reflex agents \u00b6 function SIMPLE-REFLEX-AGENT(Pereept) returns an action persistent: rules: a set of condition-action rules state<-INTERPRET-INPUT(percept) rule<-RULE-MATCH(state, rules) action<-rule.ACTION return action Model-based reflex agents \u00b6 function MoDEL-BASED-REFLEX-AGENT(percept) returns an action persistent: state: the agent's current conception of the world state model: a description of how the next state depends on current state and action rules: a set of condition-action rules action: the most recent action, initially none state<-UPDATE-STATE(state, aetion, pereept, model) rule<-RULE-MATCH(state, rules) action<-rule.ACTION return action Goal-based \u00b6 Knowing about the current state of the environment is not always enough to decide what to do (e.g. decision at a road junction) The agent needs some sort of goal information that describes situations that are desirable The agent program can combine this with information about the results of possible actions in order to choose actions that achieve the goal Usually requires search and planning Vs Reflex Although goal-based agents appears less efficient, it is more flexible because the knowledge that supports its decision is represented explicitly and can be modified On the other hand, for the reflex-agent, we would have to rewrite many condition-action rules The goal based agent's behavior can easily be changed The reflex agent's rules must be changed for a new situation Utility-based \u00b6 Goals alone are not really enough to generate high quality behavior in most environments \u2013 they just provide a binary distinction between happy and unhappy states A more general performance measure should allow a comparison of different world states according to exactly how happy they would make the agent if they could be achieved Happy \u2013 Utility (the quality of being useful) A utility function maps a state onto a real number which describes the associated degree of happiness Learning agents \u00b6 Turing \u2013 instead of actually programming intelligent machines by hand, which is too much work, build learning machines and then teach them Learning also allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow Learning element \u2013 responsible for making improvements Performance element \u2013 responsible for selecting external actions. Learning element uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future Problem generator is responsible for suggesting actions that will lead to a new and informative experiences.","title":"Agents"},{"location":"ai/agents.html#agents","text":"perceiving its environment through sensors acting upon that environment through actuators = architecture + program","title":"Agents"},{"location":"ai/agents.html#agent-function","text":"[f: P* -> A]","title":"Agent Function"},{"location":"ai/agents.html#rationality","text":"expected to maximize its performance measure. Rationality is distinct from omniscience (all-knowing with infinite knowledge)","title":"Rationality"},{"location":"ai/agents.html#types","text":"","title":"Types"},{"location":"ai/agents.html#table-driven","text":"function TABLE-DRIVEN-AGENT(percept) returns an action persistent: perepts: a sequence, initially empty table: a table of actions, indexed by percept sequences, initially fully specified append perept to the end of percepts action-LoOKUP(percepts, table)","title":"Table Driven"},{"location":"ai/agents.html#simple-reflex-agents","text":"function SIMPLE-REFLEX-AGENT(Pereept) returns an action persistent: rules: a set of condition-action rules state<-INTERPRET-INPUT(percept) rule<-RULE-MATCH(state, rules) action<-rule.ACTION return action","title":"Simple reflex agents"},{"location":"ai/agents.html#model-based-reflex-agents","text":"function MoDEL-BASED-REFLEX-AGENT(percept) returns an action persistent: state: the agent's current conception of the world state model: a description of how the next state depends on current state and action rules: a set of condition-action rules action: the most recent action, initially none state<-UPDATE-STATE(state, aetion, pereept, model) rule<-RULE-MATCH(state, rules) action<-rule.ACTION return action","title":"Model-based reflex agents"},{"location":"ai/agents.html#goal-based","text":"Knowing about the current state of the environment is not always enough to decide what to do (e.g. decision at a road junction) The agent needs some sort of goal information that describes situations that are desirable The agent program can combine this with information about the results of possible actions in order to choose actions that achieve the goal Usually requires search and planning Vs Reflex Although goal-based agents appears less efficient, it is more flexible because the knowledge that supports its decision is represented explicitly and can be modified On the other hand, for the reflex-agent, we would have to rewrite many condition-action rules The goal based agent's behavior can easily be changed The reflex agent's rules must be changed for a new situation","title":"Goal-based"},{"location":"ai/agents.html#utility-based","text":"Goals alone are not really enough to generate high quality behavior in most environments \u2013 they just provide a binary distinction between happy and unhappy states A more general performance measure should allow a comparison of different world states according to exactly how happy they would make the agent if they could be achieved Happy \u2013 Utility (the quality of being useful) A utility function maps a state onto a real number which describes the associated degree of happiness","title":"Utility-based"},{"location":"ai/agents.html#learning-agents","text":"Turing \u2013 instead of actually programming intelligent machines by hand, which is too much work, build learning machines and then teach them Learning also allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow Learning element \u2013 responsible for making improvements Performance element \u2013 responsible for selecting external actions. Learning element uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future Problem generator is responsible for suggesting actions that will lead to a new and informative experiences.","title":"Learning agents"},{"location":"ai/constraint-satisfaction.html","text":"Many AI problems can be viewed as problems of constraint satisfaction in which the goal is to discover some problem state that satisfies a given set of constraints.","title":"Constraint satisfaction"},{"location":"ai/environment.html","text":"Environment \u00b6 PEAS Performance measure Environment Actuators Sensors Types \u00b6 Fully vs Partially Observable Deterministic vs Stochastic Episodic vs Sequential Static vs Dynamic Discrete vs Continuous Single agent vs multi-agent","title":"Environment"},{"location":"ai/environment.html#environment","text":"PEAS Performance measure Environment Actuators Sensors","title":"Environment"},{"location":"ai/environment.html#types","text":"Fully vs Partially Observable Deterministic vs Stochastic Episodic vs Sequential Static vs Dynamic Discrete vs Continuous Single agent vs multi-agent","title":"Types"},{"location":"ai/expert-systems.html","text":"","title":"Expert systems"},{"location":"ai/fields.html","text":"Fields \u00b6 Speech synthesis Computer vision Learning Planning and Reasoning","title":"Fields"},{"location":"ai/fields.html#fields","text":"Speech synthesis Computer vision Learning Planning and Reasoning","title":"Fields"},{"location":"ai/game-playing.html","text":"","title":"Game playing"},{"location":"ai/history.html","text":"History \u00b6 TODO","title":"History"},{"location":"ai/history.html#history","text":"TODO","title":"History"},{"location":"ai/intelligence.html","text":"Intelligence \u00b6 Perceiving one's environment Acting in complex environments Learning and understanding from experience Reasoning to solve problems and discover hidden knowledge Knowledge applying successfully in new situations Thinking abstractly, using analogies. Communicating with others, and more like \u2022 Creativity, Ingenuity, Expressive-ness, Curiosity. Thinking humanly: The cognitive modeling approach Thinking rationally: The \"laws of thought\" approach Acting humanly: The Turing Test approach Acting rationally: The rational agent approach What is artificial intelligence? It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Yes, but what is intelligence? Intelligence is the computational part of the ability to achieve goals in the world. Varying kinds and degrees of intelligence occur in people, many animals and some machines. Isn't there a solid definition of intelligence that doesn't depend on relating it to human intelligence? Not yet. The problem is that we cannot yet characterize in general what kinds of computational procedures we want to call intelligent. We understand some of the mechanisms of intelligence and not others.","title":"Intelligence"},{"location":"ai/intelligence.html#intelligence","text":"Perceiving one's environment Acting in complex environments Learning and understanding from experience Reasoning to solve problems and discover hidden knowledge Knowledge applying successfully in new situations Thinking abstractly, using analogies. Communicating with others, and more like \u2022 Creativity, Ingenuity, Expressive-ness, Curiosity. Thinking humanly: The cognitive modeling approach Thinking rationally: The \"laws of thought\" approach Acting humanly: The Turing Test approach Acting rationally: The rational agent approach What is artificial intelligence? It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Yes, but what is intelligence? Intelligence is the computational part of the ability to achieve goals in the world. Varying kinds and degrees of intelligence occur in people, many animals and some machines. Isn't there a solid definition of intelligence that doesn't depend on relating it to human intelligence? Not yet. The problem is that we cannot yet characterize in general what kinds of computational procedures we want to call intelligent. We understand some of the mechanisms of intelligence and not others.","title":"Intelligence"},{"location":"ai/intro.html","text":"Artificial Intelligence \u00b6 Types \u00b6 hard or strong -> supersedes human intelligence, AGI. soft or weak -> solve specific problems like chess.","title":"Artificial Intelligence"},{"location":"ai/intro.html#artificial-intelligence","text":"","title":"Artificial Intelligence"},{"location":"ai/intro.html#types","text":"hard or strong -> supersedes human intelligence, AGI. soft or weak -> solve specific problems like chess.","title":"Types"},{"location":"ai/means-ends.html","text":"We have studied the strategies which can reason either in forward or backward, but a mixture of the two directions is appropriate for solving a complex and large problem. Such a mixed strategy, make it possible that first to solve the major part of a problem and then go back and solve the small problems arise during combining the big parts of the problem. Such a technique is called Means-Ends Analysis. \u2022 Means-Ends Analysis is problem-solving techniques used in Artificial intelligence for limiting search in AI programs. \u2022 It is a mixture of Backward and forward search technique. \u2022 The MEA technique was first introduced in 1961 by Allen Newell, and Herbert A. Simon in their problem-solving computer program, which was named as General Problem Solver (GPS). \u2022 The MEA analysis process centered on the evaluation of the difference between the current state and goal state.","title":"Means ends"},{"location":"ai/nlp.html","text":"","title":"Nlp"},{"location":"ai/planning.html","text":"Planning \u00b6 Generate sequences of actions to perform tasks and achieve. objectives -> States, actions and goals. Search for solution over abstract space of plans. Classical planning environment: fully observable, deterministic, finite, static(change happens only when the agent acts) and discrete(in time, action, objects, and effects). Planning vs. Problem Solving \u00b6 States, goals, and actions are decomposed into sets of sentences (usually in first-order logic) Search often proceeds through plan space rather than state space (though first we will talk about state-space planners) Subgoals can be planned independently, reducing the complexity of the planning problem Planning Language \u00b6 Expressive enough to describe a wide variety of problems. Restrictive enough to allow efficient algorithms to operate on it. Planning algorithm should be able to take advantage of the logical structure of the problem. STRIPS stands for STanford Research Institute Problem Solver ADL: Action Description Language","title":"Planning"},{"location":"ai/planning.html#planning","text":"Generate sequences of actions to perform tasks and achieve. objectives -> States, actions and goals. Search for solution over abstract space of plans. Classical planning environment: fully observable, deterministic, finite, static(change happens only when the agent acts) and discrete(in time, action, objects, and effects).","title":"Planning"},{"location":"ai/planning.html#planning-vs-problem-solving","text":"States, goals, and actions are decomposed into sets of sentences (usually in first-order logic) Search often proceeds through plan space rather than state space (though first we will talk about state-space planners) Subgoals can be planned independently, reducing the complexity of the planning problem","title":"Planning vs. Problem Solving"},{"location":"ai/planning.html#planning-language","text":"Expressive enough to describe a wide variety of problems. Restrictive enough to allow efficient algorithms to operate on it. Planning algorithm should be able to take advantage of the logical structure of the problem. STRIPS stands for STanford Research Institute Problem Solver ADL: Action Description Language","title":"Planning Language"},{"location":"ai/search.html","text":"Search \u00b6 Why? \u00b6 To achieve goals or to maximize our utility we need to predict what the result of our actions in the future will be. There are many sequences of actions, each with their own utility. We want to find, or search for, the best one. Problem \u00b6 initial state actions: set of possible actions in current state x. transition model: Result(x,a) = state that follows from applying action goal test path cost (additive) A solution is a sequence of steps / path Trees \u00b6 Exploration of state space by generating successors of already-explored states (a.k.a.~expanding states). Every states is evaluated: is it a goal state? Repeated states: Failure to detect repeated states can turn a linear problem into an exponential one! must keep track of all possible states (uses a lot of memory) vs Graph Search \u00b6 **function** TREE-SEARCH(problem) **returns** a solution, or failure initialize the frontier using the initial state of problem **loop do** **if** the frontier is empty **then return** failure choose a leaf node and remove it from the frontier **if** the node contains a goal state **then return** the corresponding solution expand the chosen node, adding the resulting nodes to the frontier **function** GRAPH-SEARCH(problem) **returns** a solution, or failure initialize the frontier using the initial state of problem **initialize the explored set to be empty** loop do **if** the frontier is empty **then return** failure choose a leaf node and remove it from the frontier **if** the node contains a goal state **then return** the corresponding solution **_add the node to the explored set_** expand the chosen node, adding the resulting nodes to the frontier **only if not in the frontier or explored set** Implementation \u00b6 A state is a (representation of) a physical configuration A node n is a data structure constituting part of a search tree contains info such as: state, parent node, action, path cost g(n). The Expand function creates new nodes, filling in the various fields and using the SuccessorFn of the problem to create the corresponding states. Search Strategy \u00b6 picking the order of node expansion Evaluation Metrics \u00b6 completeness time complexity space complexity optimality ( least-cost ) b: maximum branching factor d: depth of the least-cost solution m: maximum depth of the state space (may be \u221e) Uninformed Search \u00b6 While searching you have no clue whether one non-goal state is better than any other. Your search is blind. You don\u2019t know if your current exploration is likely to be fruitful. Various blind strategies: Breadth-first search Uniform-cost search Depth-first search Iterative deepening search Bi-Directional Search Criterion Breadth- First Uniform- Cost Depth- First Depth- Limited Iterative Deepening Bidirectional (if applicable) Time bd b bm b' bd 64n Space b b bm bl bd ber Optimal? Yes Yes No No Yes Yes Complete? Yes Yes No Yes, if I > d Yes Yes Heuristic Search \u00b6 Direct techniques (blind search) are not always possible Weak techniques can be effective if applied correctly on the right kinds of tasks. (Heuristic Search Techniques) Heuristic search techniques make use of domain specific information - a heuristic. Generate and Test \u00b6 do while goal not accomplished generate a possible solution test solution to see if it is a goal Hill Climbing \u00b6 Variation on generate-and-test: generation of next state depends on feedback from the test procedure. Test now includes a heuristic function that provides a guess as to how good each possible state is. There are a number of ways to use the information returned by the test procedure. Searching for a goal state = Climbing to the top of a hill Simple Hill Climbing \u00b6 Use heuristic to move only to states that are better than the current state. Always move to better state when possible. The process ends when all operators have been applied and none of the resulting states are better than the current state. Problems: Will terminate when at local optimum. The order of application of operators can make a big difference. Can\u2019t see past a single move in the state space. Steepest-Ascent Hill Climbing (Gradient Search) \u00b6 Simulated Annealing \u2022 Based on physical process of annealing a metal to get the best (minimal energy) state. \u2022 Hill climbing with a twist: \u2013 allow some moves downhill (to worse states) \u2013 start out allowing large downhill moves (to much worse states) and gradually allow only small downhill moves. A variation of hill climbing in which, at the beginning of the process, some downhill moves may be made. \u2022 To do enough exploration of the whole space early on, so that the final solution is relatively insensitive to the starting state. \u2022 Lowering the chances of getting caught at a local maximum, or plateau, or a ridge. The search initially jumps around a lot, exploring many regions of the state space. \u2022 The jumping is gradually reduced and the search becomes a simple hill climb (search for local optimum).","title":"Search"},{"location":"ai/search.html#search","text":"","title":"Search"},{"location":"ai/search.html#why","text":"To achieve goals or to maximize our utility we need to predict what the result of our actions in the future will be. There are many sequences of actions, each with their own utility. We want to find, or search for, the best one.","title":"Why?"},{"location":"ai/search.html#problem","text":"initial state actions: set of possible actions in current state x. transition model: Result(x,a) = state that follows from applying action goal test path cost (additive) A solution is a sequence of steps / path","title":"Problem"},{"location":"ai/search.html#trees","text":"Exploration of state space by generating successors of already-explored states (a.k.a.~expanding states). Every states is evaluated: is it a goal state? Repeated states: Failure to detect repeated states can turn a linear problem into an exponential one! must keep track of all possible states (uses a lot of memory)","title":"Trees"},{"location":"ai/search.html#vs-graph-search","text":"**function** TREE-SEARCH(problem) **returns** a solution, or failure initialize the frontier using the initial state of problem **loop do** **if** the frontier is empty **then return** failure choose a leaf node and remove it from the frontier **if** the node contains a goal state **then return** the corresponding solution expand the chosen node, adding the resulting nodes to the frontier **function** GRAPH-SEARCH(problem) **returns** a solution, or failure initialize the frontier using the initial state of problem **initialize the explored set to be empty** loop do **if** the frontier is empty **then return** failure choose a leaf node and remove it from the frontier **if** the node contains a goal state **then return** the corresponding solution **_add the node to the explored set_** expand the chosen node, adding the resulting nodes to the frontier **only if not in the frontier or explored set**","title":"vs Graph Search"},{"location":"ai/search.html#implementation","text":"A state is a (representation of) a physical configuration A node n is a data structure constituting part of a search tree contains info such as: state, parent node, action, path cost g(n). The Expand function creates new nodes, filling in the various fields and using the SuccessorFn of the problem to create the corresponding states.","title":"Implementation"},{"location":"ai/search.html#search-strategy","text":"picking the order of node expansion","title":"Search Strategy"},{"location":"ai/search.html#evaluation-metrics","text":"completeness time complexity space complexity optimality ( least-cost ) b: maximum branching factor d: depth of the least-cost solution m: maximum depth of the state space (may be \u221e)","title":"Evaluation Metrics"},{"location":"ai/search.html#uninformed-search","text":"While searching you have no clue whether one non-goal state is better than any other. Your search is blind. You don\u2019t know if your current exploration is likely to be fruitful. Various blind strategies: Breadth-first search Uniform-cost search Depth-first search Iterative deepening search Bi-Directional Search Criterion Breadth- First Uniform- Cost Depth- First Depth- Limited Iterative Deepening Bidirectional (if applicable) Time bd b bm b' bd 64n Space b b bm bl bd ber Optimal? Yes Yes No No Yes Yes Complete? Yes Yes No Yes, if I > d Yes Yes","title":"Uninformed Search"},{"location":"ai/search.html#heuristic-search","text":"Direct techniques (blind search) are not always possible Weak techniques can be effective if applied correctly on the right kinds of tasks. (Heuristic Search Techniques) Heuristic search techniques make use of domain specific information - a heuristic.","title":"Heuristic Search"},{"location":"ai/search.html#generate-and-test","text":"do while goal not accomplished generate a possible solution test solution to see if it is a goal","title":"Generate and Test"},{"location":"ai/search.html#hill-climbing","text":"Variation on generate-and-test: generation of next state depends on feedback from the test procedure. Test now includes a heuristic function that provides a guess as to how good each possible state is. There are a number of ways to use the information returned by the test procedure. Searching for a goal state = Climbing to the top of a hill","title":"Hill Climbing"},{"location":"ai/search.html#simple-hill-climbing","text":"Use heuristic to move only to states that are better than the current state. Always move to better state when possible. The process ends when all operators have been applied and none of the resulting states are better than the current state. Problems: Will terminate when at local optimum. The order of application of operators can make a big difference. Can\u2019t see past a single move in the state space.","title":"Simple Hill Climbing"},{"location":"ai/search.html#steepest-ascent-hill-climbing-gradient-search","text":"Simulated Annealing \u2022 Based on physical process of annealing a metal to get the best (minimal energy) state. \u2022 Hill climbing with a twist: \u2013 allow some moves downhill (to worse states) \u2013 start out allowing large downhill moves (to much worse states) and gradually allow only small downhill moves. A variation of hill climbing in which, at the beginning of the process, some downhill moves may be made. \u2022 To do enough exploration of the whole space early on, so that the final solution is relatively insensitive to the starting state. \u2022 Lowering the chances of getting caught at a local maximum, or plateau, or a ridge. The search initially jumps around a lot, exploring many regions of the state space. \u2022 The jumping is gradually reduced and the search becomes a simple hill climb (search for local optimum).","title":"Steepest-Ascent Hill Climbing (Gradient Search)"},{"location":"ai/subjects.html","text":"Subjects \u00b6 Philosophy Logic, methods of reasoning, mind as physical Mathematics Probability/Statistics Economics Neuroscience Psychology Computer engineering Control theory design Linguistics","title":"Subjects"},{"location":"ai/subjects.html#subjects","text":"Philosophy Logic, methods of reasoning, mind as physical Mathematics Probability/Statistics Economics Neuroscience Psychology Computer engineering Control theory design Linguistics","title":"Subjects"},{"location":"ai/tasks.html","text":"Tasks \u00b6 Formal \u00b6 Games \u00b6 Chess Go Backgammon Checkers Maths \u00b6 Geometry Logic Integral Calculas Properties of Program Expert \u00b6 Egineering Scientific analysis Medical Diagnosis Financial Analysis","title":"Tasks"},{"location":"ai/tasks.html#tasks","text":"","title":"Tasks"},{"location":"ai/tasks.html#formal","text":"","title":"Formal"},{"location":"ai/tasks.html#games","text":"Chess Go Backgammon Checkers","title":"Games"},{"location":"ai/tasks.html#maths","text":"Geometry Logic Integral Calculas Properties of Program","title":"Maths"},{"location":"ai/tasks.html#expert","text":"Egineering Scientific analysis Medical Diagnosis Financial Analysis","title":"Expert"},{"location":"ai/technique.html","text":"Techniques \u00b6 AI technique is a method that exploits knowledge that should be represenetd in such a way that: generalizations. understood by provdors (people) It can easily be modified to correct errors reflect changes in the world. It can be used in many situations even if it is not totally accurate or complete. It can be used to narrow the range of possibilities Metrics \u00b6 complexity use of generalizations clarity of knowledge extensibility of the approach AI Problem Features \u00b6 symbolic reasoning A focus on problems that do not respond to algorithmic solutions. Heuristic search Manipulate the significant quantitative features of a situation rather than relying on numeric methods. semantic meaning Answer that are neither exact nor optimal but \u201csufficient\u201d. Domain specific knowledge meta-level knowledge Criteria for Success \u00b6 long term : Turing Test (for Weak AI) Loebner Prize competition, extremely controversial short term : more modest success in limited domains performance equal or better than humans real-world practicality expert systems (XCON, Prospector), fuzzy logic (cruise control) Turing Test \u00b6","title":"Techniques"},{"location":"ai/technique.html#techniques","text":"AI technique is a method that exploits knowledge that should be represenetd in such a way that: generalizations. understood by provdors (people) It can easily be modified to correct errors reflect changes in the world. It can be used in many situations even if it is not totally accurate or complete. It can be used to narrow the range of possibilities","title":"Techniques"},{"location":"ai/technique.html#metrics","text":"complexity use of generalizations clarity of knowledge extensibility of the approach","title":"Metrics"},{"location":"ai/technique.html#ai-problem-features","text":"symbolic reasoning A focus on problems that do not respond to algorithmic solutions. Heuristic search Manipulate the significant quantitative features of a situation rather than relying on numeric methods. semantic meaning Answer that are neither exact nor optimal but \u201csufficient\u201d. Domain specific knowledge meta-level knowledge","title":"AI Problem Features"},{"location":"ai/technique.html#criteria-for-success","text":"long term : Turing Test (for Weak AI) Loebner Prize competition, extremely controversial short term : more modest success in limited domains performance equal or better than humans real-world practicality expert systems (XCON, Prospector), fuzzy logic (cruise control)","title":"Criteria for Success"},{"location":"ai/technique.html#turing-test","text":"","title":"Turing Test"},{"location":"algo/bits.html","text":"Bits Tricks \u00b6 Java \u00b6 >> (Signed right shift) \u00b6 negative number -> 1 is used as a filler positive number -> 0 is used as a filler 4>>1 -> 2 -4>>1 -> -2 >>> (Unsigned right shift) \u00b6 fills 0 -1 >>> 29 -> 00...0111 -1 >>> 30 -> 00...0011 -1>>>31 -> 00...0001 Detect if two integers have opposite signs \u00b6 XOR \u00b6 bool oppositeSigns(int x, int y) { return ((x ^ y) < 0); } ternery \u00b6 bool oppositeSigns(int x, int y) { return (x < 0)? (y >= 0): (y < 0); } 32 bit ints \u00b6 bool oppositeSigns(int x, int y) { return ((x ^ y) >> 31); } Count total set bits in all numbers from 1 to n \u00b6 A simple solution is to run a loop from 1 to n and sum the count of set bits in all numbers from 1 to n. unsigned int countSetBits(unsigned int n) { int bitCount = 0; for (int i = 1; i <= n; i++) bitCount += countSetBitsUtil(i); return bitCount; } unsigned int countSetBitsUtil(unsigned int x) { if (x <= 0) return 0; return (x % 2 == 0 ? 0 : 1) + countSetBitsUtil(x / 2); } Add two numbers without using arithmetic operators \u00b6 iterative \u00b6 int Add(int x, int y) { while (y != 0) { int carry = x & y; x = x ^ y; y = carry << 1; } return x; } Recursive \u00b6 int Add(int x, int y) { if (y == 0) return x; else return Add( x ^ y, (x & y) << 1); } Smallest of three integers without comparison operators \u00b6 Method 1 (Repeated Subtraction) \u00b6 int smallest(int x, int y, int z) { int c = 0; while (x && y && z) { x--; y--; z--; c++; } return c; } Method 2 (Use Bit Operations) \u00b6 ####define CHAR_BIT 8 int min(int x, int y) { return y + ((x - y) & ((x - y) >> (sizeof(int) * CHAR_BIT - 1))); } int smallest(int x, int y, int z) { return min(x, min(y, z)); } Method 3 (Use Division operator) \u00b6 int smallest(int x, int y, int z) { if (!(y / x)) // Same as \"if (y < x)\" return (!(y / z)) ? y : z; return (!(x / z)) ? x : z; } A Boolean Array Puzzle \u00b6 Input: A array arr[] of two elements having value 0 and 1 Output: Make both elements 0. Specifications: Following are the specifications to follow. It is guaranteed that one element is 0 but we do not know its position. We can\u2019t say about another element it can be 0 or 1. We can only complement array elements, no other operation like and, or, multi, division, \u2026. etc. We can\u2019t use if, else and loop constructs. Obviously, we can\u2019t directly assign 0 to array elements. void changeToZero(int a[2]) { a[ a[1] ] = a[ !a[1] ]; } void changeToZero(int a[2]) { a[ !a[0] ] = a[ !a[1] ] } void changeToZero(int a[2]) { a[ a[1] ] = a[ a[0] ] } void changeToZero(int a[2]) { a[0] = a[a[0]]; a[1] = a[0]; } int main() { int a[] = {1, 0}; changeToZero(a); cout<<\"arr[0] = \"<<a[0]<<endl; cout<<\" arr[1] = \"<<a[1]; return 0; }","title":"Bits Tricks"},{"location":"algo/bits.html#bits-tricks","text":"","title":"Bits Tricks"},{"location":"algo/bits.html#java","text":"","title":"Java"},{"location":"algo/bits.html#signed-right-shift","text":"negative number -> 1 is used as a filler positive number -> 0 is used as a filler 4>>1 -> 2 -4>>1 -> -2","title":"&gt;&gt; (Signed right shift)"},{"location":"algo/bits.html#unsigned-right-shift","text":"fills 0 -1 >>> 29 -> 00...0111 -1 >>> 30 -> 00...0011 -1>>>31 -> 00...0001","title":"&gt;&gt;&gt; (Unsigned right shift)"},{"location":"algo/bits.html#detect-if-two-integers-have-opposite-signs","text":"","title":"Detect if two integers have opposite signs"},{"location":"algo/bits.html#xor","text":"bool oppositeSigns(int x, int y) { return ((x ^ y) < 0); }","title":"XOR"},{"location":"algo/bits.html#ternery","text":"bool oppositeSigns(int x, int y) { return (x < 0)? (y >= 0): (y < 0); }","title":"ternery"},{"location":"algo/bits.html#32-bit-ints","text":"bool oppositeSigns(int x, int y) { return ((x ^ y) >> 31); }","title":"32 bit ints"},{"location":"algo/bits.html#count-total-set-bits-in-all-numbers-from-1-to-n","text":"A simple solution is to run a loop from 1 to n and sum the count of set bits in all numbers from 1 to n. unsigned int countSetBits(unsigned int n) { int bitCount = 0; for (int i = 1; i <= n; i++) bitCount += countSetBitsUtil(i); return bitCount; } unsigned int countSetBitsUtil(unsigned int x) { if (x <= 0) return 0; return (x % 2 == 0 ? 0 : 1) + countSetBitsUtil(x / 2); }","title":"Count total set bits in all numbers from 1 to n"},{"location":"algo/bits.html#add-two-numbers-without-using-arithmetic-operators","text":"","title":"Add two numbers without using arithmetic operators"},{"location":"algo/bits.html#iterative","text":"int Add(int x, int y) { while (y != 0) { int carry = x & y; x = x ^ y; y = carry << 1; } return x; }","title":"iterative"},{"location":"algo/bits.html#recursive","text":"int Add(int x, int y) { if (y == 0) return x; else return Add( x ^ y, (x & y) << 1); }","title":"Recursive"},{"location":"algo/bits.html#smallest-of-three-integers-without-comparison-operators","text":"","title":"Smallest of three integers without comparison operators"},{"location":"algo/bits.html#method-1-repeated-subtraction","text":"int smallest(int x, int y, int z) { int c = 0; while (x && y && z) { x--; y--; z--; c++; } return c; }","title":"Method 1 (Repeated Subtraction)"},{"location":"algo/bits.html#method-2-use-bit-operations","text":"####define CHAR_BIT 8 int min(int x, int y) { return y + ((x - y) & ((x - y) >> (sizeof(int) * CHAR_BIT - 1))); } int smallest(int x, int y, int z) { return min(x, min(y, z)); }","title":"Method 2 (Use Bit Operations)"},{"location":"algo/bits.html#method-3-use-division-operator","text":"int smallest(int x, int y, int z) { if (!(y / x)) // Same as \"if (y < x)\" return (!(y / z)) ? y : z; return (!(x / z)) ? x : z; }","title":"Method 3 (Use Division operator)"},{"location":"algo/bits.html#a-boolean-array-puzzle","text":"Input: A array arr[] of two elements having value 0 and 1 Output: Make both elements 0. Specifications: Following are the specifications to follow. It is guaranteed that one element is 0 but we do not know its position. We can\u2019t say about another element it can be 0 or 1. We can only complement array elements, no other operation like and, or, multi, division, \u2026. etc. We can\u2019t use if, else and loop constructs. Obviously, we can\u2019t directly assign 0 to array elements. void changeToZero(int a[2]) { a[ a[1] ] = a[ !a[1] ]; } void changeToZero(int a[2]) { a[ !a[0] ] = a[ !a[1] ] } void changeToZero(int a[2]) { a[ a[1] ] = a[ a[0] ] } void changeToZero(int a[2]) { a[0] = a[a[0]]; a[1] = a[0]; } int main() { int a[] = {1, 0}; changeToZero(a); cout<<\"arr[0] = \"<<a[0]<<endl; cout<<\" arr[1] = \"<<a[1]; return 0; }","title":"A Boolean Array Puzzle"},{"location":"algo/glossary.html","text":"Glossary \u00b6 arr - array l - length L - Levels N - number (length)","title":"Glossary"},{"location":"algo/glossary.html#glossary","text":"arr - array l - length L - Levels N - number (length)","title":"Glossary"},{"location":"algo/maths.html","text":"Maths \u00b6 Multiple of 3 \u00b6 Proof: Above can be proved by taking the example of 11 in decimal numbers. (In this context 11 in decimal numbers is same as 3 in binary numbers) If difference between sum of odd digits and even digits is multiple of 11 then decimal number is multiple of 11. Let\u2019s see how. Let\u2019s take the example of 2 digit numbers in decimal AB = 11A -A + B = 11A + (B \u2013 A) So if (B \u2013 A) is a multiple of 11 then is AB. Let us take 3 digit numbers. ABC = 99A + A + 11B \u2013 B + C = (99A + 11B) + (A + C \u2013 B) So if (A + C \u2013 B) is a multiple of 11 then is (ABC) Let us take 4 digit numbers now. ABCD = 1001A + D + 11C \u2013 C + 999B + B \u2013 A = (1001A \u2013 999B + 11C) + (D + B \u2013 A -C ) So, if (B + D \u2013 A \u2013 C) is a multiple of 11 then is ABCD. This can be continued for all decimal numbers. Above concept can be proved for 3 in binary numbers in the same way. int isMultipleOf3(int n) { int odd_count = 0; int even_count = 0; if (n < 0) n = -n; if (n == 0) return 1; if (n == 1) return 0; while (n) { if (n & 1) odd_count++; if (n & 2) even_count++; n = n >> 2; } return isMultipleOf3(abs(odd_count - even_count)); } Multiply by 7 \u00b6 7n = 8n -n n<<3 -n Note: Works only for positive integers. Same concept can be used for fast multiplication by 9 or other numbers. Lucky number \u00b6 bool isLucky(int n) { static int counter = 2; int next_position = n; if(counter > n) return 1; if(n % counter == 0) return 0; counter++; return isLucky(next_position); } Base n Addition \u00b6 column by column addition -> carry if sum > base -> sum - base; char *sumBase14(char num1[], char num2[]) { int l1 = strlen(num1); int l2 = strlen(num2); char *res; int i; int nml1, nml2, res_nml; bool carry = 0; if(l1 != l2) { cout << \"Function doesn't support numbers of different\" \" lengths. If you want to add such numbers then\" \" prefix smaller number with required no. of zeroes\"; assert(0); } res = new char[(sizeof(char)*(l1 + 1))]; for(i = l1-1; i >= 0; i--) { nml1 = getNumeralValue(num1[i]); nml2 = getNumeralValue(num2[i]); res_nml = carry + nml1 + nml2; if(res_nml >= 14) { carry = 1; res_nml -= 14; } else { carry = 0; } res[i+1] = getNumeral(res_nml); } if(carry == 0) return (res + 1); res[0] = '1'; return res; } int getNumeralValue(char num) { if( num >= '0' && num <= '9') return (num - '0'); if( num >= 'A' && num <= 'D') return (num - 'A' + 10); assert(0); } char getNumeral(int val) { if( val >= 0 && val <= 9) return (val + '0'); if( val >= 10 && val <= 14) return (val + 'A' - 10); assert(0); } Notes: The above approach can be used to add numbers to any base. We don\u2019t have to do string operations if the base is smaller than 10. You can try extending the above program for numbers of different lengths. Square Root \u00b6 float squareRoot(float n) { /*We are using n itself as initial approximation This can definitely be improved */ float x = n; float y = 1; float e = 0.000001; /* e decides the accuracy level*/ while (x - y > e) { x = (x + y) / 2; y = n / x; } return x; } Multiply two integers without using multiplication, division and bitwise operators, and no loops \u00b6 public : int multiply(int x, int y) { if(y == 0) return 0; if(y > 0 ) return (x + multiply(x, y-1)); if(y < 0 ) return -multiply(x, -y); } Russian Peasant (Multiply two numbers using bitwise operators) \u00b6 unsigned int russianPeasant(unsigned int a, unsigned int b) { int res = 0; // initialize result // While second number doesn't become 1 while (b > 0) { // If second number becomes odd, add the first number to result if (b & 1) res = res + a; // Double the first number and halve the second number a = a << 1; b = b >> 1; } return res; } How does this work? The value of a b is same as (a 2) (b/2) if b is even, otherwise the value is same as ((a 2)*(b/2) + a). In the while loop, we keep multiplying \u2018a\u2019 with 2 and keep dividing \u2018b\u2019 by 2. If \u2018b\u2019 becomes odd in loop, we add \u2018a\u2019 to \u2018res\u2019. When value of \u2018b\u2019 becomes 1, the value of \u2018res\u2019 + \u2018a\u2019, gives us the result. Note that when \u2018b\u2019 is a power of 2, the \u2018res\u2019 would remain 0 and \u2018a\u2019 would have the multiplication Print all combinations of points that can compose a given number \u00b6 void printCompositions(int n, int i) { static int arr[ARR_SIZE]; if (n == 0) { printArray(arr, i); } else if(n > 0) { int k; for (k = 1; k <= MAX_POINT; k++) { arr[i]= k; printCompositions(n-k, i+1); } } } Write you own Power without using multiplication(*) and division(/) operators \u00b6 Method 1 (Using Nested Loops) \u00b6 int pow(int a, int b) { if (b == 0) return 1; int answer = a; int increment = a; int i, j; for(i = 1; i < b; i++) { for(j = 1; j < a; j++) { answer += increment; } increment = answer; } return answer; } Method 2 (Using Recursion) \u00b6 /* A recursive function to get x*y */ int multiply(int x, int y) { if(y) return (x + multiply(x, y - 1)); else return 0; } /* A recursive function to get a^b Works only if a >= 0 and b >= 0 */ int pow(int a, int b) { if(b) return multiply(a, pow(a, b - 1)); else return 1; } Fibonici \u00b6 Method 1 (Recurssion) \u00b6 Time Complexity: T(n) = T(n-1) + T(n-2) which is exponential. Extra Space: O(n) if we consider the function call stack size, otherwise O(1). int fib(int n) { if (n <= 1) return n; return fib(n-1) + fib(n-2); } Method 2 (DP) \u00b6 Time Complexity: O(n) Extra Space: O(n) int fib(int n) { int f[n + 2]; int i; f[0] = 0; f[1] = 1; for(i = 2; i <= n; i++) { f[i] = f[i - 1] + f[i - 2]; } return f[n]; } }; Method 3 (space optimised 2) \u00b6 Time Complexity:O(n) Extra Space: O(1) int fib(int n) { int a = 0, b = 1, c, i; if( n == 0) return a; for(i = 2; i <= n; i++) { c = a + b; a = b; b = c; } return b; } Method 4 ( Using power of the matrix {{1,1},{1,0}} ) \u00b6 This another O(n) which relies on the fact that if we n times multiply the matrix M = {{1,1},{1,0}} to itself (in other words calculate power(M, n )), then we get the (n+1)th Fibonacci number as the element at row and column (0, 0) in the resultant matrix. The matrix representation gives the following closed expression for the Fibonacci numbers: Time Complexity: O(n) Extra Space: O(1) int fib(int n) { int F[2][2] = {{1,1},{1,0}}; if (n == 0) return 0; power(F, n-1); return F[0][0]; } void multiply(int F[2][2], int M[2][2]) { int x = F[0][0]*M[0][0] + F[0][1]*M[1][0]; int y = F[0][0]*M[0][1] + F[0][1]*M[1][1]; int z = F[1][0]*M[0][0] + F[1][1]*M[1][0]; int w = F[1][0]*M[0][1] + F[1][1]*M[1][1]; F[0][0] = x; F[0][1] = y; F[1][0] = z; F[1][1] = w; } void power(int F[2][2], int n) { int i; int M[2][2] = {{1,1},{1,0}}; // n - 1 times multiply the matrix to {{1,0},{0,1}} for (i = 2; i <= n; i++) multiply(F, M); } Method 5 ( Optimized Method 4 ) \u00b6 The method 4 can be optimized to work in O(Logn) time complexity. We can do recursive multiplication to get power(M, n) in the prevous method Time Complexity: O(Logn) Extra Space: O(Logn) if we consider the function call stack size, otherwise O(1). int fib(int n) { int F[2][2] = {{1, 1}, {1, 0}}; if (n == 0) return 0; power(F, n - 1); return F[0][0]; } void power(int F[2][2], int n) { if(n == 0 || n == 1) return; int M[2][2] = {{1, 1}, {1, 0}}; power(F, n / 2); multiply(F, F); if (n % 2 != 0) multiply(F, M); } void multiply(int F[2][2], int M[2][2]) { int x = F[0][0] * M[0][0] + F[0][1] * M[1][0]; int y = F[0][0] * M[0][1] + F[0][1] * M[1][1]; int z = F[1][0] * M[0][0] + F[1][1] * M[1][0]; int w = F[1][0] * M[0][1] + F[1][1] * M[1][1]; F[0][0] = x; F[0][1] = y; F[1][0] = z; F[1][1] = w; } Method 6 (O(Log n) Time) \u00b6 If n is even then k = n/2: F(n) = [2*F(k-1) + F(k)]*F(k) If n is odd then k = (n + 1)/2 F(n) = F(k)*F(k) + F(k-1)*F(k-1) Method 7 (Using formula) \u00b6 Fn = {[(\u221a5 + 1)/2] ^ n} / \u221a5 Average of Stream of numbers \u00b6 float getAvg(float prev_avg, int x, int n) { return (prev_avg * n + x) / (n + 1); } The above function getAvg() can be optimized using following changes. We can avoid the use of prev_avg and number of elements by using static variables (Assuming that only this function is called for average of stream). Following is the oprimnized version. filter_none float getAvg(int x) { static int sum, n; sum += x; return (((float)sum) / ++n); }","title":"Maths"},{"location":"algo/maths.html#maths","text":"","title":"Maths"},{"location":"algo/maths.html#multiple-of-3","text":"Proof: Above can be proved by taking the example of 11 in decimal numbers. (In this context 11 in decimal numbers is same as 3 in binary numbers) If difference between sum of odd digits and even digits is multiple of 11 then decimal number is multiple of 11. Let\u2019s see how. Let\u2019s take the example of 2 digit numbers in decimal AB = 11A -A + B = 11A + (B \u2013 A) So if (B \u2013 A) is a multiple of 11 then is AB. Let us take 3 digit numbers. ABC = 99A + A + 11B \u2013 B + C = (99A + 11B) + (A + C \u2013 B) So if (A + C \u2013 B) is a multiple of 11 then is (ABC) Let us take 4 digit numbers now. ABCD = 1001A + D + 11C \u2013 C + 999B + B \u2013 A = (1001A \u2013 999B + 11C) + (D + B \u2013 A -C ) So, if (B + D \u2013 A \u2013 C) is a multiple of 11 then is ABCD. This can be continued for all decimal numbers. Above concept can be proved for 3 in binary numbers in the same way. int isMultipleOf3(int n) { int odd_count = 0; int even_count = 0; if (n < 0) n = -n; if (n == 0) return 1; if (n == 1) return 0; while (n) { if (n & 1) odd_count++; if (n & 2) even_count++; n = n >> 2; } return isMultipleOf3(abs(odd_count - even_count)); }","title":"Multiple of 3"},{"location":"algo/maths.html#multiply-by-7","text":"7n = 8n -n n<<3 -n Note: Works only for positive integers. Same concept can be used for fast multiplication by 9 or other numbers.","title":"Multiply by 7"},{"location":"algo/maths.html#lucky-number","text":"bool isLucky(int n) { static int counter = 2; int next_position = n; if(counter > n) return 1; if(n % counter == 0) return 0; counter++; return isLucky(next_position); }","title":"Lucky number"},{"location":"algo/maths.html#base-n-addition","text":"column by column addition -> carry if sum > base -> sum - base; char *sumBase14(char num1[], char num2[]) { int l1 = strlen(num1); int l2 = strlen(num2); char *res; int i; int nml1, nml2, res_nml; bool carry = 0; if(l1 != l2) { cout << \"Function doesn't support numbers of different\" \" lengths. If you want to add such numbers then\" \" prefix smaller number with required no. of zeroes\"; assert(0); } res = new char[(sizeof(char)*(l1 + 1))]; for(i = l1-1; i >= 0; i--) { nml1 = getNumeralValue(num1[i]); nml2 = getNumeralValue(num2[i]); res_nml = carry + nml1 + nml2; if(res_nml >= 14) { carry = 1; res_nml -= 14; } else { carry = 0; } res[i+1] = getNumeral(res_nml); } if(carry == 0) return (res + 1); res[0] = '1'; return res; } int getNumeralValue(char num) { if( num >= '0' && num <= '9') return (num - '0'); if( num >= 'A' && num <= 'D') return (num - 'A' + 10); assert(0); } char getNumeral(int val) { if( val >= 0 && val <= 9) return (val + '0'); if( val >= 10 && val <= 14) return (val + 'A' - 10); assert(0); } Notes: The above approach can be used to add numbers to any base. We don\u2019t have to do string operations if the base is smaller than 10. You can try extending the above program for numbers of different lengths.","title":"Base n Addition"},{"location":"algo/maths.html#square-root","text":"float squareRoot(float n) { /*We are using n itself as initial approximation This can definitely be improved */ float x = n; float y = 1; float e = 0.000001; /* e decides the accuracy level*/ while (x - y > e) { x = (x + y) / 2; y = n / x; } return x; }","title":"Square Root"},{"location":"algo/maths.html#multiply-two-integers-without-using-multiplication-division-and-bitwise-operators-and-no-loops","text":"public : int multiply(int x, int y) { if(y == 0) return 0; if(y > 0 ) return (x + multiply(x, y-1)); if(y < 0 ) return -multiply(x, -y); }","title":"Multiply two integers without using multiplication, division and bitwise operators, and no loops"},{"location":"algo/maths.html#russian-peasant-multiply-two-numbers-using-bitwise-operators","text":"unsigned int russianPeasant(unsigned int a, unsigned int b) { int res = 0; // initialize result // While second number doesn't become 1 while (b > 0) { // If second number becomes odd, add the first number to result if (b & 1) res = res + a; // Double the first number and halve the second number a = a << 1; b = b >> 1; } return res; } How does this work? The value of a b is same as (a 2) (b/2) if b is even, otherwise the value is same as ((a 2)*(b/2) + a). In the while loop, we keep multiplying \u2018a\u2019 with 2 and keep dividing \u2018b\u2019 by 2. If \u2018b\u2019 becomes odd in loop, we add \u2018a\u2019 to \u2018res\u2019. When value of \u2018b\u2019 becomes 1, the value of \u2018res\u2019 + \u2018a\u2019, gives us the result. Note that when \u2018b\u2019 is a power of 2, the \u2018res\u2019 would remain 0 and \u2018a\u2019 would have the multiplication","title":"Russian Peasant (Multiply two numbers using bitwise operators)"},{"location":"algo/maths.html#print-all-combinations-of-points-that-can-compose-a-given-number","text":"void printCompositions(int n, int i) { static int arr[ARR_SIZE]; if (n == 0) { printArray(arr, i); } else if(n > 0) { int k; for (k = 1; k <= MAX_POINT; k++) { arr[i]= k; printCompositions(n-k, i+1); } } }","title":"Print all combinations of points that can compose a given number"},{"location":"algo/maths.html#write-you-own-power-without-using-multiplication-and-division-operators","text":"","title":"Write you own Power without using multiplication(*) and division(/) operators"},{"location":"algo/maths.html#method-1-using-nested-loops","text":"int pow(int a, int b) { if (b == 0) return 1; int answer = a; int increment = a; int i, j; for(i = 1; i < b; i++) { for(j = 1; j < a; j++) { answer += increment; } increment = answer; } return answer; }","title":"Method 1 (Using Nested Loops)"},{"location":"algo/maths.html#method-2-using-recursion","text":"/* A recursive function to get x*y */ int multiply(int x, int y) { if(y) return (x + multiply(x, y - 1)); else return 0; } /* A recursive function to get a^b Works only if a >= 0 and b >= 0 */ int pow(int a, int b) { if(b) return multiply(a, pow(a, b - 1)); else return 1; }","title":"Method 2 (Using Recursion)"},{"location":"algo/maths.html#fibonici","text":"","title":"Fibonici"},{"location":"algo/maths.html#method-1-recurssion","text":"Time Complexity: T(n) = T(n-1) + T(n-2) which is exponential. Extra Space: O(n) if we consider the function call stack size, otherwise O(1). int fib(int n) { if (n <= 1) return n; return fib(n-1) + fib(n-2); }","title":"Method 1 (Recurssion)"},{"location":"algo/maths.html#method-2-dp","text":"Time Complexity: O(n) Extra Space: O(n) int fib(int n) { int f[n + 2]; int i; f[0] = 0; f[1] = 1; for(i = 2; i <= n; i++) { f[i] = f[i - 1] + f[i - 2]; } return f[n]; } };","title":"Method 2 (DP)"},{"location":"algo/maths.html#method-3-space-optimised-2","text":"Time Complexity:O(n) Extra Space: O(1) int fib(int n) { int a = 0, b = 1, c, i; if( n == 0) return a; for(i = 2; i <= n; i++) { c = a + b; a = b; b = c; } return b; }","title":"Method 3 (space optimised 2)"},{"location":"algo/maths.html#method-4-using-power-of-the-matrix-1110","text":"This another O(n) which relies on the fact that if we n times multiply the matrix M = {{1,1},{1,0}} to itself (in other words calculate power(M, n )), then we get the (n+1)th Fibonacci number as the element at row and column (0, 0) in the resultant matrix. The matrix representation gives the following closed expression for the Fibonacci numbers: Time Complexity: O(n) Extra Space: O(1) int fib(int n) { int F[2][2] = {{1,1},{1,0}}; if (n == 0) return 0; power(F, n-1); return F[0][0]; } void multiply(int F[2][2], int M[2][2]) { int x = F[0][0]*M[0][0] + F[0][1]*M[1][0]; int y = F[0][0]*M[0][1] + F[0][1]*M[1][1]; int z = F[1][0]*M[0][0] + F[1][1]*M[1][0]; int w = F[1][0]*M[0][1] + F[1][1]*M[1][1]; F[0][0] = x; F[0][1] = y; F[1][0] = z; F[1][1] = w; } void power(int F[2][2], int n) { int i; int M[2][2] = {{1,1},{1,0}}; // n - 1 times multiply the matrix to {{1,0},{0,1}} for (i = 2; i <= n; i++) multiply(F, M); }","title":"Method 4 ( Using power of the matrix {{1,1},{1,0}} )"},{"location":"algo/maths.html#method-5-optimized-method-4","text":"The method 4 can be optimized to work in O(Logn) time complexity. We can do recursive multiplication to get power(M, n) in the prevous method Time Complexity: O(Logn) Extra Space: O(Logn) if we consider the function call stack size, otherwise O(1). int fib(int n) { int F[2][2] = {{1, 1}, {1, 0}}; if (n == 0) return 0; power(F, n - 1); return F[0][0]; } void power(int F[2][2], int n) { if(n == 0 || n == 1) return; int M[2][2] = {{1, 1}, {1, 0}}; power(F, n / 2); multiply(F, F); if (n % 2 != 0) multiply(F, M); } void multiply(int F[2][2], int M[2][2]) { int x = F[0][0] * M[0][0] + F[0][1] * M[1][0]; int y = F[0][0] * M[0][1] + F[0][1] * M[1][1]; int z = F[1][0] * M[0][0] + F[1][1] * M[1][0]; int w = F[1][0] * M[0][1] + F[1][1] * M[1][1]; F[0][0] = x; F[0][1] = y; F[1][0] = z; F[1][1] = w; }","title":"Method 5 ( Optimized Method 4 )"},{"location":"algo/maths.html#method-6-olog-n-time","text":"If n is even then k = n/2: F(n) = [2*F(k-1) + F(k)]*F(k) If n is odd then k = (n + 1)/2 F(n) = F(k)*F(k) + F(k-1)*F(k-1)","title":"Method 6 (O(Log n) Time)"},{"location":"algo/maths.html#method-7-using-formula","text":"Fn = {[(\u221a5 + 1)/2] ^ n} / \u221a5","title":"Method 7 (Using formula)"},{"location":"algo/maths.html#average-of-stream-of-numbers","text":"float getAvg(float prev_avg, int x, int n) { return (prev_avg * n + x) / (n + 1); } The above function getAvg() can be optimized using following changes. We can avoid the use of prev_avg and number of elements by using static variables (Assuming that only this function is called for average of stream). Following is the oprimnized version. filter_none float getAvg(int x) { static int sum, n; sum += x; return (((float)sum) / ++n); }","title":"Average of Stream of numbers"},{"location":"algo/analysis/amortised.html","text":"Amortized Analysis \u00b6 is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. In Amortized Analysis, we analyze a sequence of operations and guarantee a worst case average time which is lower than the worst case time of a particular expensive operation. The example data structures whose operations are analyzed using Amortized Analysis are Hash Tables, Disjoint Sets and Splay Trees. Let us consider an example of a simple hash table insertions. How do we decide table size? There is a trade-off between space and time, if we make hash-table size big, search time becomes fast, but space required becomes high. Dynamic Table The solution to this trade-off problem is to use Dynamic Table (or Arrays). The idea is to increase size of table whenever it becomes full. Following are the steps to follow when table becomes full. 1) Allocate memory for a larger table of size, typically twice the old table. 2) Copy the contents of old table to new table. 3) Free the old table. If the table has space available, we simply insert new item in available space. What is the time complexity of n insertions using the above scheme? If we use simple analysis, the worst case cost of an insertion is O(n). Therefore, worst case cost of n inserts is n * O(n) which is O(n2). This analysis gives an upper bound, but not a tight upper bound for n insertions as all insertions don\u2019t take \u0398(n) time. So using Amortized Analysis, we could prove that the Dynamic Table scheme has O(1) insertion time which is a great result used in hashing. Also, the concept of dynamic table is used in vectors in C++, ArrayList in Java. Following are few important notes. 1) Amortized cost of a sequence of operations can be seen as expenses of a salaried person. The average monthly expense of the person is less than or equal to the salary, but the person can spend more money in a particular month by buying a car or something. In other months, he or she saves money for the expensive month. 2) The above Amortized Analysis done for Dynamic Array example is called Aggregate Method. There are two more powerful ways to do Amortized analysis called Accounting Method and Potential Method. We will be discussing the other two methods in separate posts. 3) The amortized analysis doesn\u2019t involve probability. There is also another different notion of average-case running time where algorithms use randomization to make them faster and expected running time is faster than the worst-case running time. These algorithms are analyzed using Randomized Analysis. Examples of these algorithms are Randomized Quick Sort, Quick Select and Hashing. We will soon be covering Randomized analysis in a different post. To perform the amortized analysis of Red-Black Tree Insertion operation, we use Potential(or Physicist\u2019s) method. For potential method, we define a potential function \\phi that maps a data structure to a non-negative real value. An operation can result in a change of this potential. Let us define the potential function \\phi in the following manner: https://www.geeksforgeeks.org/analysis-algorithm-set-5-amortized-analysis-introduction/ Thus, the total amortized cost of insertion in Red-Black Tree is O(n).","title":"Amortized Analysis"},{"location":"algo/analysis/amortised.html#amortized-analysis","text":"is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. In Amortized Analysis, we analyze a sequence of operations and guarantee a worst case average time which is lower than the worst case time of a particular expensive operation. The example data structures whose operations are analyzed using Amortized Analysis are Hash Tables, Disjoint Sets and Splay Trees. Let us consider an example of a simple hash table insertions. How do we decide table size? There is a trade-off between space and time, if we make hash-table size big, search time becomes fast, but space required becomes high. Dynamic Table The solution to this trade-off problem is to use Dynamic Table (or Arrays). The idea is to increase size of table whenever it becomes full. Following are the steps to follow when table becomes full. 1) Allocate memory for a larger table of size, typically twice the old table. 2) Copy the contents of old table to new table. 3) Free the old table. If the table has space available, we simply insert new item in available space. What is the time complexity of n insertions using the above scheme? If we use simple analysis, the worst case cost of an insertion is O(n). Therefore, worst case cost of n inserts is n * O(n) which is O(n2). This analysis gives an upper bound, but not a tight upper bound for n insertions as all insertions don\u2019t take \u0398(n) time. So using Amortized Analysis, we could prove that the Dynamic Table scheme has O(1) insertion time which is a great result used in hashing. Also, the concept of dynamic table is used in vectors in C++, ArrayList in Java. Following are few important notes. 1) Amortized cost of a sequence of operations can be seen as expenses of a salaried person. The average monthly expense of the person is less than or equal to the salary, but the person can spend more money in a particular month by buying a car or something. In other months, he or she saves money for the expensive month. 2) The above Amortized Analysis done for Dynamic Array example is called Aggregate Method. There are two more powerful ways to do Amortized analysis called Accounting Method and Potential Method. We will be discussing the other two methods in separate posts. 3) The amortized analysis doesn\u2019t involve probability. There is also another different notion of average-case running time where algorithms use randomization to make them faster and expected running time is faster than the worst-case running time. These algorithms are analyzed using Randomized Analysis. Examples of these algorithms are Randomized Quick Sort, Quick Select and Hashing. We will soon be covering Randomized analysis in a different post. To perform the amortized analysis of Red-Black Tree Insertion operation, we use Potential(or Physicist\u2019s) method. For potential method, we define a potential function \\phi that maps a data structure to a non-negative real value. An operation can result in a change of this potential. Let us define the potential function \\phi in the following manner: https://www.geeksforgeeks.org/analysis-algorithm-set-5-amortized-analysis-introduction/ Thus, the total amortized cost of insertion in Red-Black Tree is O(n).","title":"Amortized Analysis"},{"location":"algo/analysis/caching.html","text":"Caching \u00b6 Ignoring the compiler optimizations, which of the two is better implementation of sum? int fun1(int arr[R][C]) { int sum = 0; for (int i=0; i<R; i++) for (int j=0; j<C; j++) sum += arr[i][j]; } int fun2(int arr[R][C]) { int sum = 0; for (int j=0; j<C; j++) for (int i=0; i<R; i++) sum += arr[i][j]; } In C/C++, elements are stored in Row-Major order. So the first implementation has better spatial locality (nearby memory locations are referenced in successive iterations). Therefore, first implementation should always be preferred for iterating multidimensional arrays.","title":"Caching"},{"location":"algo/analysis/caching.html#caching","text":"Ignoring the compiler optimizations, which of the two is better implementation of sum? int fun1(int arr[R][C]) { int sum = 0; for (int i=0; i<R; i++) for (int j=0; j<C; j++) sum += arr[i][j]; } int fun2(int arr[R][C]) { int sum = 0; for (int j=0; j<C; j++) for (int i=0; i<R; i++) sum += arr[i][j]; } In C/C++, elements are stored in Row-Major order. So the first implementation has better spatial locality (nearby memory locations are referenced in successive iterations). Therefore, first implementation should always be preferred for iterating multidimensional arrays.","title":"Caching"},{"location":"algo/analysis/classes.html","text":"Time complexity classes \u00b6 O(1) loop with constant swap O(n): if the loop variables is incremented / decremented by a constant amount. O(nc): Time complexity of nested loops is equal to the number of times the innermost statement is executed. O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount. Binary -> is 1, c, c2, c3, \u2026 ck. If we put k equals to Logcn, we get cLogcn which is n. O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables is reduced / increased exponentially by a constant amount. // Here c is a constant greater than 1 for (int i = 2; i <=n; i = pow(i, c)) { // some O(1) expressions } //Here fun is sqrt or cuberoot or any other constant root for (int i = n; i > 1; i = fun(i)) { // some O(1) expressions } See this for mathematical details. https://www.geeksforgeeks.org/time-complexity-loop-loop-variable-expands-shrinks-exponentially/ How to combine time complexities of consecutive loops? \u00b6 When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops. How to calculate time complexity when there are many if, else statements inside loops? Consider worst case bitch! What did you expect you little piece of shit For example consider the linear search function where we consider the case when element is present at the end or not present at all. When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if else and other complex control statements. P \u00b6 solved by deterministic Turing machine in Polynomial time. NP \u00b6 set of decision problems solved by a Non-deterministic Turing Machine in Polynomial time. P is subset of NP. NP is set of decision problems which can be solved by a polynomial time via a \u201cLucky Algorithm\u201d, a magical algorithm that always makes a right guess among the given set of choices L: A decision problem. A: Algoriithm on L. NP-complete are hardest in NP set. L is NP-complete if: 1) L is in NP (Any given solution for NP-complete problems can be verified quickly, but there is no efficient known solution). 2) Every problem in NP is reducible to L in polynomial time. NP-Hard -> follows property 2 but no neccassirally property 1. NP-Complete set is also a subset of NP-Hard set. Decision vs Optimization Problems \u00b6 NP-completeness applies on decision problems . As it\u2019s easier to compare the difficulty of decision problems than that of optimization problems. In reality, though, being able to solve a decision problem in polynomial time will often permit us to solve the corresponding optimization problem in polynomial time (using a polynomial number of calls to the decision problem). So, discussing the difficulty of decision problems is often really equivalent to discussing the difficulty of optimization problems. For example, consider the vertex cover problem (Given a graph, find out the minimum sized vertex set that covers all edges). It is an optimization problem. Corresponding decision problem is, given undirected graph G and k, is there a vertex cover of size k? Reduction \u00b6 if A1 solves L1. i.e. x1->A1->y1 given l1 = {x1,y1} if A2 solves L2. i.e. x2->A2->y2 given l2 = {x2,y2} Transoform input x1 into x2 with funtion f such that A1 = A2(f(x1)) The idea is to find a transformation(f) from L1 to L2 so that the algorithm A2 can be part of an algorithm A1 to solve L1. Learning reduction in general is very important. if we have library functions to solve certain problem and if we can reduce a new problem to one of the solved problems, we save a lot of time. For example, Consider the example of a problem where we have to find minimum product path in a given directed graph where product of path is multiplication of weights of edges along the path. If we have code for Dijkstra\u2019s algorithm to find shortest path, we can take log of all weights and use Dijkstra\u2019s algorithm to find the minimum product path rather than writing a fresh code for this new problem. How to prove that a given problem is NP complete? \u00b6 From the definition of NP-complete, it appears impossible to prove that a problem L is NP-Complete. By definition, it requires us to that show every problem in NP is polynomial time reducible to L. Fortunately, there is an alternate way to prove it. The idea is to take a known NP-Complete problem and reduce it to L. If polynomial time reduction is possible, we can prove that L is NP-Complete by transitivity of reduction (If a NP-Complete problem is reducible to L in polynomial time, then all problems are reducible to L in polynomial time). first problem as NP-Complete \u00b6 (SAT)Boolean satisfiability problem It is always useful to know about NP-Completeness even for engineers. Suppose you are asked to write an efficient algorithm to solve an extremely important problem for your company. After a lot of thinking, you can only come up exponential time approach which is impractical. If you don\u2019t know about NP-Completeness, you can only say that I could not come with an efficient algorithm. If you know about NP-Completeness and prove that the problem as NP-complete, you can proudly say that the polynomial time solution is unlikely to exist. If there is a polynomial time solution possible, then that solution solves a big problem of computer science many scientists have been trying for years.","title":"Time complexity classes"},{"location":"algo/analysis/classes.html#time-complexity-classes","text":"O(1) loop with constant swap O(n): if the loop variables is incremented / decremented by a constant amount. O(nc): Time complexity of nested loops is equal to the number of times the innermost statement is executed. O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount. Binary -> is 1, c, c2, c3, \u2026 ck. If we put k equals to Logcn, we get cLogcn which is n. O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables is reduced / increased exponentially by a constant amount. // Here c is a constant greater than 1 for (int i = 2; i <=n; i = pow(i, c)) { // some O(1) expressions } //Here fun is sqrt or cuberoot or any other constant root for (int i = n; i > 1; i = fun(i)) { // some O(1) expressions } See this for mathematical details. https://www.geeksforgeeks.org/time-complexity-loop-loop-variable-expands-shrinks-exponentially/","title":"Time complexity classes"},{"location":"algo/analysis/classes.html#how-to-combine-time-complexities-of-consecutive-loops","text":"When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops. How to calculate time complexity when there are many if, else statements inside loops? Consider worst case bitch! What did you expect you little piece of shit For example consider the linear search function where we consider the case when element is present at the end or not present at all. When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if else and other complex control statements.","title":"How to combine time complexities of consecutive loops?"},{"location":"algo/analysis/classes.html#p","text":"solved by deterministic Turing machine in Polynomial time.","title":"P "},{"location":"algo/analysis/classes.html#np","text":"set of decision problems solved by a Non-deterministic Turing Machine in Polynomial time. P is subset of NP. NP is set of decision problems which can be solved by a polynomial time via a \u201cLucky Algorithm\u201d, a magical algorithm that always makes a right guess among the given set of choices L: A decision problem. A: Algoriithm on L. NP-complete are hardest in NP set. L is NP-complete if: 1) L is in NP (Any given solution for NP-complete problems can be verified quickly, but there is no efficient known solution). 2) Every problem in NP is reducible to L in polynomial time. NP-Hard -> follows property 2 but no neccassirally property 1. NP-Complete set is also a subset of NP-Hard set.","title":"NP "},{"location":"algo/analysis/classes.html#decision-vs-optimization-problems","text":"NP-completeness applies on decision problems . As it\u2019s easier to compare the difficulty of decision problems than that of optimization problems. In reality, though, being able to solve a decision problem in polynomial time will often permit us to solve the corresponding optimization problem in polynomial time (using a polynomial number of calls to the decision problem). So, discussing the difficulty of decision problems is often really equivalent to discussing the difficulty of optimization problems. For example, consider the vertex cover problem (Given a graph, find out the minimum sized vertex set that covers all edges). It is an optimization problem. Corresponding decision problem is, given undirected graph G and k, is there a vertex cover of size k?","title":"Decision vs Optimization Problems"},{"location":"algo/analysis/classes.html#reduction","text":"if A1 solves L1. i.e. x1->A1->y1 given l1 = {x1,y1} if A2 solves L2. i.e. x2->A2->y2 given l2 = {x2,y2} Transoform input x1 into x2 with funtion f such that A1 = A2(f(x1)) The idea is to find a transformation(f) from L1 to L2 so that the algorithm A2 can be part of an algorithm A1 to solve L1. Learning reduction in general is very important. if we have library functions to solve certain problem and if we can reduce a new problem to one of the solved problems, we save a lot of time. For example, Consider the example of a problem where we have to find minimum product path in a given directed graph where product of path is multiplication of weights of edges along the path. If we have code for Dijkstra\u2019s algorithm to find shortest path, we can take log of all weights and use Dijkstra\u2019s algorithm to find the minimum product path rather than writing a fresh code for this new problem.","title":"Reduction"},{"location":"algo/analysis/classes.html#how-to-prove-that-a-given-problem-is-np-complete","text":"From the definition of NP-complete, it appears impossible to prove that a problem L is NP-Complete. By definition, it requires us to that show every problem in NP is polynomial time reducible to L. Fortunately, there is an alternate way to prove it. The idea is to take a known NP-Complete problem and reduce it to L. If polynomial time reduction is possible, we can prove that L is NP-Complete by transitivity of reduction (If a NP-Complete problem is reducible to L in polynomial time, then all problems are reducible to L in polynomial time).","title":"How to prove that a given problem is NP complete?"},{"location":"algo/analysis/classes.html#first-problem-as-np-complete","text":"(SAT)Boolean satisfiability problem It is always useful to know about NP-Completeness even for engineers. Suppose you are asked to write an efficient algorithm to solve an extremely important problem for your company. After a lot of thinking, you can only come up exponential time approach which is impractical. If you don\u2019t know about NP-Completeness, you can only say that I could not come with an efficient algorithm. If you know about NP-Completeness and prove that the problem as NP-complete, you can proudly say that the polynomial time solution is unlikely to exist. If there is a polynomial time solution possible, then that solution solves a big problem of computer science many scientists have been trying for years.","title":"first problem as NP-Complete"},{"location":"algo/analysis/examples.html","text":"Examples \u00b6 void fun() { int i, j; for (i=1; i<=n; i++) for (j=1; j<=log(i); j++) printf(\"GeeksforGeeks\"); } \u0398(log 1) + \u0398(log 2) + \u0398(log 3) + . . . . + \u0398(log n) \u0398 (log n!) ~ \u0398(n log n) for large valusee of n by Stirling\u2019s approximation (or Stirling\u2019s formula). log n! = n log n - n + O(log(n)) void fun(int n) { int j = 1, i = 0; while (i < n) { // Some O(1) task i = i + j; j++; } } x(x+1)/2 after x iterations then x(x+1)/2 < n ===== \u0398(\u221an). void fun(int n, int k) { for (int i=1; i<=n; i++) { int p = pow(i, k); for (int j=1; j<=p; j++) { // Some O(1) work } } } 1k + 2k + 3k + \u2026 n1k. k=1 Sum = 1 + 2 + 3 ... n = n(n+1)/2 = n2/2 + n/2 k=2 Sum = 12 + 22 + 32 + ... n12. = n(n+1)(2n+1)/6 = n3/3 + n2/2 + n/6 k=3 Sum = 13 + 23 + 33 + ... n13. = n2(n+1)2/4 = n4/4 + n3/2 + n2/4 (nk+1)/(k+1) + \u0398(nk) \u0398(nk+1 / (k+1)) as can ignore lower terms","title":"Examples"},{"location":"algo/analysis/examples.html#examples","text":"void fun() { int i, j; for (i=1; i<=n; i++) for (j=1; j<=log(i); j++) printf(\"GeeksforGeeks\"); } \u0398(log 1) + \u0398(log 2) + \u0398(log 3) + . . . . + \u0398(log n) \u0398 (log n!) ~ \u0398(n log n) for large valusee of n by Stirling\u2019s approximation (or Stirling\u2019s formula). log n! = n log n - n + O(log(n)) void fun(int n) { int j = 1, i = 0; while (i < n) { // Some O(1) task i = i + j; j++; } } x(x+1)/2 after x iterations then x(x+1)/2 < n ===== \u0398(\u221an). void fun(int n, int k) { for (int i=1; i<=n; i++) { int p = pow(i, k); for (int j=1; j<=p; j++) { // Some O(1) work } } } 1k + 2k + 3k + \u2026 n1k. k=1 Sum = 1 + 2 + 3 ... n = n(n+1)/2 = n2/2 + n/2 k=2 Sum = 12 + 22 + 32 + ... n12. = n(n+1)(2n+1)/6 = n3/3 + n2/2 + n/6 k=3 Sum = 13 + 23 + 33 + ... n13. = n2(n+1)2/4 = n4/4 + n3/2 + n2/4 (nk+1)/(k+1) + \u0398(nk) \u0398(nk+1 / (k+1)) as can ignore lower terms","title":"Examples"},{"location":"algo/analysis/graph-coloring.html","text":"Graph coloring problem is to assign colors to certain elements of a graph subject to certain constraints. Vertex coloring \u00b6 The problem is, given m colors, find a way of coloring the vertices of a graph such that no two adjacent vertices are colored using same color. Edge Coloring and Face Coloring can be transformed into vertex coloring. Chromatic Number \u00b6 The smallest number of colors needed to color a graph G is called its chromatic number. Applications \u00b6 Making Schedule or Time Table This problem can be represented as a graph where every vertex is a subject and an edge between two vertices mean there is a common student. So this is a graph coloring problem where minimum number of time slots is equal to the chromatic number of the graph. Mobile Radio Frequency Assignment When frequencies are assigned to towers, frequencies assigned to all towers at the same location must be different. How to assign frequencies with this constraint? What is the minimum number of frequencies needed? This problem is also an instance of graph coloring problem where every tower represents a vertex and an edge between two towers represents that they are in range of each other. Sudoku Sudoku is also a variation of Graph coloring problem where every cell represents a vertex. There is an edge between two vertices if they are in same row or same column or same block. Register Allocation In compiler optimization, register allocation is the process of assigning a large number of target program variables onto a small number of CPU registers. This problem is also a graph coloring problem. Bipartite Graphs We can check if a graph is Bipartite or not by coloring the graph using two colors. If a given graph is 2-colorable, then it is Bipartite, otherwise not. ref-> https://www.geeksforgeeks.org/bipartite-graph/ Map Coloring Geographical maps of countries or states where no two adjacent cities cannot be assigned same color. Four colors are sufficient to color any map Four Color Theorem-> http://en.wikipedia.org/wiki/Four_color_theorem Updates on servers distributing content on internet Updates-> every week The update cannot be deployed on every server at the same time, because the server may have to be taken down for the install. Also, the update should not be done one at a time, because it will take a lot of time. There are sets of servers that cannot be taken down together, because they have certain critical functions. This is a typical scheduling application of graph coloring problem. It turned out that 8 colors were good enough to color the graph of 75000 nodes. So they could install updates in 8 passes. Greedy Aproach \u00b6 TODO","title":"Graph coloring"},{"location":"algo/analysis/graph-coloring.html#vertex-coloring","text":"The problem is, given m colors, find a way of coloring the vertices of a graph such that no two adjacent vertices are colored using same color. Edge Coloring and Face Coloring can be transformed into vertex coloring.","title":"Vertex coloring"},{"location":"algo/analysis/graph-coloring.html#chromatic-number","text":"The smallest number of colors needed to color a graph G is called its chromatic number.","title":"Chromatic Number"},{"location":"algo/analysis/graph-coloring.html#applications","text":"Making Schedule or Time Table This problem can be represented as a graph where every vertex is a subject and an edge between two vertices mean there is a common student. So this is a graph coloring problem where minimum number of time slots is equal to the chromatic number of the graph. Mobile Radio Frequency Assignment When frequencies are assigned to towers, frequencies assigned to all towers at the same location must be different. How to assign frequencies with this constraint? What is the minimum number of frequencies needed? This problem is also an instance of graph coloring problem where every tower represents a vertex and an edge between two towers represents that they are in range of each other. Sudoku Sudoku is also a variation of Graph coloring problem where every cell represents a vertex. There is an edge between two vertices if they are in same row or same column or same block. Register Allocation In compiler optimization, register allocation is the process of assigning a large number of target program variables onto a small number of CPU registers. This problem is also a graph coloring problem. Bipartite Graphs We can check if a graph is Bipartite or not by coloring the graph using two colors. If a given graph is 2-colorable, then it is Bipartite, otherwise not. ref-> https://www.geeksforgeeks.org/bipartite-graph/ Map Coloring Geographical maps of countries or states where no two adjacent cities cannot be assigned same color. Four colors are sufficient to color any map Four Color Theorem-> http://en.wikipedia.org/wiki/Four_color_theorem Updates on servers distributing content on internet Updates-> every week The update cannot be deployed on every server at the same time, because the server may have to be taken down for the install. Also, the update should not be done one at a time, because it will take a lot of time. There are sets of servers that cannot be taken down together, because they have certain critical functions. This is a typical scheduling application of graph coloring problem. It turned out that 8 colors were good enough to color the graph of 75000 nodes. So they could install updates in 8 passes.","title":"Applications"},{"location":"algo/analysis/graph-coloring.html#greedy-aproach","text":"TODO","title":"Greedy Aproach"},{"location":"algo/analysis/intro.html","text":"Algorithm Analysis \u00b6 Why \u00b6 Care about user friendliness, modularity, security, maintainability, etc? turns out you cant do that without performance. So performance is like currency through which we can buy all the above things performance == scale. Why not just run it? (see running time) \u00b6 Different machines -> Different comparison. Different input -> Different comparison. Big Idea -> Asymptotic analysis (just depends on input) Linear Search running time in seconds on A: 0.2 * n Binary Search running time in seconds on B: 1000*log(n) We ignore constants in Asymptotic Analysis. Also, in Asymptotic analysis, we always talk about input sizes larger than a constant value. So, you may end up choosing an algorithm that is Asymptotically slower but faster for your software. Worst Case \u00b6 Upper bound of running -> should know when. \u0398(n) Average Case (Rarely) \u00b6 For the linear search problem, let us assume that all cases are uniformly distributed (including the case of x not being present in array). So we sum all the cases and divide the sum by (n+1). Following is the value of average case time complexity. Best Case Analysis (Bogus) \u00b6 \u0398(1) Why not average? \u00b6 The average case analysis is not easy to do in most of the practical cases and it is rarely done. In the average case analysis, we must know (or predict) the mathematical distribution of all possible inputs. For some algorithms, all the cases are asymptotically same, i.e., there are no worst and best cases. eg Merge sort - > \u0398(nLogn) Most of the other sorting algorithms have worst and best cases. For example, in the typical implementation of Quick Sort (where pivot is chosen as a corner element), the worst occurs when the input array is already sorted and the best occur when the pivot elements always divide array in two halves. For insertion sort, the worst case occurs when the array is reverse sorted and the best case occurs when the array is sorted in the same order as output. The main idea of asymptotic analysis is to * have a measure of efficiency of algorithms that doesn\u2019t depend on machine specific constants, * and doesn\u2019t require algorithms to be implemented and time taken by programs to be compared. Notations \u00b6 \u0398(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0} The theta notation bounds a functions from above and below, so it defines exact asymptotic behavior. A simple way to get Theta notation of an expression is to drop low order terms and ignore leading constants f(n) must be non-negative for values of n greater than n0. Why drop? \u00b6 its fine chill. O(g(n)) = { f(n): there exist positive constants c and n0 such that 0 <= f(n) <= c*g(n) for all n >= n0} Big O notation defines an upper bound of an algorithm, it bounds a function only from above O(n^2) also covers linear time. -> quick sort \u03a9 (g(n)) = {f(n): there exist positive constants c and n0 such that 0 <= c*g(n) <= f(n) for all n >= n0}. \u03a9 notation provides an asymptotic lower bound. Properties \u00b6 General Properties \u00b6 If f(n) is O(g(n)) then a f(n) is also O(g(n)) ; where a is a constant. If f(n) is \u0398(g(n)) then a f(n) is also \u0398(g(n)) ; where a is a constant. If f(n) is \u03a9 (g(n)) then a*f(n) is also \u03a9 (g(n)) ; where a is a constant. Reflexive Properties \u00b6 If f(n) is given then f(n) is O(f(n)). If f(n) is given then f(n) is \u0398(f(n)). If f(n) is given then f(n) is \u03a9 (f(n)). Transitive Properties \u00b6 If f(n) is O(g(n)) and g(n) is O(h(n)) then f(n) = O(h(n)) . If f(n) is \u0398(g(n)) and g(n) is \u0398(h(n)) then f(n) = \u0398(h(n)) . If f(n) is \u03a9 (g(n)) and g(n) is \u03a9 (h(n)) then f(n) = \u03a9 (h(n)) Symmetric Properties \u00b6 If f(n) is \u0398(g(n)) then g(n) is \u0398(f(n)) . This property only satisfies for \u0398 notation. Transpose Symmetric Properties \u00b6 If f(n) is O(g(n)) then g(n) is \u03a9 (f(n)). This property only satisfies for O and \u03a9 notations. Some More Properties \u00b6 If f(n) = O(g(n)) and f(n) = \u03a9(g(n)) then f(n) = \u0398(g(n)) If f(n) = O(g(n)) and d(n)=O(e(n)) then f(n) + d(n) = O( max( g(n), e(n) )) Example: f(n) = n i.e O(n) d(n) = n\u00b2 i.e O(n\u00b2) then f(n) + d(n) = n + n\u00b2 i.e O(n\u00b2) If f(n)=O(g(n)) and d(n)=O(e(n)) then f(n) * d(n) = O( g(n) * e(n) ) Example: f(n) = n i.e O(n) d(n) = n\u00b2 i.e O(n\u00b2) then f(n) * d(n) = n * n\u00b2 = n\u00b3 i.e O(n\u00b3) Time complexity of all computer algorithms can be written as \u03a9(1) Little o provides strict upper bound (equality condition is removed from Big O) and little omega provides strict lower bound (equality condition removed from big omega) Little \u03bf asymptotic notation \u00b6 Let f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is \u03bf(g(n)) (or f(n) \u0395 \u03bf(g(n))) if for any real constant c > 0, there exists an integer constant n0 \u2265 1 such that 0 \u2264 f(n) < c*g(n). Thus, little o() means loose upper-bound of f(n). Little o is a rough estimate of the maximum order of growth whereas Big-\u039f may be the actual order of growth. In mathematical relation, f(n) = o(g(n)) means lim f(n)/g(n) = 0 n\u2192\u221e lim f(n)/g(n) = lim (7n + 8)/(n2) = lim 7/2n = 0 (l\u2019hospital) n\u2192\u221e n\u2192\u221e n\u2192\u221e hence 7n + 8 \u2208 o(n2) Little \u03c9 asymptotic notation \u00b6 Let f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is \u03c9(g(n)) (or f(n) \u2208 \u03c9(g(n))) if for any real constant c > 0, there exists an integer constant n0 \u2265 1 such that f(n) > c * g(n) \u2265 0 for every integer n \u2265 n0. Little Omega (\u03c9) is a rough estimate of the order of the growth whereas Big Omega (\u03a9) may represent exact order of growth. We use \u03c9 notation to denote a lower bound that is not asymptotically tight. And, f(n) \u2208 \u03c9(g(n)) if and only if g(n) \u2208 \u03bf((f(n)). if f(n) \u2208 \u03c9(g(n)) then, lim f(n)/g(n) = \u221e n\u2192\u221e Prove that 4n + 6 \u2208 \u03c9(1);","title":"Algorithm Analysis"},{"location":"algo/analysis/intro.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"algo/analysis/intro.html#why","text":"Care about user friendliness, modularity, security, maintainability, etc? turns out you cant do that without performance. So performance is like currency through which we can buy all the above things performance == scale.","title":"Why"},{"location":"algo/analysis/intro.html#why-not-just-run-it-see-running-time","text":"Different machines -> Different comparison. Different input -> Different comparison. Big Idea -> Asymptotic analysis (just depends on input) Linear Search running time in seconds on A: 0.2 * n Binary Search running time in seconds on B: 1000*log(n) We ignore constants in Asymptotic Analysis. Also, in Asymptotic analysis, we always talk about input sizes larger than a constant value. So, you may end up choosing an algorithm that is Asymptotically slower but faster for your software.","title":"Why not just run it? (see running time)"},{"location":"algo/analysis/intro.html#worst-case","text":"Upper bound of running -> should know when. \u0398(n)","title":"Worst Case"},{"location":"algo/analysis/intro.html#average-case-rarely","text":"For the linear search problem, let us assume that all cases are uniformly distributed (including the case of x not being present in array). So we sum all the cases and divide the sum by (n+1). Following is the value of average case time complexity.","title":"Average Case (Rarely)"},{"location":"algo/analysis/intro.html#best-case-analysis-bogus","text":"\u0398(1)","title":"Best Case Analysis (Bogus)"},{"location":"algo/analysis/intro.html#why-not-average","text":"The average case analysis is not easy to do in most of the practical cases and it is rarely done. In the average case analysis, we must know (or predict) the mathematical distribution of all possible inputs. For some algorithms, all the cases are asymptotically same, i.e., there are no worst and best cases. eg Merge sort - > \u0398(nLogn) Most of the other sorting algorithms have worst and best cases. For example, in the typical implementation of Quick Sort (where pivot is chosen as a corner element), the worst occurs when the input array is already sorted and the best occur when the pivot elements always divide array in two halves. For insertion sort, the worst case occurs when the array is reverse sorted and the best case occurs when the array is sorted in the same order as output. The main idea of asymptotic analysis is to * have a measure of efficiency of algorithms that doesn\u2019t depend on machine specific constants, * and doesn\u2019t require algorithms to be implemented and time taken by programs to be compared.","title":"Why not average?"},{"location":"algo/analysis/intro.html#notations","text":"\u0398(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0} The theta notation bounds a functions from above and below, so it defines exact asymptotic behavior. A simple way to get Theta notation of an expression is to drop low order terms and ignore leading constants f(n) must be non-negative for values of n greater than n0.","title":"Notations"},{"location":"algo/analysis/intro.html#why-drop","text":"its fine chill. O(g(n)) = { f(n): there exist positive constants c and n0 such that 0 <= f(n) <= c*g(n) for all n >= n0} Big O notation defines an upper bound of an algorithm, it bounds a function only from above O(n^2) also covers linear time. -> quick sort \u03a9 (g(n)) = {f(n): there exist positive constants c and n0 such that 0 <= c*g(n) <= f(n) for all n >= n0}. \u03a9 notation provides an asymptotic lower bound.","title":"Why drop?"},{"location":"algo/analysis/intro.html#properties","text":"","title":"Properties"},{"location":"algo/analysis/intro.html#general-properties","text":"If f(n) is O(g(n)) then a f(n) is also O(g(n)) ; where a is a constant. If f(n) is \u0398(g(n)) then a f(n) is also \u0398(g(n)) ; where a is a constant. If f(n) is \u03a9 (g(n)) then a*f(n) is also \u03a9 (g(n)) ; where a is a constant.","title":"General Properties"},{"location":"algo/analysis/intro.html#reflexive-properties","text":"If f(n) is given then f(n) is O(f(n)). If f(n) is given then f(n) is \u0398(f(n)). If f(n) is given then f(n) is \u03a9 (f(n)).","title":"Reflexive Properties"},{"location":"algo/analysis/intro.html#transitive-properties","text":"If f(n) is O(g(n)) and g(n) is O(h(n)) then f(n) = O(h(n)) . If f(n) is \u0398(g(n)) and g(n) is \u0398(h(n)) then f(n) = \u0398(h(n)) . If f(n) is \u03a9 (g(n)) and g(n) is \u03a9 (h(n)) then f(n) = \u03a9 (h(n))","title":"Transitive Properties"},{"location":"algo/analysis/intro.html#symmetric-properties","text":"If f(n) is \u0398(g(n)) then g(n) is \u0398(f(n)) . This property only satisfies for \u0398 notation.","title":"Symmetric Properties"},{"location":"algo/analysis/intro.html#transpose-symmetric-properties","text":"If f(n) is O(g(n)) then g(n) is \u03a9 (f(n)). This property only satisfies for O and \u03a9 notations.","title":"Transpose Symmetric Properties"},{"location":"algo/analysis/intro.html#some-more-properties","text":"If f(n) = O(g(n)) and f(n) = \u03a9(g(n)) then f(n) = \u0398(g(n)) If f(n) = O(g(n)) and d(n)=O(e(n)) then f(n) + d(n) = O( max( g(n), e(n) )) Example: f(n) = n i.e O(n) d(n) = n\u00b2 i.e O(n\u00b2) then f(n) + d(n) = n + n\u00b2 i.e O(n\u00b2) If f(n)=O(g(n)) and d(n)=O(e(n)) then f(n) * d(n) = O( g(n) * e(n) ) Example: f(n) = n i.e O(n) d(n) = n\u00b2 i.e O(n\u00b2) then f(n) * d(n) = n * n\u00b2 = n\u00b3 i.e O(n\u00b3) Time complexity of all computer algorithms can be written as \u03a9(1) Little o provides strict upper bound (equality condition is removed from Big O) and little omega provides strict lower bound (equality condition removed from big omega)","title":"Some More Properties"},{"location":"algo/analysis/intro.html#little-asymptotic-notation","text":"Let f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is \u03bf(g(n)) (or f(n) \u0395 \u03bf(g(n))) if for any real constant c > 0, there exists an integer constant n0 \u2265 1 such that 0 \u2264 f(n) < c*g(n). Thus, little o() means loose upper-bound of f(n). Little o is a rough estimate of the maximum order of growth whereas Big-\u039f may be the actual order of growth. In mathematical relation, f(n) = o(g(n)) means lim f(n)/g(n) = 0 n\u2192\u221e lim f(n)/g(n) = lim (7n + 8)/(n2) = lim 7/2n = 0 (l\u2019hospital) n\u2192\u221e n\u2192\u221e n\u2192\u221e hence 7n + 8 \u2208 o(n2)","title":"Little \u03bf asymptotic notation"},{"location":"algo/analysis/intro.html#little-asymptotic-notation_1","text":"Let f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is \u03c9(g(n)) (or f(n) \u2208 \u03c9(g(n))) if for any real constant c > 0, there exists an integer constant n0 \u2265 1 such that f(n) > c * g(n) \u2265 0 for every integer n \u2265 n0. Little Omega (\u03c9) is a rough estimate of the order of the growth whereas Big Omega (\u03a9) may represent exact order of growth. We use \u03c9 notation to denote a lower bound that is not asymptotically tight. And, f(n) \u2208 \u03c9(g(n)) if and only if g(n) \u2208 \u03bf((f(n)). if f(n) \u2208 \u03c9(g(n)) then, lim f(n)/g(n) = \u221e n\u2192\u221e Prove that 4n + 6 \u2208 \u03c9(1);","title":"Little \u03c9 asymptotic notation"},{"location":"algo/analysis/lower-upper-bound-theory.html","text":"Lower and Upper Bound Theory \u00b6 Lower Bound \u00b6 Let L(n) be the running time of an algorithm A(say), then g(n) is the Lower Bound of A if there exist two constants C and N such that L(n) >= C*g(n) for n > N. Lower bound of an algorithm is shown by the asymptotic notation called Big Omega (or just Omega). Upper Bound \u00b6 Let U(n) be the running time of an algorithm A(say), then g(n) is the Upper Bound of A if there exist two constants C and N such that U(n) <= C*g(n) for n > N. Upper bound of an algorithm is shown by the asymptotic notation called Big Oh(O) (or just Oh). Lower Bound Theory \u00b6 According to the lower bound theory, for a lower bound L(n) of an algorithm, it is not possible to have any other algorithm (for a common problem) whose time complexity is less than L(n) for random input. Also every algorithm must take at least L(n) time in worst case. Note that L(n) here is the minimum of all the possible algorithm, of maximum complexity. The Lower Bound is a very important for any algorithm. Once we calculated it, then we can compare it with the actual complexity of the algorithm and if their order are same then we can declare our algorithm as optimal. So in this section we will be discussing about techniques for finding the lower bound of an algorithm. Note that our main motive is to get an optimal algorithm, which is the one having its Upper Bound Same as its Lower Bound (U(n)=L(n)). Merge Sort is a common example of an optimal algorithm. Trivial Lower Bound \u00b6 It is the easiest method to find the lower bound. The Lower bounds which can be easily observed on the basis of the number of input taken and the number of output produces are called Trivial Lower Bound. Multiplication of n x n matrix Input: For 2 matrix we will have 2n2 inputs Output: 1 matrix of order n x n, i.e., n2 outputs In the above example its easily predictable that the lower bound is O(n2). Computational Model The method is for all those algorithms that are comparison based. For example in sorting we have to compare the elements of the list among themselves and then sort them accordingly. Similar is the case with searching and thus we can implement the same in this case. Now we will look at some examples to understand its usage. Ordered Searching It is a type of searching in which the list is already sorted. Linear search In linear search we compare the key with first element if it does not match we compare with second element and so on till we check against the nth element. Else we will end up with a failure. Binary search In binary search, we check the middle element against the key, if it is greater we search the first half else we check the second half and repeat the same process. The diagram below there is an illustration of binary search in an array consisting of 4 elements Calculating the lower bound : The max no of comparisons are n. Let there be k levels in the tree. No. of nodes will be 2k-1 The upper bound of no of nodes in any comparison based search of an element in list of size n will be n as there are maximum of n comparisons in worst case scenario 2k-1 Each level will take 1 comparison thus no. of comparisons k\u2265|log2n| Thus the lower bound of any comparison based search from a list of n elements cannot be less than log(n). Therefore we can say that Binary Search is optimal as its complexity is \u0398(log n). Sorting The diagram below is an example of tree formed in sorting combinations with 3 elements. For n elements, finding lower bound using computation model For n elements we have a total on n! combinations (leaf nodes). (Refer the diagram the total combinations are 3! or 6) also it is clear that the tree formed is a binary tree. Each level in the diagram indicates a comparison. Let there be k levels => 2k is the total number of leaf nodes in a full binary tree thus in this case we have n!\u22642k. As the k in the above example is the no of comparisons thus by computational model lower bond = k. n!\u22642T(n) Thus, T(n)>|log n!| => n!<=nn Thus, log n!<=log nn Taking ceiling function on both sides, we get |-log nn-|>=|-log n!-| Thus complexity becomes \u0398(lognn) or \u0398(nlogn) Using Lower bond theory to solve algebraic problem: Straight Line Program The type of programs build without any loops or control structures is called Straight Line Program. For example, summing to nos Sum(a, b) { //no loops and no control structures c:= a+b; return c; } Algebraic Problem Problems related to algebra like solving equations inequalities etc., comes under algebraic problems. For example, solving equation ax2+bx+c with simple programming. Algo_Sol(a, b, c, x) { //1 assignment v:=a*x; //1 assignment v:=v+b; //1 assignment v:=v*x; //1 assignment ans:=v+c; return ans; } Complexity for solving here is 4 (excluding the returning). The above example shows us a simple way to solve an equation for 2 degree polynomial i.e., 4 thus for nth degree polynomial we will have complexity of O(n2). Let us demonstrate via an algorithm. anxn+an-1xn-1+an-2xn-2+\u2026+a1x+a0 is a polynomial of degree n. pow(x, n) { p := 1; //loop from 1 to n for i:=1 to n p := p*x; return p; } polynomial(A, x, n) { int p, v:=0; for i := 0 to n //loop within a loop from 0 to n v := v + A[i]*pow(x, i); return v; } Loop within a loop => complexity = O(n^2); Now to find an optimal algorithm we need to find the lower bound here (as per lower bound theory). As per Lower Bound Theory, The optimal algorithm to solve the above problem is the one having complexity O(n). Lets prove this theorem using lower bounds. To prove that optimal algo of solving a n degree polynomial is O(n) Proof: The best solution for reducing the algo is to make this problem less complex by dividing the polynomial into several straight line problems. => anxn+an-1xn-1+an-2xn-2+...+a1x+a0 can be written as, ((..(anx+an-1)x+..+a2)x+a1)x+a0 Now, algorithm will be as, v=0 v=v+an v=v*x v=v+an-1 v=v*x v=v+a1 v=v*x v=v+a0 polynomial(A, x, n) { int p, v=0; // loop executed n times for i = n to 0 v = (v + A[i])*x; return v; } Clearly, the complexity of this code is O(n). This way of solving such equations is called Horner\u2019s method. Here is were lower bound theory works and give the optimum algorithm\u2019s complexity as O(n). Upper Bound Theory \u00b6 According to the upper bound theory, for an upper bound U(n) of an algorithm, we can always solve the problem in at most U(n) time.Time taken by a known algorithm to solve a problem with worse case input gives us the upper bound. https://www.geeksforgeeks.org/lower-and-upper-bound-theory/ Lower Bound \u00b6 L(n) >= C*g(n) for n > N. Big Omega L(n) here is the minimum of all the possible algorithm, of maximum complexity. worst case no algo below it optimal Algo -> (U(n)=L(n)). Trivial Lower Bound Computational Model Calculating the lower bound: The max no of comparisons are n. Let there be k levels in the tree. No. of nodes will be 2k-1 The upper bound of no of nodes in any comparison based search of an element in list of size n will be n as there are maximum of n comparisons in worst case scenario 2k-1 Each level will take 1 comparison thus no. of comparisons k\u2265|log2n| Thus the lower bound of any comparison based search from a list of n elements cannot be less than log(n). Therefore we can say that Binary Search is optimal as its complexity is \u0398(log n). Upper Bound \u00b6 U(n) <= C*g(n) for n > N. Big oh.","title":"Lower and Upper Bound Theory"},{"location":"algo/analysis/lower-upper-bound-theory.html#lower-and-upper-bound-theory","text":"","title":"Lower and Upper Bound Theory"},{"location":"algo/analysis/lower-upper-bound-theory.html#lower-bound","text":"Let L(n) be the running time of an algorithm A(say), then g(n) is the Lower Bound of A if there exist two constants C and N such that L(n) >= C*g(n) for n > N. Lower bound of an algorithm is shown by the asymptotic notation called Big Omega (or just Omega).","title":"Lower Bound"},{"location":"algo/analysis/lower-upper-bound-theory.html#upper-bound","text":"Let U(n) be the running time of an algorithm A(say), then g(n) is the Upper Bound of A if there exist two constants C and N such that U(n) <= C*g(n) for n > N. Upper bound of an algorithm is shown by the asymptotic notation called Big Oh(O) (or just Oh).","title":"Upper Bound"},{"location":"algo/analysis/lower-upper-bound-theory.html#lower-bound-theory","text":"According to the lower bound theory, for a lower bound L(n) of an algorithm, it is not possible to have any other algorithm (for a common problem) whose time complexity is less than L(n) for random input. Also every algorithm must take at least L(n) time in worst case. Note that L(n) here is the minimum of all the possible algorithm, of maximum complexity. The Lower Bound is a very important for any algorithm. Once we calculated it, then we can compare it with the actual complexity of the algorithm and if their order are same then we can declare our algorithm as optimal. So in this section we will be discussing about techniques for finding the lower bound of an algorithm. Note that our main motive is to get an optimal algorithm, which is the one having its Upper Bound Same as its Lower Bound (U(n)=L(n)). Merge Sort is a common example of an optimal algorithm.","title":"Lower Bound Theory"},{"location":"algo/analysis/lower-upper-bound-theory.html#trivial-lower-bound","text":"It is the easiest method to find the lower bound. The Lower bounds which can be easily observed on the basis of the number of input taken and the number of output produces are called Trivial Lower Bound. Multiplication of n x n matrix Input: For 2 matrix we will have 2n2 inputs Output: 1 matrix of order n x n, i.e., n2 outputs In the above example its easily predictable that the lower bound is O(n2). Computational Model The method is for all those algorithms that are comparison based. For example in sorting we have to compare the elements of the list among themselves and then sort them accordingly. Similar is the case with searching and thus we can implement the same in this case. Now we will look at some examples to understand its usage. Ordered Searching It is a type of searching in which the list is already sorted. Linear search In linear search we compare the key with first element if it does not match we compare with second element and so on till we check against the nth element. Else we will end up with a failure. Binary search In binary search, we check the middle element against the key, if it is greater we search the first half else we check the second half and repeat the same process. The diagram below there is an illustration of binary search in an array consisting of 4 elements Calculating the lower bound : The max no of comparisons are n. Let there be k levels in the tree. No. of nodes will be 2k-1 The upper bound of no of nodes in any comparison based search of an element in list of size n will be n as there are maximum of n comparisons in worst case scenario 2k-1 Each level will take 1 comparison thus no. of comparisons k\u2265|log2n| Thus the lower bound of any comparison based search from a list of n elements cannot be less than log(n). Therefore we can say that Binary Search is optimal as its complexity is \u0398(log n). Sorting The diagram below is an example of tree formed in sorting combinations with 3 elements. For n elements, finding lower bound using computation model For n elements we have a total on n! combinations (leaf nodes). (Refer the diagram the total combinations are 3! or 6) also it is clear that the tree formed is a binary tree. Each level in the diagram indicates a comparison. Let there be k levels => 2k is the total number of leaf nodes in a full binary tree thus in this case we have n!\u22642k. As the k in the above example is the no of comparisons thus by computational model lower bond = k. n!\u22642T(n) Thus, T(n)>|log n!| => n!<=nn Thus, log n!<=log nn Taking ceiling function on both sides, we get |-log nn-|>=|-log n!-| Thus complexity becomes \u0398(lognn) or \u0398(nlogn) Using Lower bond theory to solve algebraic problem: Straight Line Program The type of programs build without any loops or control structures is called Straight Line Program. For example, summing to nos Sum(a, b) { //no loops and no control structures c:= a+b; return c; } Algebraic Problem Problems related to algebra like solving equations inequalities etc., comes under algebraic problems. For example, solving equation ax2+bx+c with simple programming. Algo_Sol(a, b, c, x) { //1 assignment v:=a*x; //1 assignment v:=v+b; //1 assignment v:=v*x; //1 assignment ans:=v+c; return ans; } Complexity for solving here is 4 (excluding the returning). The above example shows us a simple way to solve an equation for 2 degree polynomial i.e., 4 thus for nth degree polynomial we will have complexity of O(n2). Let us demonstrate via an algorithm. anxn+an-1xn-1+an-2xn-2+\u2026+a1x+a0 is a polynomial of degree n. pow(x, n) { p := 1; //loop from 1 to n for i:=1 to n p := p*x; return p; } polynomial(A, x, n) { int p, v:=0; for i := 0 to n //loop within a loop from 0 to n v := v + A[i]*pow(x, i); return v; } Loop within a loop => complexity = O(n^2); Now to find an optimal algorithm we need to find the lower bound here (as per lower bound theory). As per Lower Bound Theory, The optimal algorithm to solve the above problem is the one having complexity O(n). Lets prove this theorem using lower bounds. To prove that optimal algo of solving a n degree polynomial is O(n) Proof: The best solution for reducing the algo is to make this problem less complex by dividing the polynomial into several straight line problems. => anxn+an-1xn-1+an-2xn-2+...+a1x+a0 can be written as, ((..(anx+an-1)x+..+a2)x+a1)x+a0 Now, algorithm will be as, v=0 v=v+an v=v*x v=v+an-1 v=v*x v=v+a1 v=v*x v=v+a0 polynomial(A, x, n) { int p, v=0; // loop executed n times for i = n to 0 v = (v + A[i])*x; return v; } Clearly, the complexity of this code is O(n). This way of solving such equations is called Horner\u2019s method. Here is were lower bound theory works and give the optimum algorithm\u2019s complexity as O(n).","title":"Trivial Lower Bound"},{"location":"algo/analysis/lower-upper-bound-theory.html#upper-bound-theory","text":"According to the upper bound theory, for an upper bound U(n) of an algorithm, we can always solve the problem in at most U(n) time.Time taken by a known algorithm to solve a problem with worse case input gives us the upper bound. https://www.geeksforgeeks.org/lower-and-upper-bound-theory/","title":"Upper Bound Theory"},{"location":"algo/analysis/lower-upper-bound-theory.html#lower-bound_1","text":"L(n) >= C*g(n) for n > N. Big Omega L(n) here is the minimum of all the possible algorithm, of maximum complexity. worst case no algo below it optimal Algo -> (U(n)=L(n)). Trivial Lower Bound Computational Model Calculating the lower bound: The max no of comparisons are n. Let there be k levels in the tree. No. of nodes will be 2k-1 The upper bound of no of nodes in any comparison based search of an element in list of size n will be n as there are maximum of n comparisons in worst case scenario 2k-1 Each level will take 1 comparison thus no. of comparisons k\u2265|log2n| Thus the lower bound of any comparison based search from a list of n elements cannot be less than log(n). Therefore we can say that Binary Search is optimal as its complexity is \u0398(log n).","title":"Lower Bound"},{"location":"algo/analysis/lower-upper-bound-theory.html#upper-bound_1","text":"U(n) <= C*g(n) for n > N. Big oh.","title":"Upper Bound"},{"location":"algo/analysis/master-theorum.html","text":"Master's Theorum \u00b6 Master Method is a direct way to get the solution. The master method works only for following type of recurrences or for recurrences that can be transformed to following type. T(n) = aT(n/b) + f(n) where a >= 1 and b > 1 There are following three cases: If f(n) = \u0398(nc) where c < Logba then T(n) = \u0398(nLogba) If f(n) = \u0398(nc) where c = Logba then T(n) = \u0398(ncLog n) If f(n) = \u0398(nc) where c > Logba then T(n) = \u0398(f(n)) How does this work? \u00b6 Master method is mainly derived from recurrence tree method. If we draw recurrence tree of T(n) = aT(n/b) + f(n), we can see that the work done at root is f(n) and work done at all leaves is \u0398(nc) where c is Logba. And the height of recurrence tree is Logbn Master Theorem In recurrence tree method, we calculate total work done. If the work done at leaves is polynomially more, then leaves are the dominant part, and our result becomes the work done at leaves (Case 1). If work done at leaves and root is asymptotically same, then our result becomes height multiplied by work done at any level (Case 2). If work done at root is asymptotically more, then our result becomes work done at root (Case 3). Examples of some standard algorithms whose time complexity can be evaluated using Master Method Merge Sort: T(n) = 2T(n/2) + \u0398(n). It falls in case 2 as c is 1 and Logba] is also 1. So the solution is \u0398(n Logn) Binary Search: T(n) = T(n/2) + \u0398(1). It also falls in case 2 as c is 0 and Logba is also 0. So the solution is \u0398(Logn) Notes: 1. It is not necessary that a recurrence of the form T(n) = aT(n/b) + f(n) can be solved using Master Theorem. The given three cases have some gaps between them. For example, the recurrence T(n) = 2T(n/2) + n/Logn cannot be solved using master method. 2. Case 2 can be extended for f(n) = \u0398(ncLogkn). If f(n) = \u0398(ncLogkn) for some constant k >= 0 and c = Logba, then T(n) = \u0398(ncLogk+1n) http://www.csd.uwo.ca/~moreno//CS424/Ressources/master.pdf TODO","title":"Master's Theorum"},{"location":"algo/analysis/master-theorum.html#masters-theorum","text":"Master Method is a direct way to get the solution. The master method works only for following type of recurrences or for recurrences that can be transformed to following type. T(n) = aT(n/b) + f(n) where a >= 1 and b > 1 There are following three cases: If f(n) = \u0398(nc) where c < Logba then T(n) = \u0398(nLogba) If f(n) = \u0398(nc) where c = Logba then T(n) = \u0398(ncLog n) If f(n) = \u0398(nc) where c > Logba then T(n) = \u0398(f(n))","title":"Master's Theorum"},{"location":"algo/analysis/master-theorum.html#how-does-this-work","text":"Master method is mainly derived from recurrence tree method. If we draw recurrence tree of T(n) = aT(n/b) + f(n), we can see that the work done at root is f(n) and work done at all leaves is \u0398(nc) where c is Logba. And the height of recurrence tree is Logbn Master Theorem In recurrence tree method, we calculate total work done. If the work done at leaves is polynomially more, then leaves are the dominant part, and our result becomes the work done at leaves (Case 1). If work done at leaves and root is asymptotically same, then our result becomes height multiplied by work done at any level (Case 2). If work done at root is asymptotically more, then our result becomes work done at root (Case 3). Examples of some standard algorithms whose time complexity can be evaluated using Master Method Merge Sort: T(n) = 2T(n/2) + \u0398(n). It falls in case 2 as c is 1 and Logba] is also 1. So the solution is \u0398(n Logn) Binary Search: T(n) = T(n/2) + \u0398(1). It also falls in case 2 as c is 0 and Logba is also 0. So the solution is \u0398(Logn) Notes: 1. It is not necessary that a recurrence of the form T(n) = aT(n/b) + f(n) can be solved using Master Theorem. The given three cases have some gaps between them. For example, the recurrence T(n) = 2T(n/2) + n/Logn cannot be solved using master method. 2. Case 2 can be extended for f(n) = \u0398(ncLogkn). If f(n) = \u0398(ncLogkn) for some constant k >= 0 and c = Logba, then T(n) = \u0398(ncLogk+1n) http://www.csd.uwo.ca/~moreno//CS424/Ressources/master.pdf TODO","title":"How does this work?"},{"location":"algo/analysis/pseudo-polynomial.html","text":"Pseudo-polynomial \u00b6 An algorithm whose worst case time complexity depends on numeric value of input (not number of inputs) is called Pseudo-polynomial algorithm. For example, consider the problem of counting frequencies of all elements in an array of positive numbers. A pseudo-polynomial time solution for this is to first find the maximum value, then iterate from 1 to maximum value and for each value, find its frequency in array. This solution requires time according to maximum value in input array, therefore pseudo-polynomial. On the other hand, an algorithm whose time complexity is only based on number of elements in array (not value) is considered as polynomial time algorithm. Pseudo-polynomial and NP-Completeness \u00b6 Some NP-Complete problems have Pseudo Polynomial time solutions. For example, Dynamic Programming Solutions of 0-1 Knapsack, Subset-Sum and Partition problems are Pseudo-Polynomial. NP complete problems that can be solved using a pseudo-polynomial time algorithms are called weakly NP-complete.","title":"Pseudo-polynomial"},{"location":"algo/analysis/pseudo-polynomial.html#pseudo-polynomial","text":"An algorithm whose worst case time complexity depends on numeric value of input (not number of inputs) is called Pseudo-polynomial algorithm. For example, consider the problem of counting frequencies of all elements in an array of positive numbers. A pseudo-polynomial time solution for this is to first find the maximum value, then iterate from 1 to maximum value and for each value, find its frequency in array. This solution requires time according to maximum value in input array, therefore pseudo-polynomial. On the other hand, an algorithm whose time complexity is only based on number of elements in array (not value) is considered as polynomial time algorithm.","title":"Pseudo-polynomial"},{"location":"algo/analysis/pseudo-polynomial.html#pseudo-polynomial-and-np-completeness","text":"Some NP-Complete problems have Pseudo Polynomial time solutions. For example, Dynamic Programming Solutions of 0-1 Knapsack, Subset-Sum and Partition problems are Pseudo-Polynomial. NP complete problems that can be solved using a pseudo-polynomial time algorithms are called weakly NP-complete.","title":"Pseudo-polynomial and NP-Completeness"},{"location":"algo/analysis/recursive.html","text":"Recursive \u00b6 How to calculate time complexity of recursive functions? Time complexity of a recursive function can be written as a mathematical recurrence relation. To calculate time complexity, we must know how to solve recurrences. We will soon be discussing recurrence solving techniques as a separate post. T(n) = 2T(n/2) + cn (Merge Sort) Substitution Method \u00b6 We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect. For example consider the recurrence T(n) = 2T(n/2) + n We guess the solution as T(n) = O(nLogn). Now we use induction to prove our guess. We need to prove that T(n) <= cnLogn. We can assume that it is true for values smaller than n. T(n) = 2T(n/2) + n <= 2cn/2Log(n/2) + n = cnLogn - cnLog2 + n = cnLogn - cn + n <= cnLogn Recurrence Tree Method \u00b6 In this method, we draw a recurrence tree and calculate the time taken by every level of tree. Finally, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. The pattern is typically a arithmetic or geometric series. For example consider the recurrence relation T(n) = T(n/4) + T(n/2) + cn2 cn2 / \\ T(n/4) T(n/2) If we further break down the expression T(n/4) and T(n/2), we get following recursion tree. cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ T(n/16) T(n/8) T(n/8) T(n/4) Breaking down further gives us following cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ c(n2)/256 c(n2)/64 c(n2)/64 c(n2)/16 / \\ / \\ / \\ / \\ To know the value of T(n), we need to calculate sum of tree nodes level by level. If we sum the above tree level by level, we get the following series T(n) = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + .... The above series is geometrical progression with ratio 5/16. To get an upper bound, we can sum the infinite series. We get the sum as (n2)/(1 - 5/16) which is O(n2)","title":"Recursive"},{"location":"algo/analysis/recursive.html#recursive","text":"How to calculate time complexity of recursive functions? Time complexity of a recursive function can be written as a mathematical recurrence relation. To calculate time complexity, we must know how to solve recurrences. We will soon be discussing recurrence solving techniques as a separate post. T(n) = 2T(n/2) + cn (Merge Sort)","title":"Recursive"},{"location":"algo/analysis/recursive.html#substitution-method","text":"We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect. For example consider the recurrence T(n) = 2T(n/2) + n We guess the solution as T(n) = O(nLogn). Now we use induction to prove our guess. We need to prove that T(n) <= cnLogn. We can assume that it is true for values smaller than n. T(n) = 2T(n/2) + n <= 2cn/2Log(n/2) + n = cnLogn - cnLog2 + n = cnLogn - cn + n <= cnLogn","title":"Substitution Method"},{"location":"algo/analysis/recursive.html#recurrence-tree-method","text":"In this method, we draw a recurrence tree and calculate the time taken by every level of tree. Finally, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. The pattern is typically a arithmetic or geometric series. For example consider the recurrence relation T(n) = T(n/4) + T(n/2) + cn2 cn2 / \\ T(n/4) T(n/2) If we further break down the expression T(n/4) and T(n/2), we get following recursion tree. cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ T(n/16) T(n/8) T(n/8) T(n/4) Breaking down further gives us following cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ c(n2)/256 c(n2)/64 c(n2)/64 c(n2)/16 / \\ / \\ / \\ / \\ To know the value of T(n), we need to calculate sum of tree nodes level by level. If we sum the above tree level by level, we get the following series T(n) = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + .... The above series is geometrical progression with ratio 5/16. To get an upper bound, we can sum the infinite series. We get the sum as (n2)/(1 - 5/16) which is O(n2)","title":"Recurrence Tree Method"},{"location":"algo/analysis/space.html","text":"Space \u00b6 Auxiliary Space \u00b6 Extra or temporary space used by an algorithm. Space Complexity \u00b6 total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criteria than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort and Heap Sort use O(1) auxiliary space. Space complexity of all these sorting algorithms is O(n) though.","title":"Space"},{"location":"algo/analysis/space.html#space","text":"","title":"Space"},{"location":"algo/analysis/space.html#auxiliary-space","text":"Extra or temporary space used by an algorithm.","title":"Auxiliary Space"},{"location":"algo/analysis/space.html#space-complexity","text":"total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criteria than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort and Heap Sort use O(1) auxiliary space. Space complexity of all these sorting algorithms is O(n) though.","title":"Space Complexity"},{"location":"algo/branch-bound/intro.html","text":"Branch & Bound \u00b6 combinatorial optimization problems. exponential in terms of time complexity exploring all possible permutations We can use Backtracking to optimize the Brute Force solution. In the tree representation, we can do DFS of tree. If we reach a point where a solution no longer is feasible, there is no need to continue exploring. In the given example, backtracking would be much more effective if we had even more items or a smaller knapsack capacity. The backtracking based solution works better than brute force by ignoring infeasible solutions. We can do better (than backtracking) if we know a bound on best possible solution subtree rooted with every node. If the best in subtree is worse than current best, we can simply ignore this node and its subtrees. So we compute bound (best solution) for every node and compare the bound with current best solution before exploring the node. Branch and bound is very useful technique for searching a solution but in worst case, we need to fully calculate the entire tree. At best, we only need to fully calculate one path through the tree and prune the rest of it.","title":"Branch & Bound"},{"location":"algo/branch-bound/intro.html#branch-bound","text":"combinatorial optimization problems. exponential in terms of time complexity exploring all possible permutations We can use Backtracking to optimize the Brute Force solution. In the tree representation, we can do DFS of tree. If we reach a point where a solution no longer is feasible, there is no need to continue exploring. In the given example, backtracking would be much more effective if we had even more items or a smaller knapsack capacity. The backtracking based solution works better than brute force by ignoring infeasible solutions. We can do better (than backtracking) if we know a bound on best possible solution subtree rooted with every node. If the best in subtree is worse than current best, we can simply ignore this node and its subtrees. So we compute bound (best solution) for every node and compare the bound with current best solution before exploring the node. Branch and bound is very useful technique for searching a solution but in worst case, we need to fully calculate the entire tree. At best, we only need to fully calculate one path through the tree and prune the rest of it.","title":"Branch &amp; Bound"},{"location":"algo/branch-bound/knapsack.html","text":"Knapsack \u00b6 Bound \u00b6 The idea is to use the fact that the Greedy approach provides the best solution for Fractional Knapsack problem. To check if a particular node can give us a better solution or not, we compute the optimal solution (through the node) using Greedy approach. If the solution computed by Greedy approach itself is more than the best so far, then we can\u2019t get a better solution through the node Sort all items in decreasing order of ratio of value per unit weight so that an upper bound can be computed using Greedy Approach. Initialize maximum profit, maxProfit = 0 Create an empty queue, Q. Create a dummy node of decision tree and enqueue it to Q. Profit and weight of dummy node are 0. Do following while Q is not empty. Extract an item from Q. Let the extracted item be u. Compute profit of next level node. If the profit is more than maxProfit, then update maxProfit. Compute bound of next level node. If bound is more than maxProfit, then add next level node to Q. Consider the case when next level node is not considered as part of solution and add a node to queue with level as next, but weight and profit without considering next level nodes. O(2^n), O(1) struct Item { float weight; int value; }; struct Node { int level, profit, bound; float weight; }; bool cmp(Item a, Item b) { double r1 = (double)a.value / a.weight; double r2 = (double)b.value / b.weight; return r1 > r2; } int bound(Node u, int n, int W, Item arr[]) { if (u.weight >= W) return 0; int profit_bound = u.profit; int j = u.level + 1; int totweight = u.weight; while ((j < n) && (totweight + arr[j].weight <= W)) { totweight += arr[j].weight; profit_bound += arr[j].value; j++; } if (j < n) profit_bound += (W - totweight) * arr[j].value / arr[j].weight; return profit_bound; } int knapsack(int W, Item arr[], int n) { sort(arr, arr + n, cmp); queue<Node> Q; Node u, v; u.level = -1; u.profit = u.weight = 0; Q.push(u); int maxProfit = 0; while (!Q.empty()) { u = Q.front(); Q.pop(); if (u.level == -1) v.level = 0; if (u.level == n-1) continue; v.level = u.level + 1; v.weight = u.weight + arr[v.level].weight; v.profit = u.profit + arr[v.level].value; if (v.weight <= W && v.profit > maxProfit) maxProfit = v.profit; v.bound = bound(v, n, W, arr); if (v.bound > maxProfit) Q.push(v); v.weight = u.weight; v.profit = u.profit; v.bound = bound(v, n, W, arr); if (v.bound > maxProfit) Q.push(v); } return maxProfit; }","title":"Knapsack"},{"location":"algo/branch-bound/knapsack.html#knapsack","text":"","title":"Knapsack"},{"location":"algo/branch-bound/knapsack.html#bound","text":"The idea is to use the fact that the Greedy approach provides the best solution for Fractional Knapsack problem. To check if a particular node can give us a better solution or not, we compute the optimal solution (through the node) using Greedy approach. If the solution computed by Greedy approach itself is more than the best so far, then we can\u2019t get a better solution through the node Sort all items in decreasing order of ratio of value per unit weight so that an upper bound can be computed using Greedy Approach. Initialize maximum profit, maxProfit = 0 Create an empty queue, Q. Create a dummy node of decision tree and enqueue it to Q. Profit and weight of dummy node are 0. Do following while Q is not empty. Extract an item from Q. Let the extracted item be u. Compute profit of next level node. If the profit is more than maxProfit, then update maxProfit. Compute bound of next level node. If bound is more than maxProfit, then add next level node to Q. Consider the case when next level node is not considered as part of solution and add a node to queue with level as next, but weight and profit without considering next level nodes. O(2^n), O(1) struct Item { float weight; int value; }; struct Node { int level, profit, bound; float weight; }; bool cmp(Item a, Item b) { double r1 = (double)a.value / a.weight; double r2 = (double)b.value / b.weight; return r1 > r2; } int bound(Node u, int n, int W, Item arr[]) { if (u.weight >= W) return 0; int profit_bound = u.profit; int j = u.level + 1; int totweight = u.weight; while ((j < n) && (totweight + arr[j].weight <= W)) { totweight += arr[j].weight; profit_bound += arr[j].value; j++; } if (j < n) profit_bound += (W - totweight) * arr[j].value / arr[j].weight; return profit_bound; } int knapsack(int W, Item arr[], int n) { sort(arr, arr + n, cmp); queue<Node> Q; Node u, v; u.level = -1; u.profit = u.weight = 0; Q.push(u); int maxProfit = 0; while (!Q.empty()) { u = Q.front(); Q.pop(); if (u.level == -1) v.level = 0; if (u.level == n-1) continue; v.level = u.level + 1; v.weight = u.weight + arr[v.level].weight; v.profit = u.profit + arr[v.level].value; if (v.weight <= W && v.profit > maxProfit) maxProfit = v.profit; v.bound = bound(v, n, W, arr); if (v.bound > maxProfit) Q.push(v); v.weight = u.weight; v.profit = u.profit; v.bound = bound(v, n, W, arr); if (v.bound > maxProfit) Q.push(v); } return maxProfit; }","title":"Bound"},{"location":"algo/dac/closest-pair.html","text":"Closest Pair \u00b6 T(n) = 2T(n/2) + O(n) + O(nLogn) + O(n), T(n) = 2T(n/2) + O(nLogn), T(n) = T(n x Logn x Logn) , O(n) sorting used mmerge , quick class Point { public: int x, y; }; int compareX(const void* a, const void* b){ Point *x = (Point*) a; Point *y = (Point*) b; return (x->x - y->x); } int compareY(const void* a, const void* b){ Point *x = (Point*) a; Point *y = (Point*) b; return (x->y - y->y); } float dist(Point* a, Point* b){ return sqrt((a->y-b->y) * (a->y-b->y) + (a->x - b->x) * (a->x - b->x)); } int min(int a , int b){ return a<b ? a : b; } float bruteForce(Point P[], int n) { float min = INT_MAX; for (int i = 0; i < n; ++i) for (int j = i+1; j < n; ++j){ float distance1 = dist(&P[i], &P[j]); if (distance1 < min) min = distance1; } return min; } float min(float x, float y){ return x<y ? x : y; } float stripClosest(Point strip[], int size, float d) { float min = d; qsort(strip, size, sizeof(Point), compareY); for (int i = 0; i < size; ++i) for (int j = i+1; j < size && (strip[j].y - strip[i].y) < min; ++j) if (dist(&strip[i],&strip[j]) < min) min = dist(&strip[i], &strip[j]); return min; } float closestUtil(Point P[], int n) { if (n <= 3) return bruteForce(P, n); int mid = n/2; Point midPoint = P[mid]; float dl = closestUtil(P, mid); float dr = closestUtil(P + mid, n - mid); float d = min(dl, dr); Point strip[n]; int j = 0; for (int i = 0; i < n; i++) if (abs(P[i].x - midPoint.x) < d) strip[j] = P[i], j++; return min(d, stripClosest(strip, j, d) ); } float closest(Point P[], int n) { qsort(P, n, sizeof(Point), compareX); return closestUtil(P, n); } int main() { Point P[] = {{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}}; int n = sizeof(P) / sizeof(P[0]); cout << \"The smallest distance is \" << closest(P, n); return 0; }","title":"Closest Pair"},{"location":"algo/dac/closest-pair.html#closest-pair","text":"T(n) = 2T(n/2) + O(n) + O(nLogn) + O(n), T(n) = 2T(n/2) + O(nLogn), T(n) = T(n x Logn x Logn) , O(n) sorting used mmerge , quick class Point { public: int x, y; }; int compareX(const void* a, const void* b){ Point *x = (Point*) a; Point *y = (Point*) b; return (x->x - y->x); } int compareY(const void* a, const void* b){ Point *x = (Point*) a; Point *y = (Point*) b; return (x->y - y->y); } float dist(Point* a, Point* b){ return sqrt((a->y-b->y) * (a->y-b->y) + (a->x - b->x) * (a->x - b->x)); } int min(int a , int b){ return a<b ? a : b; } float bruteForce(Point P[], int n) { float min = INT_MAX; for (int i = 0; i < n; ++i) for (int j = i+1; j < n; ++j){ float distance1 = dist(&P[i], &P[j]); if (distance1 < min) min = distance1; } return min; } float min(float x, float y){ return x<y ? x : y; } float stripClosest(Point strip[], int size, float d) { float min = d; qsort(strip, size, sizeof(Point), compareY); for (int i = 0; i < size; ++i) for (int j = i+1; j < size && (strip[j].y - strip[i].y) < min; ++j) if (dist(&strip[i],&strip[j]) < min) min = dist(&strip[i], &strip[j]); return min; } float closestUtil(Point P[], int n) { if (n <= 3) return bruteForce(P, n); int mid = n/2; Point midPoint = P[mid]; float dl = closestUtil(P, mid); float dr = closestUtil(P + mid, n - mid); float d = min(dl, dr); Point strip[n]; int j = 0; for (int i = 0; i < n; i++) if (abs(P[i].x - midPoint.x) < d) strip[j] = P[i], j++; return min(d, stripClosest(strip, j, d) ); } float closest(Point P[], int n) { qsort(P, n, sizeof(Point), compareX); return closestUtil(P, n); } int main() { Point P[] = {{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}}; int n = sizeof(P) / sizeof(P[0]); cout << \"The smallest distance is \" << closest(P, n); return 0; }","title":"Closest Pair"},{"location":"algo/dac/count-inversion.html","text":"Counting Inversion \u00b6 O(logn) , O(logn) simple traversal = O(n^2) int merge (int arr[], int temp[], int l, int mid, int r ){ int i, j, k; int inv_count = 0; i = l; j = mid; k = l; while(i<mid&&j<=r){ if(arr[i] <= arr[j]){ temp[k++] = arr[i++]; } else{ temp[k++] = arr[j++]; inv_count += mid-i; } } while(i<mid){ temp[k++] = arr[i++]; } while(j<=r){ temp[k++] = arr[j++]; } for(int i = l; i<=r;i++){ arr[i] = temp[i]; } return inv_count; } int mergesort(int arr[], int temp[], int l , int r){ int mid, inv_count = 0; if(r>l){ mid = (l+r)/2; inv_count += mergesort(arr, temp, l, mid); inv_count += mergesort(arr, temp, mid+1, r); inv_count += merge(arr, temp, l, mid+1 , r); } return inv_count; } int mergeSort(int arr[], int N){ int temp[N]; mergesort(arr, temp, 0, N-1); }","title":"Counting Inversion"},{"location":"algo/dac/count-inversion.html#counting-inversion","text":"O(logn) , O(logn) simple traversal = O(n^2) int merge (int arr[], int temp[], int l, int mid, int r ){ int i, j, k; int inv_count = 0; i = l; j = mid; k = l; while(i<mid&&j<=r){ if(arr[i] <= arr[j]){ temp[k++] = arr[i++]; } else{ temp[k++] = arr[j++]; inv_count += mid-i; } } while(i<mid){ temp[k++] = arr[i++]; } while(j<=r){ temp[k++] = arr[j++]; } for(int i = l; i<=r;i++){ arr[i] = temp[i]; } return inv_count; } int mergesort(int arr[], int temp[], int l , int r){ int mid, inv_count = 0; if(r>l){ mid = (l+r)/2; inv_count += mergesort(arr, temp, l, mid); inv_count += mergesort(arr, temp, mid+1, r); inv_count += merge(arr, temp, l, mid+1 , r); } return inv_count; } int mergeSort(int arr[], int N){ int temp[N]; mergesort(arr, temp, 0, N-1); }","title":"Counting Inversion"},{"location":"algo/dac/intro.html","text":"Divide & Conquer \u00b6 Divide \u00b6 This involves dividing the problem into some sub problem. Conquer \u00b6 Sub problem by calling recursively until sub problem solved. Combine \u00b6 The Sub problem Solved so that we will get find problem solution. DAC(a, i, j) { if(small(a, i, j)) return(Solution(a, i, j)) else m = divide(a, i, j) // f1(n) b = DAC(a, i, mid) // T(n/2) c = DAC(a, mid+1, j) // T(n/2) d = combine(b, c) // f2(n) return(d) } T(n) = f1(n) + 2T(n/2) + f2(n)","title":"Divide & Conquer"},{"location":"algo/dac/intro.html#divide-conquer","text":"","title":"Divide &amp; Conquer"},{"location":"algo/dac/intro.html#divide","text":"This involves dividing the problem into some sub problem.","title":"Divide"},{"location":"algo/dac/intro.html#conquer","text":"Sub problem by calling recursively until sub problem solved.","title":"Conquer"},{"location":"algo/dac/intro.html#combine","text":"The Sub problem Solved so that we will get find problem solution. DAC(a, i, j) { if(small(a, i, j)) return(Solution(a, i, j)) else m = divide(a, i, j) // f1(n) b = DAC(a, i, mid) // T(n/2) c = DAC(a, mid+1, j) // T(n/2) d = combine(b, c) // f2(n) return(d) } T(n) = f1(n) + 2T(n/2) + f2(n)","title":"Combine"},{"location":"algo/dac/median.html","text":"Find Median \u00b6 O(logn) , O(logn) simple 2 way merging (linear) will give O(n) time but we need O(logn) hence DAC will be used. you should just go till n not 2n. not merge but jst comparing. int med(int arr1[], int arr2[], int N){ if (N <= 0) return -1; if (N == 1) return (arr1[0] + arr2[0]) / 2; if (N == 2) return (max(arr1[0], arr2[0]) + min(arr1[1], arr2[1])) / 2; int med1, med2; if(N%2==0){ med1 = arr1[N/2]; med2 = arr2[N/2]; } else{ med1 = (arr1[N/2] + arr1[N/2 -1])/2; med2 = (arr2[N/2] + arr2[N/2 -1])/2; } if(med1>med2){ if(N%2==0) return med(arr1, arr2 + N/2 -1, N/2+1); else { return med (arr1, arr2 + N/2, N/2); } } else if (med1==med2){ return med1; } else{ if(N%2==0) return med(arr1 + N/2 -1, arr2, N/2+1); else { return med (arr1, arr2 + N/2, N/2); } } }","title":"Find Median"},{"location":"algo/dac/median.html#find-median","text":"O(logn) , O(logn) simple 2 way merging (linear) will give O(n) time but we need O(logn) hence DAC will be used. you should just go till n not 2n. not merge but jst comparing. int med(int arr1[], int arr2[], int N){ if (N <= 0) return -1; if (N == 1) return (arr1[0] + arr2[0]) / 2; if (N == 2) return (max(arr1[0], arr2[0]) + min(arr1[1], arr2[1])) / 2; int med1, med2; if(N%2==0){ med1 = arr1[N/2]; med2 = arr2[N/2]; } else{ med1 = (arr1[N/2] + arr1[N/2 -1])/2; med2 = (arr2[N/2] + arr2[N/2 -1])/2; } if(med1>med2){ if(N%2==0) return med(arr1, arr2 + N/2 -1, N/2+1); else { return med (arr1, arr2 + N/2, N/2); } } else if (med1==med2){ return med1; } else{ if(N%2==0) return med(arr1 + N/2 -1, arr2, N/2+1); else { return med (arr1, arr2 + N/2, N/2); } } }","title":"Find Median"},{"location":"algo/dac/min-max.html","text":"Min Max \u00b6 int DAC_max (int arr[], int index, int l){ int max; if(index >= l-2){ if(arr[index]>arr[index+1]){ return arr[index]; } else { return arr[index+1]; } } max = DAC_max(arr, index+1, l); if(arr[index]>max){ return arr[index]; } else{ return max; } } int DAC_min(int arr[], int index, int l){ if(index >= l-2){ if(arr[index]>arr[index+1]){ return arr[index]; } else{ return arr[index+1]; } } int max = DAC_min(arr, index+1, l); if(arr[index]>max){ return arr[index]; } else{ return max; } }","title":"Min Max"},{"location":"algo/dac/min-max.html#min-max","text":"int DAC_max (int arr[], int index, int l){ int max; if(index >= l-2){ if(arr[index]>arr[index+1]){ return arr[index]; } else { return arr[index+1]; } } max = DAC_max(arr, index+1, l); if(arr[index]>max){ return arr[index]; } else{ return max; } } int DAC_min(int arr[], int index, int l){ if(index >= l-2){ if(arr[index]>arr[index+1]){ return arr[index]; } else{ return arr[index+1]; } } int max = DAC_min(arr, index+1, l); if(arr[index]>max){ return arr[index]; } else{ return max; } }","title":"Min Max"},{"location":"algo/dac/power.html","text":"Power \u00b6 O(n) , O(1) Space complexity - O(logn) int power(int x, unsigned int y){ if(y<=0)return 1; if(y%2==0){ return power(x, y/2) * power(x, y/2); } else{ return x * power(x, y/2) * power(x, y/2); } }","title":"Power"},{"location":"algo/dac/power.html#power","text":"O(n) , O(1) Space complexity - O(logn) int power(int x, unsigned int y){ if(y<=0)return 1; if(y%2==0){ return power(x, y/2) * power(x, y/2); } else{ return x * power(x, y/2) * power(x, y/2); } }","title":"Power"},{"location":"algo/dac/strassen-mul.html","text":"Strassen\u2019s Matrix Multiplication \u00b6 O(N^2.8074) , submatrices in recursion BRUTEFORCE: O(N3) Strassen\u2019s method -> reduce recursive calls to 7. Strassen\u2019s Method is not preferred constants are high. Sparse matrices have special methods submatrices take extra space. larger errors accumulate due to computer inprecision #define V 4 void split(int x[V][V], int y[V][V]){ } int[V][V] multiply(int x[V][V], int y[V][V]){ } int main(){ int a[V][V] = {{1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}}; int b[V][V] = {{1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}}; return 0; }","title":"Strassen\u2019s Matrix Multiplication"},{"location":"algo/dac/strassen-mul.html#strassens-matrix-multiplication","text":"O(N^2.8074) , submatrices in recursion BRUTEFORCE: O(N3) Strassen\u2019s method -> reduce recursive calls to 7. Strassen\u2019s Method is not preferred constants are high. Sparse matrices have special methods submatrices take extra space. larger errors accumulate due to computer inprecision #define V 4 void split(int x[V][V], int y[V][V]){ } int[V][V] multiply(int x[V][V], int y[V][V]){ } int main(){ int a[V][V] = {{1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}}; int b[V][V] = {{1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}, {1,2,34,34,}}; return 0; }","title":"Strassen\u2019s Matrix Multiplication"},{"location":"algo/dp/intro.html","text":"Dynamic Programming \u00b6 Divide and Conquer with overlapping results. Properties \u00b6 1) Overlapping Subproblems 2) Optimal Substructure Overlapping Subproblems \u00b6 subproblems overlap and solutions of them are needed again and again. Binary Search -> \u274e fibonnici -> \u2705 Solutions of same subproblems are needed again and again. fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ fib(2) fib(1) fib(1) fib(0) fib(1) fib(0) / \\ fib(1) fib(0) fib(3) is being called multiple times Methods a) Memoization (Top Down) int fib(int n) { if (lookup[n] == NIL) { if (n <= 1) lookup[n] = n; else lookup[n] = fib(n - 1) + fib(n - 2); } b) Tabulation (Bottom Up) int fib(int n) { int f[n+1]; int i; f[0] = 0; f[1] = 1; for (i = 2; i <= n; i++) f[i] = f[i-1] + f[i-2]; return f[n]; } Comparison Memotisation Tabular Top down bottom up on demand all fib(40) tabulated -> 0.000066 sec fib(40) memotised -> 0.000047 fib(40) naive recursive -> 0.938955 Optimal Substructure \u00b6 Optimal(problem) == Optimal(Union of subproblems) Longest Path -> \u274e Shortest path -> \u2705","title":"Dynamic Programming"},{"location":"algo/dp/intro.html#dynamic-programming","text":"Divide and Conquer with overlapping results.","title":"Dynamic Programming"},{"location":"algo/dp/intro.html#properties","text":"1) Overlapping Subproblems 2) Optimal Substructure","title":"Properties"},{"location":"algo/dp/intro.html#overlapping-subproblems","text":"subproblems overlap and solutions of them are needed again and again. Binary Search -> \u274e fibonnici -> \u2705 Solutions of same subproblems are needed again and again. fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ fib(2) fib(1) fib(1) fib(0) fib(1) fib(0) / \\ fib(1) fib(0) fib(3) is being called multiple times Methods a) Memoization (Top Down) int fib(int n) { if (lookup[n] == NIL) { if (n <= 1) lookup[n] = n; else lookup[n] = fib(n - 1) + fib(n - 2); } b) Tabulation (Bottom Up) int fib(int n) { int f[n+1]; int i; f[0] = 0; f[1] = 1; for (i = 2; i <= n; i++) f[i] = f[i-1] + f[i-2]; return f[n]; } Comparison Memotisation Tabular Top down bottom up on demand all fib(40) tabulated -> 0.000066 sec fib(40) memotised -> 0.000047 fib(40) naive recursive -> 0.938955","title":"Overlapping Subproblems"},{"location":"algo/dp/intro.html#optimal-substructure","text":"Optimal(problem) == Optimal(Union of subproblems) Longest Path -> \u274e Shortest path -> \u2705","title":"Optimal Substructure"},{"location":"algo/dp/knapsack.html","text":"Knapsack \u00b6 Iterative \u00b6 knapsack(){ int K[n+1], W[n+1]; } Recursive \u00b6 int knapsack (int* val, int* weights, int W, int n){ if(n==0 || W==0) return 0; if(val[n-1]>W) return knapsack(val, weights, W, n-1); return max( val[n-1] + knapsack(val,weights, W-val[n-1], n-1), knapsack(val,weights, W, n-1)); }","title":"Knapsack"},{"location":"algo/dp/knapsack.html#knapsack","text":"","title":"Knapsack"},{"location":"algo/dp/knapsack.html#iterative","text":"knapsack(){ int K[n+1], W[n+1]; }","title":"Iterative"},{"location":"algo/dp/knapsack.html#recursive","text":"int knapsack (int* val, int* weights, int W, int n){ if(n==0 || W==0) return 0; if(val[n-1]>W) return knapsack(val, weights, W, n-1); return max( val[n-1] + knapsack(val,weights, W-val[n-1], n-1), knapsack(val,weights, W, n-1)); }","title":"Recursive"},{"location":"algo/dp/power.html","text":"Power \u00b6 O(logn) , O(1) Space complexity - O(logn)? float power(float x, int y){ if(y==0) return 1; float temp = power (x,y/2); if(y%2==0){ return temp * temp; } else { if(y>0) return x * temp * temp; else{ return temp*temp/x; } } }","title":"Power"},{"location":"algo/dp/power.html#power","text":"O(logn) , O(1) Space complexity - O(logn)? float power(float x, int y){ if(y==0) return 1; float temp = power (x,y/2); if(y%2==0){ return temp * temp; } else { if(y>0) return x * temp * temp; else{ return temp*temp/x; } } }","title":"Power"},{"location":"algo/graph/cycle-detection.html","text":"// https://www.geeksforgeeks.org/union-find/ // https://ide.geeksforgeeks.org/hB3sIlWa48 // O(E^2) , parent array O(n) // NOTE: using disjoint and uninion-find algo // 0 1 2 // -1 -1 -1 // 0 1 2 <----- 1 is made parent of 0 (1 is now representative of subset {0, 1}) // 1 -1 -1 // 0 1 2 <----- 2 is made parent of 1 (2 is now representative of subset {0, 1, 2}) // 1 2 -1 include \u00b6 include \u00b6 using namespace std; class Edge { public: int src, dest; }; class Graph{ public: int V, E; Edge* edges; }; Graph createGraph(int V, int E){ Graph graph = new Graph(); graph->V = V; graph->E = E; graph->edges = new Edge[graph->E * sizeof(Edge)]; return graph; } int find(int parent[], int i){ // takes O(n) if skewed if(parent[i]==-1) return i; return find(parent, parent[i]); } void Union(int parent[], int x, int y){ // takes O(n) if skewed int xset = find(parent, x); int yset = find(parent, y); if ( xset != yset ) { parent [ xset ] = yset ; } } bool isCycle(Graph graph){ int parent = new int[graph->V * sizeof(int)]; memset(parent,-1,graph->V * sizeof(int)); for (int i = 0; i < graph->E; i++){ int x = find(parent, graph->edges[i].src); int y = find(parent, graph->edges[i].dest); if(x == y){ return 1; } Union ( parent , x , y ) ; } return 0 ; } int main(){ / 0 | \\ | \\ 1---2 / int V = 3 , E = 3 ; Graph * graph = createGraph ( V , E ); graph -> edges [ 0 ]. src = 0 ; graph -> edges [ 0 ]. dest = 1 ; graph -> edges [ 1 ]. src = 1 ; graph -> edges [ 1 ]. dest = 2 ; graph -> edges [ 2 ]. src = 0 ; graph -> edges [ 2 ]. dest = 2 ; if ( isCycle ( graph )) cout << \"graph contains cycle\" ; else cout << \"graph doesn' t contain cycle \"; return 0; } // // https://ide.geeksforgeeks.org/GYaaRIEaD8 // O(nlogn + V) , subsets O(n) // NOTE: find and union takes O(n) due to union by rank and path compression // This technique is called union by rank. The term rank is preferred instead of height because // if path compression technique is used, then rank is not always equal to height. // Path Compression. The idea is to flatten the tree when find() is called. // Also, size (in place of height) of trees can also be used as rank. Using size as rank also yields worst case time complexity as O(Logn) include \u00b6 using namespace std; struct Edge{ int src, dest; }; struct Graph{ int E, V; Edge * Edges; }; struct subset{ int parent; int rank; }; Graph* createGraph(int E, int V){ Graph * graph = ( Graph * ) malloc ( sizeof ( struct Graph )); graph -> E = E ; graph -> V = V ; graph -> Edges = ( Edge * ) malloc ( sizeof ( Edge ) * graph -> E ); return graph ; } int Find(struct subset subsets[], int i){ // takes O(logn) if(subsets[i].parent!=i) return Find(subsets, subsets[i].parent); return subsets [ i ] . parent ; } void Union(struct subset subsets[], int x, int y){ // takes O(logn) int xset = Find(subsets, x); int yset = Find(subsets, y); if(subsets[xset].rank <= subsets[yset].rank){ subsets[xset].parent = yset; } else if(subsets[xset].rank > subsets[yset].rank){ subsets[yset].parent = xset; } else { subsets[yset].parent = xset; subsets[xset].rank++; } } bool isCycle(Graph* graph){ int V = graph -> V ; int E = graph -> E ; struct subset * subsets = ( struct subset * ) malloc ( sizeof ( subset ) * V ); for ( int v = 0 ; v < V ; v ++ ) { subsets [ v ] . parent = v ; subsets [ v ] . rank = 0 ; } for ( int e = 0 ; e < E ; e ++ ) { int x = Find ( subsets , graph -> Edges [ e ] . src ); int y = Find ( subsets , graph -> Edges [ e ] . dest ); if ( x == y ) return 1 ; Union ( subsets , x , y ); } return 0 ; } int main() { / 0 | \\ | \\ 1-----2 / int V = 3 , E = 3 ; struct Graph * graph = createGraph ( V , E ); // add 0-1 graph -> Edges [ 0 ]. src = 0 ; graph -> Edges [ 0 ]. dest = 1 ; // add 1-2 graph -> Edges [ 1 ]. src = 1 ; graph -> Edges [ 1 ]. dest = 2 ; // add 0-2 graph -> Edges [ 2 ]. src = 0 ; graph -> Edges [ 2 ]. dest = 2 ; if ( isCycle ( graph )) printf ( \"Graph contains cycle\" ); else printf ( \"Graph doesn' t contain cycle \" ); return 0; }","title":"Cycle detection"},{"location":"algo/graph/cycle-detection.html#include","text":"","title":"include"},{"location":"algo/graph/cycle-detection.html#include_1","text":"using namespace std; class Edge { public: int src, dest; }; class Graph{ public: int V, E; Edge* edges; }; Graph createGraph(int V, int E){ Graph graph = new Graph(); graph->V = V; graph->E = E; graph->edges = new Edge[graph->E * sizeof(Edge)]; return graph; } int find(int parent[], int i){ // takes O(n) if skewed if(parent[i]==-1) return i; return find(parent, parent[i]); } void Union(int parent[], int x, int y){ // takes O(n) if skewed int xset = find(parent, x); int yset = find(parent, y); if ( xset != yset ) { parent [ xset ] = yset ; } } bool isCycle(Graph graph){ int parent = new int[graph->V * sizeof(int)]; memset(parent,-1,graph->V * sizeof(int)); for (int i = 0; i < graph->E; i++){ int x = find(parent, graph->edges[i].src); int y = find(parent, graph->edges[i].dest); if(x == y){ return 1; } Union ( parent , x , y ) ; } return 0 ; } int main(){ / 0 | \\ | \\ 1---2 / int V = 3 , E = 3 ; Graph * graph = createGraph ( V , E ); graph -> edges [ 0 ]. src = 0 ; graph -> edges [ 0 ]. dest = 1 ; graph -> edges [ 1 ]. src = 1 ; graph -> edges [ 1 ]. dest = 2 ; graph -> edges [ 2 ]. src = 0 ; graph -> edges [ 2 ]. dest = 2 ; if ( isCycle ( graph )) cout << \"graph contains cycle\" ; else cout << \"graph doesn' t contain cycle \"; return 0; } // // https://ide.geeksforgeeks.org/GYaaRIEaD8 // O(nlogn + V) , subsets O(n) // NOTE: find and union takes O(n) due to union by rank and path compression // This technique is called union by rank. The term rank is preferred instead of height because // if path compression technique is used, then rank is not always equal to height. // Path Compression. The idea is to flatten the tree when find() is called. // Also, size (in place of height) of trees can also be used as rank. Using size as rank also yields worst case time complexity as O(Logn)","title":"include"},{"location":"algo/graph/cycle-detection.html#include_2","text":"using namespace std; struct Edge{ int src, dest; }; struct Graph{ int E, V; Edge * Edges; }; struct subset{ int parent; int rank; }; Graph* createGraph(int E, int V){ Graph * graph = ( Graph * ) malloc ( sizeof ( struct Graph )); graph -> E = E ; graph -> V = V ; graph -> Edges = ( Edge * ) malloc ( sizeof ( Edge ) * graph -> E ); return graph ; } int Find(struct subset subsets[], int i){ // takes O(logn) if(subsets[i].parent!=i) return Find(subsets, subsets[i].parent); return subsets [ i ] . parent ; } void Union(struct subset subsets[], int x, int y){ // takes O(logn) int xset = Find(subsets, x); int yset = Find(subsets, y); if(subsets[xset].rank <= subsets[yset].rank){ subsets[xset].parent = yset; } else if(subsets[xset].rank > subsets[yset].rank){ subsets[yset].parent = xset; } else { subsets[yset].parent = xset; subsets[xset].rank++; } } bool isCycle(Graph* graph){ int V = graph -> V ; int E = graph -> E ; struct subset * subsets = ( struct subset * ) malloc ( sizeof ( subset ) * V ); for ( int v = 0 ; v < V ; v ++ ) { subsets [ v ] . parent = v ; subsets [ v ] . rank = 0 ; } for ( int e = 0 ; e < E ; e ++ ) { int x = Find ( subsets , graph -> Edges [ e ] . src ); int y = Find ( subsets , graph -> Edges [ e ] . dest ); if ( x == y ) return 1 ; Union ( subsets , x , y ); } return 0 ; } int main() { / 0 | \\ | \\ 1-----2 / int V = 3 , E = 3 ; struct Graph * graph = createGraph ( V , E ); // add 0-1 graph -> Edges [ 0 ]. src = 0 ; graph -> Edges [ 0 ]. dest = 1 ; // add 1-2 graph -> Edges [ 1 ]. src = 1 ; graph -> Edges [ 1 ]. dest = 2 ; // add 0-2 graph -> Edges [ 2 ]. src = 0 ; graph -> Edges [ 2 ]. dest = 2 ; if ( isCycle ( graph )) printf ( \"Graph contains cycle\" ); else printf ( \"Graph doesn' t contain cycle \" ); return 0; }","title":"include"},{"location":"algo/greedy/activities.html","text":"Activity Selection \u00b6 unsorted O(n^2logn), O(1) exist A = {B \u2013 {k}} U {1}.(Note that the activities in B are independent and k has smallest finishing time among all. Since k is not 1, finish(k) >= finish(1)). struct Activitiy { int start, finish; }; bool activityCompare(Activitiy s1, Activitiy s2) { return (s1.finish < s2.finish); } void printMaxActivities(Activitiy arr[], int n) { sort(arr, arr+n, activityCompare); cout << \"Following activities are selected n\"; int i = 0; cout << \"(\" << arr[i].start << \", \" << arr[i].finish << \"), \"; for (int j = 1; j < n; j++) { if (arr[j].start >= arr[i].finish) { cout << \"(\" << arr[j].start << \", \" << arr[j].finish << \"), \"; i = j; } } }","title":"Activity Selection"},{"location":"algo/greedy/activities.html#activity-selection","text":"unsorted O(n^2logn), O(1) exist A = {B \u2013 {k}} U {1}.(Note that the activities in B are independent and k has smallest finishing time among all. Since k is not 1, finish(k) >= finish(1)). struct Activitiy { int start, finish; }; bool activityCompare(Activitiy s1, Activitiy s2) { return (s1.finish < s2.finish); } void printMaxActivities(Activitiy arr[], int n) { sort(arr, arr+n, activityCompare); cout << \"Following activities are selected n\"; int i = 0; cout << \"(\" << arr[i].start << \", \" << arr[i].finish << \"), \"; for (int j = 1; j < n; j++) { if (arr[j].start >= arr[i].finish) { cout << \"(\" << arr[j].start << \", \" << arr[j].finish << \"), \"; i = j; } } }","title":"Activity Selection"},{"location":"algo/greedy/dijkstra.html","text":"Dijkstra \u00b6 O(V^2) , PRE: trees, prim's mst NOTE: Similar to prim's mst we generate a SPT (shortest path tree) with source as root two sets -> contains vertices included in shortest path tree, if not then other set. find vertex in other set with min distance from source. If path is needed then maintain parent array. mainly for undirected graph, can be customized for directed. code gets min distance of source to every vertex, if want only one, break loop at finding it. input graph -> adjacency list, reduced to O(E log V) with binary heap. only for +ve weghts, -ve -> Bellman\u2013Ford. #define V 9 int minDistance(int dist[], bool sptSet[]){ int min = INT_MAX, min_index; for (int v = 0; v<V; v++) if(!sptSet[v] && dist[v]<min){ min = dist[v]; min_index = v; } return min_index; } void printSolution(int dist[]){ cout << \"distance from src:\" << endl; for(int i = 0; i<V; i++){ cout << i + \": \" << dist[i] << endl; } } void dijkstra(int graph[V][V], int src){ int dist[V]; distance from (Src) bool sptSet[V]; if v is in shortest path set for(int v = 0; v < V; v++) dist[v] = INT_MAX, sptSet[v] = false; dist[src] = 0; for (int count = 0; count<V-1; count++){ int u = minDistance(dist, sptSet); sptSet[u] = true; for (int v = 0; v < V; v++) if (!sptSet[v] && graph[u][v] && dist[u] != INT_MAX && dist[u] + graph[u][v] < dist[v]) dist[v] = dist[u] + graph[u][v]; } printSolution(dist); } int main() { see dikjitsra_graph.bmp int graph[V][V] = { { 0, 4, 0, 0, 0, 0, 0, 8, 0 }, { 4, 0, 8, 0, 0, 0, 0, 11, 0 }, { 0, 8, 0, 7, 0, 4, 0, 0, 2 }, { 0, 0, 7, 0, 9, 14, 0, 0, 0 }, { 0, 0, 0, 9, 0, 10, 0, 0, 0 }, { 0, 0, 4, 14, 10, 0, 2, 0, 0 }, { 0, 0, 0, 0, 0, 2, 0, 1, 6 }, { 8, 11, 0, 0, 0, 0, 1, 0, 7 }, { 0, 0, 2, 0, 0, 0, 6, 7, 0 } }; dijkstra(graph, 0); return 0; }","title":"Dijkstra"},{"location":"algo/greedy/dijkstra.html#dijkstra","text":"O(V^2) , PRE: trees, prim's mst NOTE: Similar to prim's mst we generate a SPT (shortest path tree) with source as root two sets -> contains vertices included in shortest path tree, if not then other set. find vertex in other set with min distance from source. If path is needed then maintain parent array. mainly for undirected graph, can be customized for directed. code gets min distance of source to every vertex, if want only one, break loop at finding it. input graph -> adjacency list, reduced to O(E log V) with binary heap. only for +ve weghts, -ve -> Bellman\u2013Ford. #define V 9 int minDistance(int dist[], bool sptSet[]){ int min = INT_MAX, min_index; for (int v = 0; v<V; v++) if(!sptSet[v] && dist[v]<min){ min = dist[v]; min_index = v; } return min_index; } void printSolution(int dist[]){ cout << \"distance from src:\" << endl; for(int i = 0; i<V; i++){ cout << i + \": \" << dist[i] << endl; } } void dijkstra(int graph[V][V], int src){ int dist[V]; distance from (Src) bool sptSet[V]; if v is in shortest path set for(int v = 0; v < V; v++) dist[v] = INT_MAX, sptSet[v] = false; dist[src] = 0; for (int count = 0; count<V-1; count++){ int u = minDistance(dist, sptSet); sptSet[u] = true; for (int v = 0; v < V; v++) if (!sptSet[v] && graph[u][v] && dist[u] != INT_MAX && dist[u] + graph[u][v] < dist[v]) dist[v] = dist[u] + graph[u][v]; } printSolution(dist); } int main() { see dikjitsra_graph.bmp int graph[V][V] = { { 0, 4, 0, 0, 0, 0, 0, 8, 0 }, { 4, 0, 8, 0, 0, 0, 0, 11, 0 }, { 0, 8, 0, 7, 0, 4, 0, 0, 2 }, { 0, 0, 7, 0, 9, 14, 0, 0, 0 }, { 0, 0, 0, 9, 0, 10, 0, 0, 0 }, { 0, 0, 4, 14, 10, 0, 2, 0, 0 }, { 0, 0, 0, 0, 0, 2, 0, 1, 6 }, { 8, 11, 0, 0, 0, 0, 1, 0, 7 }, { 0, 0, 2, 0, 0, 0, 6, 7, 0 } }; dijkstra(graph, 0); return 0; }","title":"Dijkstra"},{"location":"algo/greedy/huffman-coding.html","text":"Huffman Coding \u00b6 O(nlogn) , Heap Tree, Trees lossless compression algo build huffman tree, traverse to assign codes to charcters frequency increases, code decreases prefix codes {0,11} - prefix codes {0,1,11} - non prefix code #define MAX_TREE_HT 100 struct minHeapNode{ char data; unsigned frequency; struct minHeapNode *left, *right; }; struct minHeap{ unsigned capacity; unsigned size; struct minHeapNode ** arr; }; struct minHeapNode* newNode(char data, unsigned freq){ struct minHeapNode * temp = (struct minHeapNode*) malloc(sizeof(struct minHeapNode)); temp->frequency = freq; temp->data = data; temp->left = NULL; temp->right = NULL; return temp; } struct minHeap* createMinHeap(unsigned capacity){ struct minHeap* temp = (struct minHeap*)malloc(sizeof(struct minHeap)); temp->size = 0; temp->capacity = capacity; temp->arr = (struct minHeapNode**)malloc(sizeof(struct minHeapNode*) * capacity); return temp; } void swapMinHeapNodes(struct minHeapNode** a, struct minHeapNode** b){ struct minHeapNode* temp = *a; *a = *b; *b = temp; } void heapify(struct minHeap* heap, int idx){ int smallest = idx; int left = 2*idx + 1; int right = 2*idx + 2; if(left<heap->size && heap->arr[left]->frequency < heap->arr[right]->frequency) smallest = left; if(right<heap->size && heap->arr[right]->frequency < heap->arr[smallest]->frequency) smallest = right; if(smallest!=idx){ swapMinHeapNodes(&heap->arr[smallest], &heap->arr[idx]); heapify(heap, smallest); } } bool isSize1(struct minHeap* heap){ return heap->size == 1; } struct minHeapNode* pullMin(struct minHeap* heap){ // TODO doubt why not replace the last node with null struct minHeapNode* min = heap->arr[0]; heap->arr[0] = heap->arr[heap->size -1]; --heap->size; heapify(heap, 0); return min; } void insertNode(struct minHeap* heap,struct minHeapNode* node){ ++heap->size; int curNode = heap->size -1; int root = (curNode-1)/2; while(curNode && heap->arr[curNode]->frequency > heap->arr[root]->frequency){ heap->arr[curNode] = heap->arr[root]; // TODO doubt what happens to curNode? curNode = root; root = (curNode-1)/2; } heap->arr[curNode] = node; } void buildMinHeap(struct minHeap* heap){ int n = heap->size -1; for(int i = (n-1)/2; i>=0; i--) heapify(heap, i); } void printArr(int arr[], int n) { for (int i = 0; i < n; ++i) cout << arr[i]; cout << endl; } bool isLeaf(struct minHeapNode* node){ return !(node->left) && !(node->right); } struct minHeap* createAndBuildMinHeap(char data[], int frq[], int size){ struct minHeap* heap = createMinHeap(size); for(int i = 0; i<size; i++) heap->arr[i] = newNode(data[i], frq[i]); heap->size = size; buildMinHeap(heap); return heap; } struct minHeapNode* buildHuffmanTree(char data[], int freq[], int size){ struct minHeapNode *left, *right, *top; struct minHeap* heap = createAndBuildMinHeap(data, freq, size); while(!isSize1(heap)){ left = pullMin(heap); right = pullMin(heap); top = newNode('$', left->frequency + right->frequency); top->left = left; top->right = right; insertNode(heap, top); } return pullMin(heap); } void printCodes(struct minHeapNode* root, int arr[], int top){ if(root->left){ arr[top] = 0; printCodes(root, arr, top+1); } if(root->right){ // TODO doubt if top is unchanged then it will be overwritten? arr[top] = 1; printCodes(root, arr, top+1); } if(isLeaf(root)){ cout << root->data; printArr(arr,top); } } void HuffmanCodes(char data[], int freq[], int size){ struct minHeapNode* heap = buildHuffmanTree(data, freq, size); int arr[MAX_TREE_HT], top =0; printCodes(heap, arr, top); } int main() { char arr[] = { 'a', 'b', 'c', 'd', 'e', 'f' }; int freq[] = { 5, 9, 12, 13, 16, 45 }; int size = sizeof(arr) / sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; }","title":"Huffman Coding"},{"location":"algo/greedy/huffman-coding.html#huffman-coding","text":"O(nlogn) , Heap Tree, Trees lossless compression algo build huffman tree, traverse to assign codes to charcters frequency increases, code decreases prefix codes {0,11} - prefix codes {0,1,11} - non prefix code #define MAX_TREE_HT 100 struct minHeapNode{ char data; unsigned frequency; struct minHeapNode *left, *right; }; struct minHeap{ unsigned capacity; unsigned size; struct minHeapNode ** arr; }; struct minHeapNode* newNode(char data, unsigned freq){ struct minHeapNode * temp = (struct minHeapNode*) malloc(sizeof(struct minHeapNode)); temp->frequency = freq; temp->data = data; temp->left = NULL; temp->right = NULL; return temp; } struct minHeap* createMinHeap(unsigned capacity){ struct minHeap* temp = (struct minHeap*)malloc(sizeof(struct minHeap)); temp->size = 0; temp->capacity = capacity; temp->arr = (struct minHeapNode**)malloc(sizeof(struct minHeapNode*) * capacity); return temp; } void swapMinHeapNodes(struct minHeapNode** a, struct minHeapNode** b){ struct minHeapNode* temp = *a; *a = *b; *b = temp; } void heapify(struct minHeap* heap, int idx){ int smallest = idx; int left = 2*idx + 1; int right = 2*idx + 2; if(left<heap->size && heap->arr[left]->frequency < heap->arr[right]->frequency) smallest = left; if(right<heap->size && heap->arr[right]->frequency < heap->arr[smallest]->frequency) smallest = right; if(smallest!=idx){ swapMinHeapNodes(&heap->arr[smallest], &heap->arr[idx]); heapify(heap, smallest); } } bool isSize1(struct minHeap* heap){ return heap->size == 1; } struct minHeapNode* pullMin(struct minHeap* heap){ // TODO doubt why not replace the last node with null struct minHeapNode* min = heap->arr[0]; heap->arr[0] = heap->arr[heap->size -1]; --heap->size; heapify(heap, 0); return min; } void insertNode(struct minHeap* heap,struct minHeapNode* node){ ++heap->size; int curNode = heap->size -1; int root = (curNode-1)/2; while(curNode && heap->arr[curNode]->frequency > heap->arr[root]->frequency){ heap->arr[curNode] = heap->arr[root]; // TODO doubt what happens to curNode? curNode = root; root = (curNode-1)/2; } heap->arr[curNode] = node; } void buildMinHeap(struct minHeap* heap){ int n = heap->size -1; for(int i = (n-1)/2; i>=0; i--) heapify(heap, i); } void printArr(int arr[], int n) { for (int i = 0; i < n; ++i) cout << arr[i]; cout << endl; } bool isLeaf(struct minHeapNode* node){ return !(node->left) && !(node->right); } struct minHeap* createAndBuildMinHeap(char data[], int frq[], int size){ struct minHeap* heap = createMinHeap(size); for(int i = 0; i<size; i++) heap->arr[i] = newNode(data[i], frq[i]); heap->size = size; buildMinHeap(heap); return heap; } struct minHeapNode* buildHuffmanTree(char data[], int freq[], int size){ struct minHeapNode *left, *right, *top; struct minHeap* heap = createAndBuildMinHeap(data, freq, size); while(!isSize1(heap)){ left = pullMin(heap); right = pullMin(heap); top = newNode('$', left->frequency + right->frequency); top->left = left; top->right = right; insertNode(heap, top); } return pullMin(heap); } void printCodes(struct minHeapNode* root, int arr[], int top){ if(root->left){ arr[top] = 0; printCodes(root, arr, top+1); } if(root->right){ // TODO doubt if top is unchanged then it will be overwritten? arr[top] = 1; printCodes(root, arr, top+1); } if(isLeaf(root)){ cout << root->data; printArr(arr,top); } } void HuffmanCodes(char data[], int freq[], int size){ struct minHeapNode* heap = buildHuffmanTree(data, freq, size); int arr[MAX_TREE_HT], top =0; printCodes(heap, arr, top); } int main() { char arr[] = { 'a', 'b', 'c', 'd', 'e', 'f' }; int freq[] = { 5, 9, 12, 13, 16, 45 }; int size = sizeof(arr) / sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; }","title":"Huffman Coding"},{"location":"algo/greedy/improved-huffman.html","text":"Imporved Huffman Coding \u00b6 O(n) , non-deacreasing freq sorted 2 queues. Create a leaf node for each unique character and Enqueue it to the first queue in non-decreasing order of frequency. Initially second queue is empty. Dequeue two nodes with the minimum frequency by examining the front of both queues. Repeat following steps two times If second queue is empty, dequeue from first queue. If first queue is empty, dequeue from second queue. Else, compare the front of two queues and dequeue the minimum. Create a new internal node with frequency equal to the sum of the two nodes frequencies. Make the first Dequeued node as its left child and the second Dequeued node as right child. Enqueue this node to second queue. Repeat until there is more than one node in the queues. The remaining node is the root node and the tree is complete. #define MAX_TREE_HT 100 class QueueNode{ public: char data; unsigned freq; QueueNode *left, *right; }; class Queue{ public: int front, rear; int capacty; QueueNode **arr; }; QueueNode* newNode(char data, unsigned freq){ QueueNode* node = (QueueNode*)malloc(sizeof(QueueNode)); node->right = node->left = NULL; node->data = data; node->freq = freq; return node; } Queue* createQueue(unsigned capacity){ Queue* queue = (Queue*)malloc(sizeof(Queue)); queue->rear = queue->front = -1; queue->capacty = capacity; queue->arr = (QueueNode**)malloc(sizeof(QueueNode*) * capacity); return queue; } int isSizeOne(Queue* queue){ return queue->front == queue->rear && queue->front!=-1; } int isEmpty(Queue* queue){ return queue->rear == -1; } int isFull(Queue* queue){ return queue->rear == queue->capacty -1; } void enQueue(Queue* queue, QueueNode* node){ if(isFull(queue)) return; queue->arr[++queue->rear] = node; if(queue->front==-1) queue->front++; } QueueNode* deQueue(Queue* queue){ if(isEmpty(queue)) return NULL; QueueNode* node = queue->arr[queue->front]; if(queue->rear == queue->front) queue->front = queue->rear = -1; else queue->front++; return node; } QueueNode* getFront(Queue* queue){ if(isEmpty(queue)) return NULL; return queue->arr[queue->front]; } QueueNode* getMin(Queue* first, Queue* second){ if(isEmpty(first)) return deQueue(second); if(isEmpty(second)) return deQueue(first); if(getFront(first)->freq< getFront(second)->freq) return deQueue(first); else return deQueue(second); } int isLeaf(QueueNode* node){ return !(node->right) && !(node->left); } void printArr(int arr[], int N){ for(int i = 0; i<N; i++){ cout << arr[i] << \", \"; } cout << endl; } QueueNode* buildHuffmanTree(char data[], int freq[], int N){ QueueNode *top, *right, *left; Queue* firstQueue = createQueue(N); Queue* secondQueue = createQueue(N); for(int i = 0; i<N; i++) enQueue(firstQueue, newNode(data[i], freq[i])); while(!(isEmpty(firstQueue)) && !isSizeOne(secondQueue)){ left = getMin(firstQueue, secondQueue); right = getMin(firstQueue, secondQueue); top = newNode('$', left->freq + right->freq); top->left = left; top->right = right; enQueue(secondQueue, top); } return deQueue(secondQueue); } void printCodes(QueueNode* root, int arr[], int top){ if(root->left){ arr[top] = 0; printCodes(root->left, arr, top+1); } if(root->right){ arr[top] = 1; printCodes(root->right, arr, top+1); } if(isLeaf(root)){ cout << root->data <<\": \"; printArr(arr, top); } } void HuffmanCodes(char data[], int freq[], int size){ QueueNode* root = buildHuffmanTree(data, freq, size); int arr[MAX_TREE_HT] , top = 0; printCodes(root, arr, top); } int main() { char arr[] = {'a', 'b', 'c', 'd', 'e', 'f'}; int freq[] = {5, 9, 12, 13, 16, 45}; int size = sizeof(arr)/sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; }","title":"Imporved Huffman Coding"},{"location":"algo/greedy/improved-huffman.html#imporved-huffman-coding","text":"O(n) , non-deacreasing freq sorted 2 queues. Create a leaf node for each unique character and Enqueue it to the first queue in non-decreasing order of frequency. Initially second queue is empty. Dequeue two nodes with the minimum frequency by examining the front of both queues. Repeat following steps two times If second queue is empty, dequeue from first queue. If first queue is empty, dequeue from second queue. Else, compare the front of two queues and dequeue the minimum. Create a new internal node with frequency equal to the sum of the two nodes frequencies. Make the first Dequeued node as its left child and the second Dequeued node as right child. Enqueue this node to second queue. Repeat until there is more than one node in the queues. The remaining node is the root node and the tree is complete. #define MAX_TREE_HT 100 class QueueNode{ public: char data; unsigned freq; QueueNode *left, *right; }; class Queue{ public: int front, rear; int capacty; QueueNode **arr; }; QueueNode* newNode(char data, unsigned freq){ QueueNode* node = (QueueNode*)malloc(sizeof(QueueNode)); node->right = node->left = NULL; node->data = data; node->freq = freq; return node; } Queue* createQueue(unsigned capacity){ Queue* queue = (Queue*)malloc(sizeof(Queue)); queue->rear = queue->front = -1; queue->capacty = capacity; queue->arr = (QueueNode**)malloc(sizeof(QueueNode*) * capacity); return queue; } int isSizeOne(Queue* queue){ return queue->front == queue->rear && queue->front!=-1; } int isEmpty(Queue* queue){ return queue->rear == -1; } int isFull(Queue* queue){ return queue->rear == queue->capacty -1; } void enQueue(Queue* queue, QueueNode* node){ if(isFull(queue)) return; queue->arr[++queue->rear] = node; if(queue->front==-1) queue->front++; } QueueNode* deQueue(Queue* queue){ if(isEmpty(queue)) return NULL; QueueNode* node = queue->arr[queue->front]; if(queue->rear == queue->front) queue->front = queue->rear = -1; else queue->front++; return node; } QueueNode* getFront(Queue* queue){ if(isEmpty(queue)) return NULL; return queue->arr[queue->front]; } QueueNode* getMin(Queue* first, Queue* second){ if(isEmpty(first)) return deQueue(second); if(isEmpty(second)) return deQueue(first); if(getFront(first)->freq< getFront(second)->freq) return deQueue(first); else return deQueue(second); } int isLeaf(QueueNode* node){ return !(node->right) && !(node->left); } void printArr(int arr[], int N){ for(int i = 0; i<N; i++){ cout << arr[i] << \", \"; } cout << endl; } QueueNode* buildHuffmanTree(char data[], int freq[], int N){ QueueNode *top, *right, *left; Queue* firstQueue = createQueue(N); Queue* secondQueue = createQueue(N); for(int i = 0; i<N; i++) enQueue(firstQueue, newNode(data[i], freq[i])); while(!(isEmpty(firstQueue)) && !isSizeOne(secondQueue)){ left = getMin(firstQueue, secondQueue); right = getMin(firstQueue, secondQueue); top = newNode('$', left->freq + right->freq); top->left = left; top->right = right; enQueue(secondQueue, top); } return deQueue(secondQueue); } void printCodes(QueueNode* root, int arr[], int top){ if(root->left){ arr[top] = 0; printCodes(root->left, arr, top+1); } if(root->right){ arr[top] = 1; printCodes(root->right, arr, top+1); } if(isLeaf(root)){ cout << root->data <<\": \"; printArr(arr, top); } } void HuffmanCodes(char data[], int freq[], int size){ QueueNode* root = buildHuffmanTree(data, freq, size); int arr[MAX_TREE_HT] , top = 0; printCodes(root, arr, top); } int main() { char arr[] = {'a', 'b', 'c', 'd', 'e', 'f'}; int freq[] = {5, 9, 12, 13, 16, 45}; int size = sizeof(arr)/sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; }","title":"Imporved Huffman Coding"},{"location":"algo/greedy/intro.html","text":"Optimisation problems Problems having property: At every step, we can make a choice that looks best at the moment, and we get the optimal solution of the complete problem. greedy is always faster than other algos, so if a greedy is there, apply it!!!","title":"Intro"},{"location":"algo/greedy/jobs.html","text":"Jobs Selection \u00b6 O(n^2), O(n) sort profits decresingly, put slot in remaining , if no slot drop can be improved with disjoint subset struct Job{ char id; int deadline; int profit; }; bool campare(Job a, Job b){ return a.profit > b.profit; } void selectjobs(struct Job *jobs, int N){ sort(jobs, jobs+N, campare); int result[N]; int slots[N]; for(int i = 0; i<N; i++){ slots[i] = false; } for(int i = 0; i<N; i++){ for(int j= min(N, jobs[i].deadline); j>0; j--){ if(slots[j] == false){ slots[j] = true; result[j] = i; break; } } } for(int i = 0; i<N; i++){ if(slots[i]) cout << jobs[result[i]].id << \", \"; } }","title":"Jobs Selection"},{"location":"algo/greedy/jobs.html#jobs-selection","text":"O(n^2), O(n) sort profits decresingly, put slot in remaining , if no slot drop can be improved with disjoint subset struct Job{ char id; int deadline; int profit; }; bool campare(Job a, Job b){ return a.profit > b.profit; } void selectjobs(struct Job *jobs, int N){ sort(jobs, jobs+N, campare); int result[N]; int slots[N]; for(int i = 0; i<N; i++){ slots[i] = false; } for(int i = 0; i<N; i++){ for(int j= min(N, jobs[i].deadline); j>0; j--){ if(slots[j] == false){ slots[j] = true; result[j] = i; break; } } } for(int i = 0; i<N; i++){ if(slots[i]) cout << jobs[result[i]].id << \", \"; } }","title":"Jobs Selection"},{"location":"algo/greedy/min-coins.html","text":"Minimum Coins \u00b6 O(1) < O(N*logN)>, O(1) NOTE: it doesn\u2019t work for denominations {9, 6, 5, 1} and V = 11. The above approach would print 9, 1 and 1. But we can use 2 denominations 5 and 6. For general input, dynamic programming approach can be used. int minCoins(int x){ int denom[9] = { 1, 2, 5, 10, 20, 50, 100, 500, 1000}; int no = 0; for(int i = 8; i>=0 && x!=0 ; i--){ int modd = x / denom[i]; no += modd; x %= denom[i]; } cout << no; }","title":"Minimum Coins"},{"location":"algo/greedy/min-coins.html#minimum-coins","text":"O(1) < O(N*logN)>, O(1) NOTE: it doesn\u2019t work for denominations {9, 6, 5, 1} and V = 11. The above approach would print 9, 1 and 1. But we can use 2 denominations 5 and 6. For general input, dynamic programming approach can be used. int minCoins(int x){ int denom[9] = { 1, 2, 5, 10, 20, 50, 100, 500, 1000}; int no = 0; for(int i = 8; i>=0 && x!=0 ; i--){ int modd = x / denom[i]; no += modd; x %= denom[i]; } cout << no; }","title":"Minimum Coins"},{"location":"algo/greedy/min-platform.html","text":"Minimum Platform \u00b6 https://www.geeksforgeeks.org/minimum-number-platforms-required-railwaybus-station/ naive -> find how many intervals are overlapping and return max no. O(nlogn),","title":"Minimum Platform"},{"location":"algo/greedy/min-platform.html#minimum-platform","text":"https://www.geeksforgeeks.org/minimum-number-platforms-required-railwaybus-station/ naive -> find how many intervals are overlapping and return max no. O(nlogn),","title":"Minimum Platform"},{"location":"algo/greedy/mst-kruskal.html","text":"Kruskal \u00b6 O(ELogE + ELogV), E ~ O(V2), => O(LogV) = O(LogE) => O(ElogE) or O(ElogV), O(V) telephone, electrical, hydraulic, TV cable, computer, road minimmisation of cost; (V \u2013 1) edges class Edge { public: int src, dest, weight; }; class Graph { public: int V, E; Edge* edge; }; Graph* createGraph(int V, int E) { Graph* graph = new Graph; graph->V = V; graph->E = E; graph->edge = new Edge[E]; return graph; } class subset { public: int parent; int rank; }; int find(subset subsets[], int i) { if (subsets[i].parent != i) subsets[i].parent = find(subsets, subsets[i].parent); return subsets[i].parent; } void Union(subset subsets[], int x, int y) { int xroot = find(subsets, x); int yroot = find(subsets, y); if (subsets[xroot].rank < subsets[yroot].rank) subsets[xroot].parent = yroot; else if (subsets[xroot].rank > subsets[yroot].rank) subsets[yroot].parent = xroot; else { subsets[yroot].parent = xroot; subsets[xroot].rank++; } } int myComp(const void* a, const void* b) { Edge* a1 = (Edge*)a; Edge* b1 = (Edge*)b; return a1->weight > b1->weight; } void KruskalMST(Graph* graph) { int V = graph->V; Edge result[V]; int e = 0; int i = 0; qsort(graph->edge, graph->E, sizeof(graph->edge[0]), myComp); subset *subsets = new subset[( V * sizeof(subset) )]; for (int v = 0; v < V; ++v) { subsets[v].parent = v; subsets[v].rank = 0; } while (e < V - 1 && i < graph->E) { Edge next_edge = graph->edge[i++]; int x = find(subsets, next_edge.src); int y = find(subsets, next_edge.dest); if (x != y) { result[e++] = next_edge; Union(subsets, x, y); } } cout<<\"Following are the edges in the constructed MST\\n\"; for (i = 0; i < e; ++i) cout<<result[i].src<<\" -- \"<<result[i].dest<<\" == \"<<result[i].weight<<endl; return; }","title":"Kruskal"},{"location":"algo/greedy/mst-kruskal.html#kruskal","text":"O(ELogE + ELogV), E ~ O(V2), => O(LogV) = O(LogE) => O(ElogE) or O(ElogV), O(V) telephone, electrical, hydraulic, TV cable, computer, road minimmisation of cost; (V \u2013 1) edges class Edge { public: int src, dest, weight; }; class Graph { public: int V, E; Edge* edge; }; Graph* createGraph(int V, int E) { Graph* graph = new Graph; graph->V = V; graph->E = E; graph->edge = new Edge[E]; return graph; } class subset { public: int parent; int rank; }; int find(subset subsets[], int i) { if (subsets[i].parent != i) subsets[i].parent = find(subsets, subsets[i].parent); return subsets[i].parent; } void Union(subset subsets[], int x, int y) { int xroot = find(subsets, x); int yroot = find(subsets, y); if (subsets[xroot].rank < subsets[yroot].rank) subsets[xroot].parent = yroot; else if (subsets[xroot].rank > subsets[yroot].rank) subsets[yroot].parent = xroot; else { subsets[yroot].parent = xroot; subsets[xroot].rank++; } } int myComp(const void* a, const void* b) { Edge* a1 = (Edge*)a; Edge* b1 = (Edge*)b; return a1->weight > b1->weight; } void KruskalMST(Graph* graph) { int V = graph->V; Edge result[V]; int e = 0; int i = 0; qsort(graph->edge, graph->E, sizeof(graph->edge[0]), myComp); subset *subsets = new subset[( V * sizeof(subset) )]; for (int v = 0; v < V; ++v) { subsets[v].parent = v; subsets[v].rank = 0; } while (e < V - 1 && i < graph->E) { Edge next_edge = graph->edge[i++]; int x = find(subsets, next_edge.src); int y = find(subsets, next_edge.dest); if (x != y) { result[e++] = next_edge; Union(subsets, x, y); } } cout<<\"Following are the edges in the constructed MST\\n\"; for (i = 0; i < e; ++i) cout<<result[i].src<<\" -- \"<<result[i].dest<<\" == \"<<result[i].weight<<endl; return; }","title":"Kruskal"},{"location":"algo/greedy/mst-prim.html","text":"Prim's MST \u00b6 O(V^2) , O(V) Starts empty spanning tree. maintain two sets of vertices. first set -> included in mst, otherwise second set. Min weight edge from all edges that connects the sets After picking the edge, it moves the other endpoint of the edge to the set containing MST. A group of edges that connects two set of vertices in a graph is called cut in graph theory. So, at every step of Prim\u2019s algorithm, we find a cut (of two sets, one contains the vertices already included in MST and other contains rest of the vertices), pick the minimum weight edge from the cut and include this vertex to MST Set (the set that contains already included vertices). How does Prim\u2019s Algorithm Work? The idea behind Prim\u2019s algorithm is simple, a spanning tree means all vertices must be connected. So the two disjoint subsets (discussed above) of vertices must be connected to make a Spanning Tree. And they must be connected with the minimum weight edge to make it a Minimum Spanning Tree. Create a set mstSet that keeps track of vertices already included in MST. Assign a key value to all vertices in the input graph. Initialize all key values as INFINITE. Assign key value as 0 for the first vertex so that it is picked first. While mstSet doesn\u2019t include all vertices Pick a vertex u which is not there in mstSet and has minimum key value. Include u to mstSet. Update key value of all adjacent vertices of u. To update the key values, iterate through all adjacent vertices. For every adjacent vertex v, if weight of edge u-v is less than the previous key value of v, update the key value as weight of u-v The idea of using key values is to pick the minimum weight edge from cut. The key values are used only for vertices which are not yet included in MST, the key value for these vertices indicate the minimum weight edges connecting them to the set of vertices included in MST. #define V 5 int minKey(int key[], bool mstSet[]) { int min = INT_MAX, min_index; for (int v = 0; v < V; v++) if (mstSet[v] == false && key[v] < min) min = key[v], min_index = v; return min_index; } void printMST(int parent[], int graph[V][V]) { cout<<\"Edge \\tWeight\\n\"; for (int i = 1; i < V; i++) cout<<parent[i]<<\" - \"<<i<<\" \\t\"<<graph[i][parent[i]]<<\" \\n\"; } void primMST(int graph[V][V]) { int parent[V]; int key[V]; bool mstSet[V]; for (int i = 0; i < V; i++) key[i] = INT_MAX, mstSet[i] = false; key[0] = 0; parent[0] = -1; for (int count = 0; count < V - 1; count++) { int u = minKey(key, mstSet); mstSet[u] = true; for (int v = 0; v < V; v++) if (graph[u][v] && mstSet[v] == false && graph[u][v] < key[v]) parent[v] = u, key[v] = graph[u][v]; } printMST(parent, graph); } int main() { /* 2 3 (0)--(1)--(2) | / \\ | 6| 8/ \\5 |7 | / \\ | (3)-------(4) 9 */ int graph[V][V] = { { 0, 2, 0, 6, 0 }, { 2, 0, 3, 8, 5 }, { 0, 3, 0, 0, 7 }, { 6, 8, 0, 0, 9 }, { 0, 5, 7, 9, 0 } }; primMST(graph); return 0; }","title":"Prim's MST"},{"location":"algo/greedy/mst-prim.html#prims-mst","text":"O(V^2) , O(V) Starts empty spanning tree. maintain two sets of vertices. first set -> included in mst, otherwise second set. Min weight edge from all edges that connects the sets After picking the edge, it moves the other endpoint of the edge to the set containing MST. A group of edges that connects two set of vertices in a graph is called cut in graph theory. So, at every step of Prim\u2019s algorithm, we find a cut (of two sets, one contains the vertices already included in MST and other contains rest of the vertices), pick the minimum weight edge from the cut and include this vertex to MST Set (the set that contains already included vertices). How does Prim\u2019s Algorithm Work? The idea behind Prim\u2019s algorithm is simple, a spanning tree means all vertices must be connected. So the two disjoint subsets (discussed above) of vertices must be connected to make a Spanning Tree. And they must be connected with the minimum weight edge to make it a Minimum Spanning Tree. Create a set mstSet that keeps track of vertices already included in MST. Assign a key value to all vertices in the input graph. Initialize all key values as INFINITE. Assign key value as 0 for the first vertex so that it is picked first. While mstSet doesn\u2019t include all vertices Pick a vertex u which is not there in mstSet and has minimum key value. Include u to mstSet. Update key value of all adjacent vertices of u. To update the key values, iterate through all adjacent vertices. For every adjacent vertex v, if weight of edge u-v is less than the previous key value of v, update the key value as weight of u-v The idea of using key values is to pick the minimum weight edge from cut. The key values are used only for vertices which are not yet included in MST, the key value for these vertices indicate the minimum weight edges connecting them to the set of vertices included in MST. #define V 5 int minKey(int key[], bool mstSet[]) { int min = INT_MAX, min_index; for (int v = 0; v < V; v++) if (mstSet[v] == false && key[v] < min) min = key[v], min_index = v; return min_index; } void printMST(int parent[], int graph[V][V]) { cout<<\"Edge \\tWeight\\n\"; for (int i = 1; i < V; i++) cout<<parent[i]<<\" - \"<<i<<\" \\t\"<<graph[i][parent[i]]<<\" \\n\"; } void primMST(int graph[V][V]) { int parent[V]; int key[V]; bool mstSet[V]; for (int i = 0; i < V; i++) key[i] = INT_MAX, mstSet[i] = false; key[0] = 0; parent[0] = -1; for (int count = 0; count < V - 1; count++) { int u = minKey(key, mstSet); mstSet[u] = true; for (int v = 0; v < V; v++) if (graph[u][v] && mstSet[v] == false && graph[u][v] < key[v]) parent[v] = u, key[v] = graph[u][v]; } printMST(parent, graph); } int main() { /* 2 3 (0)--(1)--(2) | / \\ | 6| 8/ \\5 |7 | / \\ | (3)-------(4) 9 */ int graph[V][V] = { { 0, 2, 0, 6, 0 }, { 2, 0, 3, 8, 5 }, { 0, 3, 0, 0, 7 }, { 6, 8, 0, 0, 9 }, { 0, 5, 7, 9, 0 } }; primMST(graph); return 0; }","title":"Prim's MST"},{"location":"algo/hashing/intro.html","text":"Introduction \u00b6 operations required -> insert, search and delete Data Structure can be \u00b6 Array ---- search -> linear O(n) , or sorted -> O(logn) but insert, delete is costly Linked Listed ---- search -> O(n) , insert -> O(1), delete -> O(1) balanced BST --> O(logn) everything Direct Access table --> O(1) but space required is huge hence we use hashing O(m * 10n) space for table where m is size of a pointer to record. Another problem is an integer in a programming language may not store n digits. Hashing \u00b6 Improvement over direct access O(1) on average {under reasonable assumptions} O(n) worst case Idea -> use small numbers for big ones by hashing them. Hash Function \u00b6 Maps big numbers to small numbers Good Hash Funtion Efficiently computable. Should uniformly distribute the keys. (Each table position equally likely for each key) For example for phone numbers a bad hash function is to take first three digits Hash table \u00b6 Array that stores pointer to record of key. Collision Handling \u00b6 When 2 keys have same hash value How often? -> With only 23 persons, the probability that two people have the same birthday is 50%. Collision handling Chaining : hash table points to linked list that has same hash value... good but requires extra space outside the hash table. Open Addressing : all elements are in hash table only. Chaining \u00b6 Advantages: Simple to implement. Hash table never fills up, we can always add more elements to the chain. Less sensitive to the hash function or load factors. It is mostly used when it is unknown how many and how frequently keys may be inserted or deleted. Disadvantages: Cache performance of chaining coz a linked list. Open addressing provides better cache performance as everything is stored in the same table. Wastage of Space If chain -> long, then search time -> O(n) worst case. extra space -> links. Performace \u00b6 ''' m = Number of slots in hash table n = Number of keys to be inserted in hash table Load factor \u03b1 = n/m Expected time to search = O(1 + \u03b1) Expected time to delete = O(1 + \u03b1) Time to insert = O(1) Time complexity of search insert and delete is O(1) if \u03b1 is O(1) ''' Open Addressing \u00b6 Insert(k): Keep probing until an empty slot is found. Search(k): Keep probing until slot\u2019s key doesn\u2019t become equal to k or an empty slot is reached. Delete(k): slots of deleted keys are marked specially as \u201cdeleted\u201d. Insert can insert an item in a deleted slot, but the search doesn\u2019t stop at a deleted slot. S -> size of hash table Linear Probing : (hash(x) + i) % S Clustering: Many consecutive elements form groups and it starts taking time to find a free slot or to search an element. Quadratic Probing : (hash(x) + i i) % S Double Hashing : (hash(x) + i hash2(x)) % S Performance of Open Addressing Each key is equally likely to be hashed to any slot of the table (simple uniform hashing) m = Number of slots in the hash table n = Number of keys to be inserted in the hash table Load factor \u03b1 = n/m ( < 1 ) Expected time to search/insert/delete < 1/(1 - \u03b1) So Search, Insert and Delete take (1/(1 - \u03b1)) time Linear probing * Best cache performance * Suffers from clustering. * Easy to compute. Challenges in Linear Probing : Primary Clustering: One of the problems with linear probing is Primary clustering, many consecutive elements form groups and it starts taking time to find a free slot or to search an element. Secondary Clustering: Secondary clustering is less severe, two records do only have the same collision chain(Probe Sequence) if their initial position is the same. Quadratic probing lies between the two Double hashing * poor cache performance but * no clustering. * More computation Separate Chaining Open Addressing Simpler to implement more computation. Can add more elements to chain. table may become full. Less sensitive to the hash function or load factors. Requires extra care for to avoid clustering and load factor. Used when it is unknown how many and how frequently keys may be inserted or deleted. used when the frequency and number of keys is known. Cache performance not good as linked list. better cache performance as everything is stored in the same table. Wastage of Space (Some Parts of hash table in chaining are never used). a slot can be used even if an input doesn\u2019t map to it. extra space for links. No links","title":"Introduction"},{"location":"algo/hashing/intro.html#introduction","text":"operations required -> insert, search and delete","title":"Introduction"},{"location":"algo/hashing/intro.html#data-structure-can-be","text":"Array ---- search -> linear O(n) , or sorted -> O(logn) but insert, delete is costly Linked Listed ---- search -> O(n) , insert -> O(1), delete -> O(1) balanced BST --> O(logn) everything Direct Access table --> O(1) but space required is huge hence we use hashing O(m * 10n) space for table where m is size of a pointer to record. Another problem is an integer in a programming language may not store n digits.","title":"Data Structure can be"},{"location":"algo/hashing/intro.html#hashing","text":"Improvement over direct access O(1) on average {under reasonable assumptions} O(n) worst case Idea -> use small numbers for big ones by hashing them.","title":"Hashing"},{"location":"algo/hashing/intro.html#hash-function","text":"Maps big numbers to small numbers Good Hash Funtion Efficiently computable. Should uniformly distribute the keys. (Each table position equally likely for each key) For example for phone numbers a bad hash function is to take first three digits","title":"Hash Function"},{"location":"algo/hashing/intro.html#hash-table","text":"Array that stores pointer to record of key.","title":"Hash table"},{"location":"algo/hashing/intro.html#collision-handling","text":"When 2 keys have same hash value How often? -> With only 23 persons, the probability that two people have the same birthday is 50%. Collision handling Chaining : hash table points to linked list that has same hash value... good but requires extra space outside the hash table. Open Addressing : all elements are in hash table only.","title":"Collision Handling"},{"location":"algo/hashing/intro.html#chaining","text":"Advantages: Simple to implement. Hash table never fills up, we can always add more elements to the chain. Less sensitive to the hash function or load factors. It is mostly used when it is unknown how many and how frequently keys may be inserted or deleted. Disadvantages: Cache performance of chaining coz a linked list. Open addressing provides better cache performance as everything is stored in the same table. Wastage of Space If chain -> long, then search time -> O(n) worst case. extra space -> links.","title":"Chaining"},{"location":"algo/hashing/intro.html#performace","text":"''' m = Number of slots in hash table n = Number of keys to be inserted in hash table Load factor \u03b1 = n/m Expected time to search = O(1 + \u03b1) Expected time to delete = O(1 + \u03b1) Time to insert = O(1) Time complexity of search insert and delete is O(1) if \u03b1 is O(1) '''","title":"Performace"},{"location":"algo/hashing/intro.html#open-addressing","text":"Insert(k): Keep probing until an empty slot is found. Search(k): Keep probing until slot\u2019s key doesn\u2019t become equal to k or an empty slot is reached. Delete(k): slots of deleted keys are marked specially as \u201cdeleted\u201d. Insert can insert an item in a deleted slot, but the search doesn\u2019t stop at a deleted slot. S -> size of hash table Linear Probing : (hash(x) + i) % S Clustering: Many consecutive elements form groups and it starts taking time to find a free slot or to search an element. Quadratic Probing : (hash(x) + i i) % S Double Hashing : (hash(x) + i hash2(x)) % S Performance of Open Addressing Each key is equally likely to be hashed to any slot of the table (simple uniform hashing) m = Number of slots in the hash table n = Number of keys to be inserted in the hash table Load factor \u03b1 = n/m ( < 1 ) Expected time to search/insert/delete < 1/(1 - \u03b1) So Search, Insert and Delete take (1/(1 - \u03b1)) time Linear probing * Best cache performance * Suffers from clustering. * Easy to compute. Challenges in Linear Probing : Primary Clustering: One of the problems with linear probing is Primary clustering, many consecutive elements form groups and it starts taking time to find a free slot or to search an element. Secondary Clustering: Secondary clustering is less severe, two records do only have the same collision chain(Probe Sequence) if their initial position is the same. Quadratic probing lies between the two Double hashing * poor cache performance but * no clustering. * More computation Separate Chaining Open Addressing Simpler to implement more computation. Can add more elements to chain. table may become full. Less sensitive to the hash function or load factors. Requires extra care for to avoid clustering and load factor. Used when it is unknown how many and how frequently keys may be inserted or deleted. used when the frequency and number of keys is known. Cache performance not good as linked list. better cache performance as everything is stored in the same table. Wastage of Space (Some Parts of hash table in chaining are never used). a slot can be used even if an input doesn\u2019t map to it. extra space for links. No links","title":"Open Addressing"},{"location":"algo/hashing/itinerary-tickets.html","text":"Itinerary Tickets \u00b6 Input: \"Chennai\" -> \"Banglore\" \"Bombay\" -> \"Delhi\" \"Goa\" -> \"Chennai\" \"Delhi\" -> \"Goa\" Output: Bombay->Delhi, Delhi->Goa, Goa->Chennai, Chennai->Banglore, Given a list of tickets, find itinerary in order using the given list. It may be assumed that the input list of tickets is not cyclic and there is one ticket from every city except final destination. One Solution is to build a graph and do Topological Sorting of the graph. Time complexity of this solution is O(n). We can also use hashing to avoid building a graph. The idea is to first find the starting point. A starting point would never be on \u2018to\u2019 side of a ticket. Once we find the starting point, we can simply traverse the given map to print itinerary in order. void printItinerary(map<string, string> dataSet) { map<string, string> reversemap; map<string, string>::iterator it; for (it = dataSet.begin(); it!=dataSet.end(); it++) reversemap[it->second] = it->first; string start; for (it = dataSet.begin(); it!=dataSet.end(); it++) { if (reversemap.find(it->first) == reversemap.end()) { start = it->first; break; } } if (start.empty()) { cout << \"Invalid Input\" << endl; return; } it = dataSet.find(start); while (it != dataSet.end()) { cout << it->first << \"->\" << it->second << endl; it = dataSet.find(it->second); } }","title":"Itinerary Tickets"},{"location":"algo/hashing/itinerary-tickets.html#itinerary-tickets","text":"Input: \"Chennai\" -> \"Banglore\" \"Bombay\" -> \"Delhi\" \"Goa\" -> \"Chennai\" \"Delhi\" -> \"Goa\" Output: Bombay->Delhi, Delhi->Goa, Goa->Chennai, Chennai->Banglore, Given a list of tickets, find itinerary in order using the given list. It may be assumed that the input list of tickets is not cyclic and there is one ticket from every city except final destination. One Solution is to build a graph and do Topological Sorting of the graph. Time complexity of this solution is O(n). We can also use hashing to avoid building a graph. The idea is to first find the starting point. A starting point would never be on \u2018to\u2019 side of a ticket. Once we find the starting point, we can simply traverse the given map to print itinerary in order. void printItinerary(map<string, string> dataSet) { map<string, string> reversemap; map<string, string>::iterator it; for (it = dataSet.begin(); it!=dataSet.end(); it++) reversemap[it->second] = it->first; string start; for (it = dataSet.begin(); it!=dataSet.end(); it++) { if (reversemap.find(it->first) == reversemap.end()) { start = it->first; break; } } if (start.empty()) { cout << \"Invalid Input\" << endl; return; } it = dataSet.find(start); while (it != dataSet.end()) { cout << it->first << \"->\" << it->second << endl; it = dataSet.find(it->second); } }","title":"Itinerary Tickets"},{"location":"algo/hashing/java-hashing.html","text":"Hashing in Java \u00b6 intersection-union of 2 LL import java.util.HashMap; import java.util.HashSet; class LinkedList { Node head; class Node { int data; Node next; Node(int d) { data = d; next = null; } } void printList() { Node temp = head; while (temp != null) { System.out.print(temp.data + \" \"); temp = temp.next; } System.out.println(); } void push(int new_data) { Node new_node = new Node(new_data); new_node.next = head; head = new_node; } public void append(int new_data) { if (this.head == null) { Node n = new Node(new_data); this.head = n; return; } Node n1 = this.head; Node n2 = new Node(new_data); while (n1.next != null) { n1 = n1.next; } n1.next = n2; n2.next = null; } boolean isPresent(Node head, int data) { Node t = head; while (t != null) { if (t.data == data) return true; t = t.next; } return false; } LinkedList getIntersection(Node head1, Node head2) { HashSet<Integer> hset = new HashSet<>(); Node n1 = head1; Node n2 = head2; LinkedList result = new LinkedList(); while (n1 != null) { if (hset.contains(n1.data)) { hset.add(n1.data); } else { hset.add(n1.data); } n1 = n1.next; } while (n2 != null) { if (hset.contains(n2.data)) { result.push(n2.data); } n2 = n2.next; } return result; } LinkedList getUnion(Node head1, Node head2) { HashMap<Integer, Integer> hmap = new HashMap<>(); Node n1 = head1; Node n2 = head2; LinkedList result = new LinkedList(); while (n1 != null) { if (hmap.containsKey(n1.data)) { int val = hmap.get(n1.data); hmap.put(n1.data, val + 1); } else { hmap.put(n1.data, 1); } n1 = n1.next; } while (n2 != null) { if (hmap.containsKey(n2.data)) { int val = hmap.get(n2.data); hmap.put(n2.data, val + 1); } else { hmap.put(n2.data, 1); } n2 = n2.next; } for (int a : hmap.keySet()) { result.append(a); } return result; } public static void main(String args[]) { LinkedList llist1 = new LinkedList(); LinkedList llist2 = new LinkedList(); LinkedList union = new LinkedList(); LinkedList intersection = new LinkedList(); /*10->15->4->20 */ llist1.push(20); llist1.push(4); llist1.push(15); llist1.push(10); /*8->4->2->10 */ llist2.push(10); llist2.push(2); llist2.push(4); llist2.push(8); intersection = intersection.getIntersection(llist1.head, llist2.head); union = union.getUnion(llist1.head, llist2.head); System.out.println(\"First List is\"); llist1.printList(); System.out.println(\"Second List is\"); llist2.printList(); System.out.println(\"Intersection List is\"); intersection.printList(); System.out.println(\"Union List is\"); union.printList(); } }","title":"Hashing in Java"},{"location":"algo/hashing/java-hashing.html#hashing-in-java","text":"intersection-union of 2 LL import java.util.HashMap; import java.util.HashSet; class LinkedList { Node head; class Node { int data; Node next; Node(int d) { data = d; next = null; } } void printList() { Node temp = head; while (temp != null) { System.out.print(temp.data + \" \"); temp = temp.next; } System.out.println(); } void push(int new_data) { Node new_node = new Node(new_data); new_node.next = head; head = new_node; } public void append(int new_data) { if (this.head == null) { Node n = new Node(new_data); this.head = n; return; } Node n1 = this.head; Node n2 = new Node(new_data); while (n1.next != null) { n1 = n1.next; } n1.next = n2; n2.next = null; } boolean isPresent(Node head, int data) { Node t = head; while (t != null) { if (t.data == data) return true; t = t.next; } return false; } LinkedList getIntersection(Node head1, Node head2) { HashSet<Integer> hset = new HashSet<>(); Node n1 = head1; Node n2 = head2; LinkedList result = new LinkedList(); while (n1 != null) { if (hset.contains(n1.data)) { hset.add(n1.data); } else { hset.add(n1.data); } n1 = n1.next; } while (n2 != null) { if (hset.contains(n2.data)) { result.push(n2.data); } n2 = n2.next; } return result; } LinkedList getUnion(Node head1, Node head2) { HashMap<Integer, Integer> hmap = new HashMap<>(); Node n1 = head1; Node n2 = head2; LinkedList result = new LinkedList(); while (n1 != null) { if (hmap.containsKey(n1.data)) { int val = hmap.get(n1.data); hmap.put(n1.data, val + 1); } else { hmap.put(n1.data, 1); } n1 = n1.next; } while (n2 != null) { if (hmap.containsKey(n2.data)) { int val = hmap.get(n2.data); hmap.put(n2.data, val + 1); } else { hmap.put(n2.data, 1); } n2 = n2.next; } for (int a : hmap.keySet()) { result.append(a); } return result; } public static void main(String args[]) { LinkedList llist1 = new LinkedList(); LinkedList llist2 = new LinkedList(); LinkedList union = new LinkedList(); LinkedList intersection = new LinkedList(); /*10->15->4->20 */ llist1.push(20); llist1.push(4); llist1.push(15); llist1.push(10); /*8->4->2->10 */ llist2.push(10); llist2.push(2); llist2.push(4); llist2.push(8); intersection = intersection.getIntersection(llist1.head, llist2.head); union = union.getUnion(llist1.head, llist2.head); System.out.println(\"First List is\"); llist1.printList(); System.out.println(\"Second List is\"); llist2.printList(); System.out.println(\"Intersection List is\"); intersection.printList(); System.out.println(\"Union List is\"); union.printList(); } }","title":"Hashing in Java"},{"location":"algo/hashing/kdistance-duplicate.html","text":"K Distance Duplicate \u00b6 Given an unsorted array that may contain duplicates. Also given a number k which is smaller than size of array. Write a function that returns true if array contains duplicates within k distance. Method 1 -> 2 loops -> O(kn) \u00b6 Method 2 -> Hashing \u00b6 \u0398(n) bool checkDuplicatesWithinK(int arr[], int n, int k) { unordered_set<int> myset; for (int i = 0; i < n; i++) { if (myset.find(arr[i]) != myset.end()) return true; myset.insert(arr[i]); if (i >= k) myset.erase(arr[i-k]); } return false; }","title":"K Distance Duplicate"},{"location":"algo/hashing/kdistance-duplicate.html#k-distance-duplicate","text":"Given an unsorted array that may contain duplicates. Also given a number k which is smaller than size of array. Write a function that returns true if array contains duplicates within k distance.","title":"K Distance Duplicate"},{"location":"algo/hashing/kdistance-duplicate.html#method-1-2-loops-okn","text":"","title":"Method 1 -&gt; 2 loops -&gt; O(kn)"},{"location":"algo/hashing/kdistance-duplicate.html#method-2-hashing","text":"\u0398(n) bool checkDuplicatesWithinK(int arr[], int n, int k) { unordered_set<int> myset; for (int i = 0; i < n; i++) { if (myset.find(arr[i]) != myset.end()) return true; myset.insert(arr[i]); if (i >= k) myset.erase(arr[i-k]); } return false; }","title":"Method 2 -&gt; Hashing"},{"location":"algo/hashing/subset-array.html","text":"Subset of Array \u00b6 Given two arrays: arr1[0..m-1] and arr2[0..n-1]. Find whether arr2[] is a subset of arr1[] or not. Both the arrays are not in sorted order. It may be assumed that elements in both array are distinct. Method 1 -> simple (2 loops) \u00b6 O(m*n) bool isSubset(int arr1[], int arr2[], int m, int n) { int i = 0; int j = 0; for (i = 0; i < n; i++) { for (j = 0; j < m; j++) { if (arr2[i] == arr1[j]) break; } if (j == m) return 0; } return 1; } Method 2 -> sorting and binary search \u00b6 O(mLogm + nLogm). -> merge sort, quick sort -> O(m^2) bool isSubset(int arr1[], int arr2[], int m, int n) { int i = 0; quickSort(arr1, 0, m - 1); for (i = 0; i < n; i++) { if (binarySearch(arr1, 0, m - 1, arr2[i]) == -1) return 0; } return 1; } Method 3 -> sorting and merging \u00b6 O(mLogm + nLogn) bool isSubset(int arr1[], int arr2[], int m, int n) { int i = 0, j = 0; if (m < n) return 0; sort(arr1, arr1 + m); sort(arr2, arr2 + n); while (i < n && j < m) { if (arr1[j] < arr2[i]) j++; else if (arr1[j] == arr2[i]) { j++; i++; } else if (arr1[j] > arr2[i]) return 0; } return (i < n) ? false : true; } Method 4 -> Hashing \u00b6 bool isSubset(int arr1[], int m, int arr2[], int n) { set<int>hashset; for(int i=0;i<m;i++) { hashset.insert(arr1[i]); } for(int i=0;i<n;i++) { if(hashset.find(arr2[i])==hashset.end()) return false; } return true; } Note that method 1, method 2 and method 4 don\u2019t handle the cases when we have duplicates in arr2[]. For example, {1, 4, 4, 2} is not a subset of {1, 4, 2}, but these methods will print it as a subset.","title":"Subset of Array"},{"location":"algo/hashing/subset-array.html#subset-of-array","text":"Given two arrays: arr1[0..m-1] and arr2[0..n-1]. Find whether arr2[] is a subset of arr1[] or not. Both the arrays are not in sorted order. It may be assumed that elements in both array are distinct.","title":"Subset of Array"},{"location":"algo/hashing/subset-array.html#method-1-simple-2-loops","text":"O(m*n) bool isSubset(int arr1[], int arr2[], int m, int n) { int i = 0; int j = 0; for (i = 0; i < n; i++) { for (j = 0; j < m; j++) { if (arr2[i] == arr1[j]) break; } if (j == m) return 0; } return 1; }","title":"Method 1 -&gt; simple (2 loops)"},{"location":"algo/hashing/subset-array.html#method-2-sorting-and-binary-search","text":"O(mLogm + nLogm). -> merge sort, quick sort -> O(m^2) bool isSubset(int arr1[], int arr2[], int m, int n) { int i = 0; quickSort(arr1, 0, m - 1); for (i = 0; i < n; i++) { if (binarySearch(arr1, 0, m - 1, arr2[i]) == -1) return 0; } return 1; }","title":"Method 2 -&gt; sorting and binary search"},{"location":"algo/hashing/subset-array.html#method-3-sorting-and-merging","text":"O(mLogm + nLogn) bool isSubset(int arr1[], int arr2[], int m, int n) { int i = 0, j = 0; if (m < n) return 0; sort(arr1, arr1 + m); sort(arr2, arr2 + n); while (i < n && j < m) { if (arr1[j] < arr2[i]) j++; else if (arr1[j] == arr2[i]) { j++; i++; } else if (arr1[j] > arr2[i]) return 0; } return (i < n) ? false : true; }","title":"Method 3 -&gt; sorting and merging"},{"location":"algo/hashing/subset-array.html#method-4-hashing","text":"bool isSubset(int arr1[], int m, int arr2[], int n) { set<int>hashset; for(int i=0;i<m;i++) { hashset.insert(arr1[i]); } for(int i=0;i<n;i++) { if(hashset.find(arr2[i])==hashset.end()) return false; } return true; } Note that method 1, method 2 and method 4 don\u2019t handle the cases when we have duplicates in arr2[]. For example, {1, 4, 4, 2} is not a subset of {1, 4, 2}, but these methods will print it as a subset.","title":"Method 4 -&gt; Hashing"},{"location":"algo/hashing/sum-2-array.html","text":"Sum of 2 = X \u00b6 Write a program that, given an array A[] of n numbers and another number x, determines whether or not there exist two elements in S whose sum is exactly x. Method 1: Sorting and Two-Pointers technique. \u00b6 Time Complexity: Merge Sort or Heap Sort -> O(nlogn) Quick Sort -> O(n^2) Auxiliary Space: O(n) for merge sort and O(1) for Heap Sort. bool hasArrayTwoCandidates(int A[], int arr_size, int sum) { int l, r; sort(A, A + arr_size); l = 0; r = arr_size - 1; while (l < r) { if (A[l] + A[r] == sum) return 1; else if (A[l] + A[r] < sum) l++; else r--; } return 0; } Method 2: Hashing \u00b6 Time Complexity: O(n). Auxiliary Space: O(n). void printPairs(int arr[], int arr_size, int sum) { unordered_set<int> s; for (int i = 0; i < arr_size; i++) { int temp = sum - arr[i]; if (s.find(temp) != s.end()) cout << \"Pair with given sum \" << sum << \" is ( \" << arr[i] << \", \" << temp << \")\" << endl; s.insert(arr[i]); } } Method 3: Using remainders of the elements less than x. \u00b6 Time Complexity: O(n+x) Auxiliary Space: O(x) void printPairs(int a[], int n, int x) { int i; int rem[x]; for (i = 0; i < x; i++) { rem[i] = 0; } for (i = 0; i < n; i++) { if (a[i] < x) { rem[a[i] % x]++; } } for (i = 1; i < x / 2; i++) { if (rem[i] > 0 && rem[x - i] > 0) { cout << \"Yes\" << \"\\n\"; break; } } if (i >= x / 2) { if (x % 2 == 0) { if (rem[x / 2] > 1) { cout << \"Yes\" << \"\\n\"; } else { cout << \"No\" << \"\\n\"; } } else { if (rem[x / 2] > 0 && rem[x - x / 2] > 0) { cout << \"Yes\" << \"\\n\"; } else { cout << \"No\" << \"\\n\"; } } } }","title":"Sum of 2 = X"},{"location":"algo/hashing/sum-2-array.html#sum-of-2-x","text":"Write a program that, given an array A[] of n numbers and another number x, determines whether or not there exist two elements in S whose sum is exactly x.","title":"Sum of 2 = X"},{"location":"algo/hashing/sum-2-array.html#method-1-sorting-and-two-pointers-technique","text":"Time Complexity: Merge Sort or Heap Sort -> O(nlogn) Quick Sort -> O(n^2) Auxiliary Space: O(n) for merge sort and O(1) for Heap Sort. bool hasArrayTwoCandidates(int A[], int arr_size, int sum) { int l, r; sort(A, A + arr_size); l = 0; r = arr_size - 1; while (l < r) { if (A[l] + A[r] == sum) return 1; else if (A[l] + A[r] < sum) l++; else r--; } return 0; }","title":"Method 1: Sorting and Two-Pointers technique."},{"location":"algo/hashing/sum-2-array.html#method-2-hashing","text":"Time Complexity: O(n). Auxiliary Space: O(n). void printPairs(int arr[], int arr_size, int sum) { unordered_set<int> s; for (int i = 0; i < arr_size; i++) { int temp = sum - arr[i]; if (s.find(temp) != s.end()) cout << \"Pair with given sum \" << sum << \" is ( \" << arr[i] << \", \" << temp << \")\" << endl; s.insert(arr[i]); } }","title":"Method 2: Hashing"},{"location":"algo/hashing/sum-2-array.html#method-3-using-remainders-of-the-elements-less-than-x","text":"Time Complexity: O(n+x) Auxiliary Space: O(x) void printPairs(int a[], int n, int x) { int i; int rem[x]; for (i = 0; i < x; i++) { rem[i] = 0; } for (i = 0; i < n; i++) { if (a[i] < x) { rem[a[i] % x]++; } } for (i = 1; i < x / 2; i++) { if (rem[i] > 0 && rem[x - i] > 0) { cout << \"Yes\" << \"\\n\"; break; } } if (i >= x / 2) { if (x % 2 == 0) { if (rem[x / 2] > 1) { cout << \"Yes\" << \"\\n\"; } else { cout << \"No\" << \"\\n\"; } } else { if (rem[x / 2] > 0 && rem[x - x / 2] > 0) { cout << \"Yes\" << \"\\n\"; } else { cout << \"No\" << \"\\n\"; } } } }","title":"Method 3: Using remainders of the elements less than x."},{"location":"algo/hashing/tree-print-vertical.html","text":"Print Vertical Tree \u00b6 O(n) under the assumption that we have good hashing function but map stl -> self balancing binary tree -> O(nlogn) the above solution may not print nodes in same vertical order as they appear in tree void getVerticalOrder(Node* root, int hd, map<int, vector<int>> &m) { if (root == NULL) return; m[hd].push_back(root->key);- getVerticalOrder(root->left, hd-1, m); getVerticalOrder(root->right, hd+1, m); } void printVerticalOrder(Node* root) { map < int,vector<int> > m; int hd = 0; getVerticalOrder(root, hd,m); map< int,vector<int> > :: iterator it; for (it=m.begin(); it!=m.end(); it++) { for (int i=0; i<it->second.size(); ++i) cout << it->second[i] << \" \"; cout << endl; } }","title":"Print Vertical Tree"},{"location":"algo/hashing/tree-print-vertical.html#print-vertical-tree","text":"O(n) under the assumption that we have good hashing function but map stl -> self balancing binary tree -> O(nlogn) the above solution may not print nodes in same vertical order as they appear in tree void getVerticalOrder(Node* root, int hd, map<int, vector<int>> &m) { if (root == NULL) return; m[hd].push_back(root->key);- getVerticalOrder(root->left, hd-1, m); getVerticalOrder(root->right, hd+1, m); } void printVerticalOrder(Node* root) { map < int,vector<int> > m; int hd = 0; getVerticalOrder(root, hd,m); map< int,vector<int> > :: iterator it; for (it=m.begin(); it!=m.end(); it++) { for (int i=0; i<it->second.size(); ++i) cout << it->second[i] << \" \"; cout << endl; } }","title":"Print Vertical Tree"},{"location":"algo/hashing/union-intersection.html","text":"Union & Intersection \u00b6 Given two Linked Lists, create union and intersection lists that contain union and intersection of the elements present in the given lists. Order of elements in output lists doesn\u2019t matter. Method 1 -> Simple \u00b6 Time Complexity: O(m*n). Auxiliary Space: O(1). struct Node* getUnion( struct Node* head1, struct Node* head2) { struct Node* result = NULL; struct Node *t1 = head1, *t2 = head2; while (t1 != NULL) { push(&result, t1->data); t1 = t1->next; } while (t2 != NULL) { if (!isPresent(result, t2->data)) push(&result, t2->data); t2 = t2->next; } return result; } struct Node* getIntersection(struct Node* head1, struct Node* head2) { struct Node* result = NULL; struct Node* t1 = head1; while (t1 != NULL) { if (isPresent(head2, t1->data)) push(&result, t1->data); t1 = t1->next; } return result; } bool isPresent(struct Node* head, int data) { struct Node* t = head; while (t != NULL) { if (t->data == data) return 1; t = t->next; } return 0; } Method 2 -> merge sort \u00b6 O(mLogm + nLogn) 1. merge sort both lists 2. traverse and add (if not present for uninun and if present add) Method 3 -> Hashing \u00b6 Time Complexity: O(m+n). (assuming hashset alsgos -> O(1) ) Auxiliary Space:O(m+n). see file","title":"Union & Intersection"},{"location":"algo/hashing/union-intersection.html#union-intersection","text":"Given two Linked Lists, create union and intersection lists that contain union and intersection of the elements present in the given lists. Order of elements in output lists doesn\u2019t matter.","title":"Union &amp; Intersection"},{"location":"algo/hashing/union-intersection.html#method-1-simple","text":"Time Complexity: O(m*n). Auxiliary Space: O(1). struct Node* getUnion( struct Node* head1, struct Node* head2) { struct Node* result = NULL; struct Node *t1 = head1, *t2 = head2; while (t1 != NULL) { push(&result, t1->data); t1 = t1->next; } while (t2 != NULL) { if (!isPresent(result, t2->data)) push(&result, t2->data); t2 = t2->next; } return result; } struct Node* getIntersection(struct Node* head1, struct Node* head2) { struct Node* result = NULL; struct Node* t1 = head1; while (t1 != NULL) { if (isPresent(head2, t1->data)) push(&result, t1->data); t1 = t1->next; } return result; } bool isPresent(struct Node* head, int data) { struct Node* t = head; while (t != NULL) { if (t->data == data) return 1; t = t->next; } return 0; }","title":"Method 1 -&gt; Simple"},{"location":"algo/hashing/union-intersection.html#method-2-merge-sort","text":"O(mLogm + nLogn) 1. merge sort both lists 2. traverse and add (if not present for uninun and if present add)","title":"Method 2 -&gt; merge sort"},{"location":"algo/hashing/union-intersection.html#method-3-hashing","text":"Time Complexity: O(m+n). (assuming hashset alsgos -> O(1) ) Auxiliary Space:O(m+n). see file","title":"Method 3 -&gt; Hashing"},{"location":"algo/misc/automata.html","text":"Automata \u00b6 The finite automata or finite state machine is an abstract machine which have five elements or tuple. It has a set of states and rules for moving from one state to another but it depends upon the applied input symbol. Basically it is an abstract model pf digital computer. simplest machine to recognize patterns Q : Finite set of states. \u03a3 : set of Input Symbols. q : Initial state. F : set of Final States. \u03b4 : Transition Function. { Q, \u03a3, q, F, \u03b4 } Deterministic Finite Automata (DFA) \u00b6 DFA consists of 5 tuples {Q, \u03a3, q, F, \u03b4}. Q : set of all states. \u03a3 : set of input symbols. ( Symbols which machine takes as input ) q : Initial state. ( Starting state of a machine ) F : set of final state. \u03b4 : Transition Function, defined as \u03b4 : Q X \u03a3 --> Q. In a DFA, for a particular input character, the machine goes to one state only. A transition function is defined on every state for every input symbol. Also in DFA null (or \u03b5) move is not allowed, i.e., DFA cannot change state without any input character. For example, below DFA with \u03a3 = {0, 1} accepts all strings ending with 0. One important thing to note is, there can be many possible DFAs for a pattern. A DFA with minimum number of states is generally preferred. Nondeterministic Finite Automata(NFA) \u00b6 NFA is similar to DFA except following additional features: Null (or \u03b5) move is allowed i.e., it can move forward without reading symbols. Ability to transmit to any number of states for a particular input. However, these above features don\u2019t add any power to NFA. If we compare both in terms of power, both are equivalent. Due to above additional features, NFA has a different transition function, rest is same as DFA. \u03b4: Transition Function \u03b4: Q X (\u03a3 U \u03b5 ) --> 2 ^ Q. As you can see in transition function is for any input including null (or \u03b5), NFA can go to any state number of states. For example, below is a NFA for above problem One important thing to note is, in NFA, if any path for an input string leads to a final state, then the input string accepted. For example, in above NFA, there are multiple paths for input string \u201c00\u201d. Since, one of the paths leads to a final state, \u201c00\u201d is accepted by above NFA. Some Important Points: Justification: Since all the tuples in DFA and NFA are the same except for one of the tuples, which is Transition Function (\u03b4) In case of DFA \u03b4 : Q X \u03a3 --> Q In case of NFA \u03b4 : Q X \u03a3 --> 2Q Now if you observe you\u2019ll find out Q X \u03a3 \u2013> Q is part of Q X \u03a3 \u2013> 2Q. In the RHS side, Q is the subset of 2Q which indicates Q is contained in 2Q or Q is a part of 2Q, however, the reverse isn\u2019t true. So mathematically, we can conclude that every DFA is NFA but not vice-versa. Yet there is a way to convert an NFA to DFA, so there exists an equivalent DFA for every NFA. Both NFA and DFA have same power and each NFA can be translated into a DFA. There can be multiple final states in both DFA and NFA. NFA is more of a theoretical concept. DFA is used in Lexical Analysis in Compiler. TODO http://quiz.geeksforgeeks.org/regular-languages-and-finite-automata/","title":"Automata"},{"location":"algo/misc/automata.html#automata","text":"The finite automata or finite state machine is an abstract machine which have five elements or tuple. It has a set of states and rules for moving from one state to another but it depends upon the applied input symbol. Basically it is an abstract model pf digital computer. simplest machine to recognize patterns Q : Finite set of states. \u03a3 : set of Input Symbols. q : Initial state. F : set of Final States. \u03b4 : Transition Function. { Q, \u03a3, q, F, \u03b4 }","title":"Automata"},{"location":"algo/misc/automata.html#deterministic-finite-automata-dfa","text":"DFA consists of 5 tuples {Q, \u03a3, q, F, \u03b4}. Q : set of all states. \u03a3 : set of input symbols. ( Symbols which machine takes as input ) q : Initial state. ( Starting state of a machine ) F : set of final state. \u03b4 : Transition Function, defined as \u03b4 : Q X \u03a3 --> Q. In a DFA, for a particular input character, the machine goes to one state only. A transition function is defined on every state for every input symbol. Also in DFA null (or \u03b5) move is not allowed, i.e., DFA cannot change state without any input character. For example, below DFA with \u03a3 = {0, 1} accepts all strings ending with 0. One important thing to note is, there can be many possible DFAs for a pattern. A DFA with minimum number of states is generally preferred.","title":"Deterministic Finite Automata (DFA)"},{"location":"algo/misc/automata.html#nondeterministic-finite-automatanfa","text":"NFA is similar to DFA except following additional features: Null (or \u03b5) move is allowed i.e., it can move forward without reading symbols. Ability to transmit to any number of states for a particular input. However, these above features don\u2019t add any power to NFA. If we compare both in terms of power, both are equivalent. Due to above additional features, NFA has a different transition function, rest is same as DFA. \u03b4: Transition Function \u03b4: Q X (\u03a3 U \u03b5 ) --> 2 ^ Q. As you can see in transition function is for any input including null (or \u03b5), NFA can go to any state number of states. For example, below is a NFA for above problem One important thing to note is, in NFA, if any path for an input string leads to a final state, then the input string accepted. For example, in above NFA, there are multiple paths for input string \u201c00\u201d. Since, one of the paths leads to a final state, \u201c00\u201d is accepted by above NFA. Some Important Points: Justification: Since all the tuples in DFA and NFA are the same except for one of the tuples, which is Transition Function (\u03b4) In case of DFA \u03b4 : Q X \u03a3 --> Q In case of NFA \u03b4 : Q X \u03a3 --> 2Q Now if you observe you\u2019ll find out Q X \u03a3 \u2013> Q is part of Q X \u03a3 \u2013> 2Q. In the RHS side, Q is the subset of 2Q which indicates Q is contained in 2Q or Q is a part of 2Q, however, the reverse isn\u2019t true. So mathematically, we can conclude that every DFA is NFA but not vice-versa. Yet there is a way to convert an NFA to DFA, so there exists an equivalent DFA for every NFA. Both NFA and DFA have same power and each NFA can be translated into a DFA. There can be multiple final states in both DFA and NFA. NFA is more of a theoretical concept. DFA is used in Lexical Analysis in Compiler. TODO http://quiz.geeksforgeeks.org/regular-languages-and-finite-automata/","title":"Nondeterministic Finite Automata(NFA)"},{"location":"algo/misc/locallity.html","text":"Locallity \u00b6 Data read operation in registers is generally 100 times faster than in the main memory Caches are installed in the middle of CPU registers and the main memory to bridge this time gap in data reading. The idea of caching the useful data centers around a fundamental property of computer programs known as locality. Programs with good locality tend to access the same set of data items over and over again from the upper levels of the memory hierarchy (i.e. cache) and thus run faster. Example: The run time of different matrix multiplication kernels that perform the same number of arithmetic operations, but have different degrees of locality, can vary by a factor of 20! Types of Locality \u00b6 Temporal locality : Temporal locality states that, the same data objects are likely to be reused multiple times by the CPU during the execution of a program Spatial locality : It states that if a data object is referenced once, then there is a high probability that it\u2019s neighbor data objects will also be referenced in near future. Importance of Locality \u00b6 In operating systems, the principle of locality allows the system to use main memory as a cache of the most recently referenced chunk of virtual address space and also in case of recently used disk blocks in disk file systems. Cache Friendly Code \u00b6 Frequently used cases need to be faster : Programs often invest most of the time in a few core functions and these functions in return have most to do with the loops. So, these loops should be designed in a way that they possess a good locality. Multiple loops : If a program constitutes of multiple loops then minimize the cache misses in the inner loop to alleviate the performance of the code. eg \u00b6 Row major: \u00b6 int sumarrayrows(int a[8][4]) { int i, j, sum = 0; for (i = 0; i < 8; i++) for (j = 0; j < 4; j++) sum += a[i][j]; return sum; } c -> row major storing of 2 dimensional array. cache hit -> 75 % in block size of 4 words each. Column Major \u00b6 int sum_array(int a[8][8]) { int i, j, sum = 0; for (j = 0; j < 8; j++) for (i = 0; i < 8; i++) sum += a[i][j]; return sum; } As C stores arrays in row-major order but in this case array is being accessed in column major order, so the locality spoils in this case. the references will be made in order: a[0][0], a[1][0], a[2][0] and so on. Conclusion \u00b6 A good example is Quick sort. Though it has a worst case complexity of O(n2), it is the most popular sorting algorithm and one of the important factor is the better cache performance than many other sorting algorithms.","title":"Locallity"},{"location":"algo/misc/locallity.html#locallity","text":"Data read operation in registers is generally 100 times faster than in the main memory Caches are installed in the middle of CPU registers and the main memory to bridge this time gap in data reading. The idea of caching the useful data centers around a fundamental property of computer programs known as locality. Programs with good locality tend to access the same set of data items over and over again from the upper levels of the memory hierarchy (i.e. cache) and thus run faster. Example: The run time of different matrix multiplication kernels that perform the same number of arithmetic operations, but have different degrees of locality, can vary by a factor of 20!","title":"Locallity"},{"location":"algo/misc/locallity.html#types-of-locality","text":"Temporal locality : Temporal locality states that, the same data objects are likely to be reused multiple times by the CPU during the execution of a program Spatial locality : It states that if a data object is referenced once, then there is a high probability that it\u2019s neighbor data objects will also be referenced in near future.","title":"Types of Locality"},{"location":"algo/misc/locallity.html#importance-of-locality","text":"In operating systems, the principle of locality allows the system to use main memory as a cache of the most recently referenced chunk of virtual address space and also in case of recently used disk blocks in disk file systems.","title":"Importance of Locality"},{"location":"algo/misc/locallity.html#cache-friendly-code","text":"Frequently used cases need to be faster : Programs often invest most of the time in a few core functions and these functions in return have most to do with the loops. So, these loops should be designed in a way that they possess a good locality. Multiple loops : If a program constitutes of multiple loops then minimize the cache misses in the inner loop to alleviate the performance of the code.","title":"Cache Friendly Code"},{"location":"algo/misc/locallity.html#eg","text":"","title":"eg"},{"location":"algo/misc/locallity.html#row-major","text":"int sumarrayrows(int a[8][4]) { int i, j, sum = 0; for (i = 0; i < 8; i++) for (j = 0; j < 4; j++) sum += a[i][j]; return sum; } c -> row major storing of 2 dimensional array. cache hit -> 75 % in block size of 4 words each.","title":"Row major:"},{"location":"algo/misc/locallity.html#column-major","text":"int sum_array(int a[8][8]) { int i, j, sum = 0; for (j = 0; j < 8; j++) for (i = 0; i < 8; i++) sum += a[i][j]; return sum; } As C stores arrays in row-major order but in this case array is being accessed in column major order, so the locality spoils in this case. the references will be made in order: a[0][0], a[1][0], a[2][0] and so on.","title":"Column Major"},{"location":"algo/misc/locallity.html#conclusion","text":"A good example is Quick sort. Though it has a worst case complexity of O(n2), it is the most popular sorting algorithm and one of the important factor is the better cache performance than many other sorting algorithms.","title":"Conclusion"},{"location":"algo/misc/recursion.html","text":"Recursion \u00b6 Tail Recursive \u00b6 (Stack Overflow)[https://stackoverflow.com/questions/33923/what-is-tail-recursion]","title":"Recursion"},{"location":"algo/misc/recursion.html#recursion","text":"","title":"Recursion"},{"location":"algo/misc/recursion.html#tail-recursive","text":"(Stack Overflow)[https://stackoverflow.com/questions/33923/what-is-tail-recursion]","title":"Tail Recursive"},{"location":"algo/misc/segmentation-fault.html","text":"Segmentation Fault \u00b6 Core Dump/Segmentation fault is a specific kind of error caused by accessing memory that \u201cdoes not belong to you.\u201d When a piece of code tries to do read and write operation in a read only location in memory or freed block of memory, it is known as core dump. It is an error indicating memory corruption. Modifying a string literal Accessing an address that is freed Accessing out of array index bounds Improper use of scanf() Stack Overflow Dereferencing uninitialized pointer","title":"Segmentation Fault"},{"location":"algo/misc/segmentation-fault.html#segmentation-fault","text":"Core Dump/Segmentation fault is a specific kind of error caused by accessing memory that \u201cdoes not belong to you.\u201d When a piece of code tries to do read and write operation in a read only location in memory or freed block of memory, it is known as core dump. It is an error indicating memory corruption. Modifying a string literal Accessing an address that is freed Accessing out of array index bounds Improper use of scanf() Stack Overflow Dereferencing uninitialized pointer","title":"Segmentation Fault"},{"location":"algo/misc/swap.html","text":"Swapping \u00b6 void swap(int *xp, int *yp) { int temp = *xp; *xp = *yp; *yp = temp; }","title":"Swapping"},{"location":"algo/misc/swap.html#swapping","text":"void swap(int *xp, int *yp) { int temp = *xp; *xp = *yp; *yp = temp; }","title":"Swapping"},{"location":"algo/search/binary.html","text":"Binary Search \u00b6 requirements : sorted array time : theta(logn) Auxiliary Space : O(1) -> iterative, O(Logn) -> recursion Problem -> m = (l+r)/2 might overflow \u00b6 use -> mid = low + ((high - low) / 2); in java .. faster -> mid = (low + high) >>> 1; in c++ ... -> mid = ((unsigned int)low + (unsigned int)high)) >> 1 http://locklessinc.com/articles/binary_search/ ----> above solutions may not always work. The above problem occurs when array length is 230 or greater and the search repeatedly moves to second half of the array. This much size of array is not likely to appear most of the time. For example, when we try the below program with 32 bit Code Blocks compiler, we get compiler error. int main() { int arr[1<<30]; return 0; } error: size of array 'arr' is too large Even when we try boolean array, the program compiles fine, but crashes when run in Windows 7.0 and Code Blocks 32 bit compiler #include <stdbool.h> int main() { bool arr[1<<30]; return 0; } Output: No compiler error, but crashes at run time. Recursive \u00b6 int binarySearch(int arr[], int l, int r, int x) { if (r >= l) { int mid = l + (r - l) / 2; if (arr[mid] == x) return mid; if (arr[mid] > x) return binarySearch(arr, l, mid - 1, x); return binarySearch(arr, mid + 1, r, x); } return -1; } Iterative \u00b6 int binarySearch(int arr[], int l, int r, int x) { while (l <= r) { int m = l + (r - l) / 2; if (arr[m] == x) return m; if (arr[m] < x) l = m + 1; else r = m - 1; } return -1; }","title":"Binary Search"},{"location":"algo/search/binary.html#binary-search","text":"requirements : sorted array time : theta(logn) Auxiliary Space : O(1) -> iterative, O(Logn) -> recursion","title":"Binary Search"},{"location":"algo/search/binary.html#problem-m-lr2-might-overflow","text":"use -> mid = low + ((high - low) / 2); in java .. faster -> mid = (low + high) >>> 1; in c++ ... -> mid = ((unsigned int)low + (unsigned int)high)) >> 1 http://locklessinc.com/articles/binary_search/ ----> above solutions may not always work. The above problem occurs when array length is 230 or greater and the search repeatedly moves to second half of the array. This much size of array is not likely to appear most of the time. For example, when we try the below program with 32 bit Code Blocks compiler, we get compiler error. int main() { int arr[1<<30]; return 0; } error: size of array 'arr' is too large Even when we try boolean array, the program compiles fine, but crashes when run in Windows 7.0 and Code Blocks 32 bit compiler #include <stdbool.h> int main() { bool arr[1<<30]; return 0; } Output: No compiler error, but crashes at run time.","title":"Problem -&gt; m = (l+r)/2 might overflow"},{"location":"algo/search/binary.html#recursive","text":"int binarySearch(int arr[], int l, int r, int x) { if (r >= l) { int mid = l + (r - l) / 2; if (arr[mid] == x) return mid; if (arr[mid] > x) return binarySearch(arr, l, mid - 1, x); return binarySearch(arr, mid + 1, r, x); } return -1; }","title":"Recursive"},{"location":"algo/search/binary.html#iterative","text":"int binarySearch(int arr[], int l, int r, int x) { while (l <= r) { int m = l + (r - l) / 2; if (arr[m] == x) return m; if (arr[m] < x) l = m + 1; else r = m - 1; } return -1; }","title":"Iterative"},{"location":"algo/search/exponential.html","text":"Exponential Search \u00b6 Time : O(Log n) Space : O(Log n) / O(1) useful : for unbounded searches, where size of array is infinite. search is closer to the first element Idea : Find range where element is present Do Binary Search in above found range. How to find the range where element may be present? The idea is to start with subarray size 1, compare its last element with x, then try size 2, then 4 and so on until last element of a subarray is not greater. Once we find an index i (after repeated doubling of i), we know that the element must be present between i/2 and i (Why i/2? because we could not find a greater value in previous iteration) int exponentialSearch(int arr[], int n, int x) { if (arr[0] == x) return 0; int i = 1; while (i < n && arr[i] <= x) i = i*2; return binarySearch(arr, i/2, min(i, n-1), x); }","title":"Exponential Search"},{"location":"algo/search/exponential.html#exponential-search","text":"Time : O(Log n) Space : O(Log n) / O(1) useful : for unbounded searches, where size of array is infinite. search is closer to the first element Idea : Find range where element is present Do Binary Search in above found range. How to find the range where element may be present? The idea is to start with subarray size 1, compare its last element with x, then try size 2, then 4 and so on until last element of a subarray is not greater. Once we find an index i (after repeated doubling of i), we know that the element must be present between i/2 and i (Why i/2? because we could not find a greater value in previous iteration) int exponentialSearch(int arr[], int n, int x) { if (arr[0] == x) return 0; int i = 1; while (i < n && arr[i] <= x) i = i*2; return binarySearch(arr, i/2, min(i, n-1), x); }","title":"Exponential Search"},{"location":"algo/search/interpolation.html","text":"interpolation Search \u00b6 requirements : sorted array time : uniformly distributed -> O (log log n). worst case -> O(n). space : O(1) Improved over binary when the data is uniformaly distributed pos = lo + [ (x-arr[lo])*(hi-lo) / (arr[hi]-arr[Lo]) ] Iterative \u00b6 int interpolationSearch(int arr[], int n, int x) { int lo = 0, hi = (n - 1); while (lo <= hi && x >= arr[lo] && x <= arr[hi]) { if (lo == hi) { if (arr[lo] == x) return lo; return -1; } int pos = lo + (((double)(hi - lo) / (arr[hi] - arr[lo])) * (x - arr[lo])); if (arr[pos] == x) return pos; if (arr[pos] < x) lo = pos + 1; else hi = pos - 1; } return -1; } Reursive \u00b6 int interpolationSearch(int arr[], int lo, int hi, int x) { int pos; if ( lo <= hi && x >= arr[lo] && x <= arr[hi]) { pos = lo + (((double)( hi - lo ) / (arr[hi] - arr[lo])) * (x - arr[lo])); if( arr[pos] == x ) return pos; if( arr[pos] < x ) return interpolationSearch(arr, pos + 1, hi, x); if( arr[pos] > x ) return interpolationSearch(arr, lo, pos - 1, x); } return -1; } Interpolation vs Binary \u00b6 log(log(n)) comparisons -> uniformly distributed In the worst case: (for instance where the numerical values of the keys increase exponentially) It can make up to O(n) comparisons.","title":"interpolation Search"},{"location":"algo/search/interpolation.html#interpolation-search","text":"requirements : sorted array time : uniformly distributed -> O (log log n). worst case -> O(n). space : O(1) Improved over binary when the data is uniformaly distributed pos = lo + [ (x-arr[lo])*(hi-lo) / (arr[hi]-arr[Lo]) ]","title":"interpolation Search"},{"location":"algo/search/interpolation.html#iterative","text":"int interpolationSearch(int arr[], int n, int x) { int lo = 0, hi = (n - 1); while (lo <= hi && x >= arr[lo] && x <= arr[hi]) { if (lo == hi) { if (arr[lo] == x) return lo; return -1; } int pos = lo + (((double)(hi - lo) / (arr[hi] - arr[lo])) * (x - arr[lo])); if (arr[pos] == x) return pos; if (arr[pos] < x) lo = pos + 1; else hi = pos - 1; } return -1; }","title":"Iterative"},{"location":"algo/search/interpolation.html#reursive","text":"int interpolationSearch(int arr[], int lo, int hi, int x) { int pos; if ( lo <= hi && x >= arr[lo] && x <= arr[hi]) { pos = lo + (((double)( hi - lo ) / (arr[hi] - arr[lo])) * (x - arr[lo])); if( arr[pos] == x ) return pos; if( arr[pos] < x ) return interpolationSearch(arr, pos + 1, hi, x); if( arr[pos] > x ) return interpolationSearch(arr, lo, pos - 1, x); } return -1; }","title":"Reursive"},{"location":"algo/search/interpolation.html#interpolation-vs-binary","text":"log(log(n)) comparisons -> uniformly distributed In the worst case: (for instance where the numerical values of the keys increase exponentially) It can make up to O(n) comparisons.","title":"Interpolation vs Binary"},{"location":"algo/search/intro.html","text":"Searching \u00b6 best -> binary and hash tables Linear Search \u00b6 O(n) -> time int search(int arr[], int n, int x) { int i; for (i = 0; i < n; i++) if (arr[i] == x) return i; return -1; } Improve Linear Search Worst-Case Complexity if element Found at last O(n) to O(1) if element Not found O(n) to O(n/2) by running right and left pointers public static void search(int arr[], int search_Element) { int left = 0; int length = arr.length; int right = length - 1; int position = -1; for (left = 0; left <= right;) { if (arr[left] == search_Element) { position = left; System.out.println( \"Element found in Array at \" + (position + 1) + \" Position with \" + (left + 1) + \" Attempt\"); break; } if (arr[right] == search_Element) { position = right; System.out.println( \"Element found in Array at \" + (position + 1) + \" Position with \" + (length - right) + \" Attempt\"); break; } left++; right--; } if (position == -1) System.out.println(\"Not found in Array with \" + left + \" Attempt\"); }","title":"Searching"},{"location":"algo/search/intro.html#searching","text":"best -> binary and hash tables","title":"Searching"},{"location":"algo/search/intro.html#linear-search","text":"O(n) -> time int search(int arr[], int n, int x) { int i; for (i = 0; i < n; i++) if (arr[i] == x) return i; return -1; } Improve Linear Search Worst-Case Complexity if element Found at last O(n) to O(1) if element Not found O(n) to O(n/2) by running right and left pointers public static void search(int arr[], int search_Element) { int left = 0; int length = arr.length; int right = length - 1; int position = -1; for (left = 0; left <= right;) { if (arr[left] == search_Element) { position = left; System.out.println( \"Element found in Array at \" + (position + 1) + \" Position with \" + (left + 1) + \" Attempt\"); break; } if (arr[right] == search_Element) { position = right; System.out.println( \"Element found in Array at \" + (position + 1) + \" Position with \" + (length - right) + \" Attempt\"); break; } left++; right--; } if (position == -1) System.out.println(\"Not found in Array with \" + left + \" Attempt\"); }","title":"Linear Search"},{"location":"algo/search/jump.html","text":"Jump Search \u00b6 requirements : sorted array time : O(\u221an) space : O(1) Find arr[km] < x < arr[(k+1)m] then perform linear search Optimal Jump Size -> m = root(n) -> min<(n/m) + m-1)> O(n) > jump > O (Log n) Binary Search > Jump Search element smallest or largest -> jump better we traverse once. int jumpSearch(int arr[], int x, int n) { int step = sqrt(n); int prev = 0; while (arr[min(step, n)-1] < x) { prev = step; step += sqrt(n); if (prev >= n) return -1; } while (arr[prev] < x) { prev++; if (prev == min(step, n)) return -1; } if (arr[prev] == x) return prev; return -1; }","title":"Jump Search"},{"location":"algo/search/jump.html#jump-search","text":"requirements : sorted array time : O(\u221an) space : O(1) Find arr[km] < x < arr[(k+1)m] then perform linear search Optimal Jump Size -> m = root(n) -> min<(n/m) + m-1)> O(n) > jump > O (Log n) Binary Search > Jump Search element smallest or largest -> jump better we traverse once. int jumpSearch(int arr[], int x, int n) { int step = sqrt(n); int prev = 0; while (arr[min(step, n)-1] < x) { prev = step; step += sqrt(n); if (prev >= n) return -1; } while (arr[prev] < x) { prev++; if (prev == min(step, n)) return -1; } if (arr[prev] == x) return prev; return -1; }","title":"Jump Search"},{"location":"algo/search/ternery.html","text":"Ternery Search \u00b6 The following is recursive formula for counting comparisons in worst case of Binary Search. T(n) = T(n/2) + 2, T(1) = 1 The following is recursive formula for counting comparisons in worst case of Ternary Search. T(n) = T(n/3) + 4, T(1) = 1 In binary search, there are 2Log2n + 1 comparisons in worst case. In ternary search, there are 4Log3n + 1 comparisons in worst case. Time Complexity for Binary search = 2clog2n + O(1) Time Complexity for Ternary search = 4clog3n + O(1) Therefore, the comparison of Ternary and Binary Searches boils down the comparison of expressions 2Log3n and Log2n . The value of 2Log3n can be written as (2 / Log23) * Log2n . Since the value of (2 / Log23) is more than one, Ternary Search does more comparisons than Binary Search in worst case. Exercise: Why Merge Sort divides input array in two halves, why not in three or more parts? int ternarySearch(int arr[], int l, int r, int x) { if (r >= l) { int mid1 = l + (r - l)/3; int mid2 = mid1 + (r - l)/3; if (arr[mid1] == x) return mid1; if (arr[mid2] == x) return mid2; if (arr[mid1] > x) return ternarySearch(arr, l, mid1-1, x); if (arr[mid2] < x) return ternarySearch(arr, mid2+1, r, x); return ternarySearch(arr, mid1+1, mid2-1, x); } return -1; }","title":"Ternery Search"},{"location":"algo/search/ternery.html#ternery-search","text":"The following is recursive formula for counting comparisons in worst case of Binary Search. T(n) = T(n/2) + 2, T(1) = 1 The following is recursive formula for counting comparisons in worst case of Ternary Search. T(n) = T(n/3) + 4, T(1) = 1 In binary search, there are 2Log2n + 1 comparisons in worst case. In ternary search, there are 4Log3n + 1 comparisons in worst case. Time Complexity for Binary search = 2clog2n + O(1) Time Complexity for Ternary search = 4clog3n + O(1) Therefore, the comparison of Ternary and Binary Searches boils down the comparison of expressions 2Log3n and Log2n . The value of 2Log3n can be written as (2 / Log23) * Log2n . Since the value of (2 / Log23) is more than one, Ternary Search does more comparisons than Binary Search in worst case. Exercise: Why Merge Sort divides input array in two halves, why not in three or more parts? int ternarySearch(int arr[], int l, int r, int x) { if (r >= l) { int mid1 = l + (r - l)/3; int mid2 = mid1 + (r - l)/3; if (arr[mid1] == x) return mid1; if (arr[mid2] == x) return mid2; if (arr[mid1] > x) return ternarySearch(arr, l, mid1-1, x); if (arr[mid2] < x) return ternarySearch(arr, mid2+1, r, x); return ternarySearch(arr, mid1+1, mid2-1, x); } return -1; }","title":"Ternery Search"},{"location":"algo/sort/bubble.html","text":"Bubble sort \u00b6 optimisation : O(n^2) time even if the array is sorted. It can be optimized by stopping the algorithm if inner loop didn\u2019t cause any swap. void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i < n-1; i++) for (j = 0; j < n-i-1; j++) if (arr[j] > arr[j+1]) swap(&arr[j], &arr[j+1]); } Worst and Average Case Time Complexity: O(n*n). Worst case occurs when array is reverse sorted. Best Case Time Complexity: O(n). Best case occurs when array is already sorted. Auxiliary Space: O(1) Sorting In Place: Yes Stable: Yes Application \u00b6 In computer graphics it is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2n). For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their x coordinate at a specific scan line (a line parallel to x axis) and with incrementing y their order changes (two elements are swapped) only at intersections of two lines.","title":"Bubble sort"},{"location":"algo/sort/bubble.html#bubble-sort","text":"optimisation : O(n^2) time even if the array is sorted. It can be optimized by stopping the algorithm if inner loop didn\u2019t cause any swap. void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i < n-1; i++) for (j = 0; j < n-i-1; j++) if (arr[j] > arr[j+1]) swap(&arr[j], &arr[j+1]); } Worst and Average Case Time Complexity: O(n*n). Worst case occurs when array is reverse sorted. Best Case Time Complexity: O(n). Best case occurs when array is already sorted. Auxiliary Space: O(1) Sorting In Place: Yes Stable: Yes","title":"Bubble sort"},{"location":"algo/sort/bubble.html#application","text":"In computer graphics it is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2n). For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their x coordinate at a specific scan line (a line parallel to x axis) and with incrementing y their order changes (two elements are swapped) only at intersections of two lines.","title":"Application"},{"location":"algo/sort/counting.html","text":"Counting Sort \u00b6 Counting sort is efficient if the range of input data is not significantly greater than the number of objects to be sorted. Consider the situation where the input sequence is between range 1 to 10K and the data is 10, 5, 10K, 5K. It is not a comparison based sorting. It running time complexity is O(n) with space proportional to the range of data. It is often used as a sub-routine to another sorting algorithm like radix sort. Counting sort uses a partial hashing to count the occurrence of the data object in O(1). Counting sort can be extended to work for negative inputs also Modify above code to sort the input data in the range from M to N. Is counting sort stable and online? Thoughts on parallelizing the counting sort algorithm. void countSort(char arr[]) { char output[strlen(arr)]; int count[RANGE + 1], i; memset(count, 0, sizeof(count)); for (i = 0; arr[i]; ++i) ++count[arr[i]]; for (i = 1; i <= RANGE; ++i) count[i] += count[i - 1]; for (i = 0; arr[i]; ++i) { output[count[arr[i]] - 1] = arr[i]; --count[arr[i]]; } /* For Stable algorithm for (i = sizeof(arr)-1; i>=0; --i) { output[count[arr[i]]-1] = arr[i]; --count[arr[i]]; } */ for (i = 0; arr[i]; ++i) arr[i] = output[i]; }","title":"Counting Sort"},{"location":"algo/sort/counting.html#counting-sort","text":"Counting sort is efficient if the range of input data is not significantly greater than the number of objects to be sorted. Consider the situation where the input sequence is between range 1 to 10K and the data is 10, 5, 10K, 5K. It is not a comparison based sorting. It running time complexity is O(n) with space proportional to the range of data. It is often used as a sub-routine to another sorting algorithm like radix sort. Counting sort uses a partial hashing to count the occurrence of the data object in O(1). Counting sort can be extended to work for negative inputs also Modify above code to sort the input data in the range from M to N. Is counting sort stable and online? Thoughts on parallelizing the counting sort algorithm. void countSort(char arr[]) { char output[strlen(arr)]; int count[RANGE + 1], i; memset(count, 0, sizeof(count)); for (i = 0; arr[i]; ++i) ++count[arr[i]]; for (i = 1; i <= RANGE; ++i) count[i] += count[i - 1]; for (i = 0; arr[i]; ++i) { output[count[arr[i]] - 1] = arr[i]; --count[arr[i]]; } /* For Stable algorithm for (i = sizeof(arr)-1; i>=0; --i) { output[count[arr[i]]-1] = arr[i]; --count[arr[i]]; } */ for (i = 0; arr[i]; ++i) arr[i] = output[i]; }","title":"Counting Sort"},{"location":"algo/sort/insertion.html","text":"Insertion Sort \u00b6 void insertionSort(int arr[], int n) { int i, key, j; for (i = 1; i < n; i++) { key = arr[i]; j = i - 1; while (j >= 0 && arr[j] > key) { arr[j + 1] = arr[j]; j = j - 1; } arr[j + 1] = key; } } Time Complexity: O(n*2) Auxiliary Space: O(1) Boundary Cases: Insertion sort takes maximum time to sort if elements are sorted in reverse order. And it takes minimum time (Order of n) when elements are already sorted. Algorithmic Paradigm: Incremental Approach Sorting In Place: Yes Stable: Yes Online: Yes Uses \u00b6 number of elements is small. It can also be useful when input array is almost sorted, only few elements are misplaced in complete big array. Binary Insertion Sort \u00b6 We can use binary search to reduce the number of comparisons in normal insertion sort. Binary Insertion Sort uses binary search to find the proper location to insert the selected item at each iteration. In normal insertion, sorting takes O(i) (at ith iteration) in worst case. We can reduce it to O(logi) by using binary search. The algorithm, as a whole, still has a running worst case running time of O(n2) because of the series of swaps required for each insertion. Insertion Sort for Linked List \u00b6 Create an empty sorted (or result) list Traverse the given list, do following for every node. Insert current node in sorted way in sorted or result list. Change head of given linked list to head of sorted list.","title":"Insertion Sort"},{"location":"algo/sort/insertion.html#insertion-sort","text":"void insertionSort(int arr[], int n) { int i, key, j; for (i = 1; i < n; i++) { key = arr[i]; j = i - 1; while (j >= 0 && arr[j] > key) { arr[j + 1] = arr[j]; j = j - 1; } arr[j + 1] = key; } } Time Complexity: O(n*2) Auxiliary Space: O(1) Boundary Cases: Insertion sort takes maximum time to sort if elements are sorted in reverse order. And it takes minimum time (Order of n) when elements are already sorted. Algorithmic Paradigm: Incremental Approach Sorting In Place: Yes Stable: Yes Online: Yes","title":"Insertion Sort"},{"location":"algo/sort/insertion.html#uses","text":"number of elements is small. It can also be useful when input array is almost sorted, only few elements are misplaced in complete big array.","title":"Uses"},{"location":"algo/sort/insertion.html#binary-insertion-sort","text":"We can use binary search to reduce the number of comparisons in normal insertion sort. Binary Insertion Sort uses binary search to find the proper location to insert the selected item at each iteration. In normal insertion, sorting takes O(i) (at ith iteration) in worst case. We can reduce it to O(logi) by using binary search. The algorithm, as a whole, still has a running worst case running time of O(n2) because of the series of swaps required for each insertion.","title":"Binary Insertion Sort"},{"location":"algo/sort/insertion.html#insertion-sort-for-linked-list","text":"Create an empty sorted (or result) list Traverse the given list, do following for every node. Insert current node in sorted way in sorted or result list. Change head of given linked list to head of sorted list.","title":"Insertion Sort for Linked List"},{"location":"algo/sort/intro.html","text":"Sorting \u00b6 change order of collected elements to whats required. In Place \u00b6 Does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log n), though sometimes anything in o(n) (Smaller than linear) is allowed. Stable \u00b6 Stability is mainly important when we have key value pairs with duplicate keys possible (like people names as keys and their details as values). And we wish to sort these objects by keys. equivalent elements retain their relative positions. need stability when in (key, value) -> keys are equal but values have some significance of order. Indistinguishable, (integers), all keys are different. -> do not need. Relative order is maintained in an Unstable Sort -> highly unlikely. Does not effect performance and takes some extra space, possibly theta(n). -> to make unstable algo to stable. by changing the key comparison operation so that the comparison of two keys considers position as a factor for objects with equal keys. External Algo \u00b6 In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a computer's main memory at once. Lower bound for comparison based sorting \u00b6 Input: . Output: permutation / reordering -> when a\u20181 <= a\u20182 \u2026.. <= a\u2018n. uses comparison operators decision trees. A decision tree -> full binary tree that represents the comparisons between elements that are performed by a particular sorting algorithm operating on an input of a given size. tracing a path from the root of the decision tree to a leaf. At each internal node, a comparison ai <= aj is made. left subtree -> ai <= aj. right subtree -> ai > aj. When reach leaf, ordering is done. n! permutations on n -> leaves for the sorting algorithm to sort properly. x -> maximum number maximum height of the decison tree_ -> x. A tree with maximum height x has at most 2^x leaves. n! <= 2^x Taking Log on both sides. log2(n!) <= x As log2(n!) = \u0398(nLogn) => x = \u03a9(nLog2n) Hence Heapsort, merge sort -> asymptotically optimal comparison sorts. minimum number of memory writes? \u00b6 Some huge data set is very expensive -> EEPROMs or Flash memory -> each write reduces the lifespan of the memory. Selection Sort makes least number of writes (it makes O(n) swaps). Cycle Sort -> zero times -> correct position or written one time to its correct position. Hence Cycle Sort","title":"Sorting"},{"location":"algo/sort/intro.html#sorting","text":"change order of collected elements to whats required.","title":"Sorting"},{"location":"algo/sort/intro.html#in-place","text":"Does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log n), though sometimes anything in o(n) (Smaller than linear) is allowed.","title":"In Place"},{"location":"algo/sort/intro.html#stable","text":"Stability is mainly important when we have key value pairs with duplicate keys possible (like people names as keys and their details as values). And we wish to sort these objects by keys. equivalent elements retain their relative positions. need stability when in (key, value) -> keys are equal but values have some significance of order. Indistinguishable, (integers), all keys are different. -> do not need. Relative order is maintained in an Unstable Sort -> highly unlikely. Does not effect performance and takes some extra space, possibly theta(n). -> to make unstable algo to stable. by changing the key comparison operation so that the comparison of two keys considers position as a factor for objects with equal keys.","title":"Stable"},{"location":"algo/sort/intro.html#external-algo","text":"In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a computer's main memory at once.","title":"External Algo"},{"location":"algo/sort/intro.html#lower-bound-for-comparison-based-sorting","text":"Input: . Output: permutation / reordering -> when a\u20181 <= a\u20182 \u2026.. <= a\u2018n. uses comparison operators decision trees. A decision tree -> full binary tree that represents the comparisons between elements that are performed by a particular sorting algorithm operating on an input of a given size. tracing a path from the root of the decision tree to a leaf. At each internal node, a comparison ai <= aj is made. left subtree -> ai <= aj. right subtree -> ai > aj. When reach leaf, ordering is done. n! permutations on n -> leaves for the sorting algorithm to sort properly. x -> maximum number maximum height of the decison tree_ -> x. A tree with maximum height x has at most 2^x leaves. n! <= 2^x Taking Log on both sides. log2(n!) <= x As log2(n!) = \u0398(nLogn) => x = \u03a9(nLog2n) Hence Heapsort, merge sort -> asymptotically optimal comparison sorts.","title":"Lower bound for comparison based sorting"},{"location":"algo/sort/intro.html#minimum-number-of-memory-writes","text":"Some huge data set is very expensive -> EEPROMs or Flash memory -> each write reduces the lifespan of the memory. Selection Sort makes least number of writes (it makes O(n) swaps). Cycle Sort -> zero times -> correct position or written one time to its correct position. Hence Cycle Sort","title":"minimum number of memory writes?"},{"location":"algo/sort/merge-quick.html","text":"Merge vs Quick Sort \u00b6 Quick Sort \u00b6 internal algorithm unstable | can be made arrays preffered small db worst case -> O(n^2) divide untill you can't (DAC) (any length) partition exchange sort pivot in place left of pivot < pivot < right of pivot good cache locallity Merge Sort \u00b6 external stable link list preffered any length db worst case -> O(nlogn) DAC (n/2) 3 arrays -> 2 to sort, 1 to store | not in place","title":"Merge vs Quick Sort"},{"location":"algo/sort/merge-quick.html#merge-vs-quick-sort","text":"","title":"Merge vs Quick Sort"},{"location":"algo/sort/merge-quick.html#quick-sort","text":"internal algorithm unstable | can be made arrays preffered small db worst case -> O(n^2) divide untill you can't (DAC) (any length) partition exchange sort pivot in place left of pivot < pivot < right of pivot good cache locallity","title":"Quick Sort"},{"location":"algo/sort/merge-quick.html#merge-sort","text":"external stable link list preffered any length db worst case -> O(nlogn) DAC (n/2) 3 arrays -> 2 to sort, 1 to store | not in place","title":"Merge Sort"},{"location":"algo/sort/merge.html","text":"Merge Sort \u00b6 Dummy nodes \u00b6 The strategy here uses a temporary dummy node as the start of the result list. The pointer Tail always points to the last node in the result list, so appending new nodes is easy. The dummy node gives the tail something to point to initially when the result list is empty. This dummy node is efficient, since it is only temporary, and it is allocated in the stack. The loop proceeds, removing one node from either \u2018a\u2019 or \u2018b\u2019, and adding it to the tail. When We are done, the result is in dummy.next. void MergeSort(Node** headRef) { Node* head = *headRef; Node* a; Node* b; if ((head == NULL) || (head->next == NULL)) { return; } FrontBackSplit(head, &a, &b); MergeSort(&a); MergeSort(&b); *headRef = SortedMerge(a, b); } Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; if (a == NULL) return (b); else if (b == NULL) return (a); if (a->data <= b->data) { result = a; result->next = SortedMerge(a->next, b); } else { result = b; result->next = SortedMerge(a, b->next); } return (result); } void FrontBackSplit(Node* source, Node** frontRef, Node** backRef) { Node* fast; Node* slow; slow = source; fast = source->next; while (fast != NULL) { fast = fast->next; if (fast != NULL) { slow = slow->next; fast = fast->next; } } *frontRef = source; *backRef = slow->next; slow->next = NULL; } Local Reference \u00b6 lastPtrRef is same as dummy node (this is tail before was head) Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; Node** lastPtrRef = &result; while(1) { if (a == NULL) { *lastPtrRef = b; break; } else if (b==NULL) { *lastPtrRef = a; break; } if(a->data <= b->data) { MoveNode(lastPtrRef, &a); } else { MoveNode(lastPtrRef, &b); } lastPtrRef = &((*lastPtrRef)->next); } return(result); } Recursive (Space is more... Prodution XXX) \u00b6 Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; if (a == NULL) return(b); else if (b == NULL) return(a); if (a->data <= b->data) { result = a; result->next = SortedMerge(a->next, b); } else { result = b; result->next = SortedMerge(a, b->next); } return(result); } O(n Log n) Merge sort is often preferred for sorting a linked list. The slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible. sorting image Let head be the first node of the linked list to be sorted and headRef be the pointer to head. Note that we need a reference to head in MergeSort() as the below implementation changes next links to sort the linked lists (not data at the nodes), so head node has to be changed if the data at the original head is not the smallest value in the linked list. void MergeSort(Node** headRef) { Node* head = *headRef; Node* a; Node* b; if ((head == NULL) || (head->next == NULL)) { return; } FrontBackSplit(head, &a, &b); MergeSort(&a); MergeSort(&b); *headRef = SortedMerge(a, b); } Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; /* Base cases */ if (a == NULL) return (b); else if (b == NULL) return (a); /* Pick either a or b, and recur */ if (a->data <= b->data) { result = a; result->next = SortedMerge(a->next, b); } else { result = b; result->next = SortedMerge(a, b->next); } return (result); } void FrontBackSplit(Node* source, Node** frontRef, Node** backRef) { Node* fast; Node* slow; slow = source; fast = source->next; while (fast != NULL) { fast = fast->next; if (fast != NULL) { slow = slow->next; fast = fast->next; } } *frontRef = source; *backRef = slow->next; slow->next = NULL; } Time : T(n) = 2T(n/2) + \u03b8(n) : \u03b8(nLogn) in all 3 cases (worst, average and best) Space: O(n) Algorithmic Paradigm: Divide and Conquer Sorting In Place: No in a typical implementation Stable: Yes Applications sorting linked lists -> as Quick Sort requires a lot of direct access Inversion Count Problem Used in External Sorting void merge(int arr[], int l, int m, int r) { int n1 = m - l + 1; int n2 = r - m; int L[n1], R[n2]; for (int i = 0; i < n1; i++) L[i] = arr[l + i]; for (int j = 0; j < n2; j++) R[j] = arr[m + 1 + j]; int i = 0; int j = 0; int k = l; while (i < n1 && j < n2) { if (L[i] <= R[j]) { arr[k] = L[i]; i++; } else { arr[k] = R[j]; j++; } k++; } while (i < n1) { arr[k] = L[i]; i++; k++; } while (j < n2) { arr[k] = R[j]; j++; k++; } } void mergeSort(int arr[],int l,int r){ if(l>=r){ return; } int m = (l+r-1)/2; mergeSort(arr,l,m); mergeSort(arr,m+1,r); merge(arr,l,m,r); } O(nlogn) T(n) = 2T(n/2) + \\Theta(n) , O(n) stable, not inplace generally External sort void merge(int arr[], int l, int mid, int r){ int i=0, j=0, k=0; int n1 = mid-l + 1; int n2 = r-mid; int L[n1]; int R[n2]; while(i<n1){ L[i] = arr[l+i]; i++; } while(j<n2){ R[j] = arr[mid+1+j]; j++; } i =0, j=0, k=l; while(i<n1 && j<n2){ if(L[i]<=R[j]){ arr[k] = L[i]; i++; } else{ arr[k] = R[j]; j++; } k++; } while(i<n1){ arr[k] = L[i]; i++; k++; } while(j<n2){ arr[k] = R[j]; j++; k++; } } void mergesort(int arr[], int l, int r){ if(l<r){ int mid = (l+r)/2; mergesort(arr, l, mid); mergesort(arr, mid+1, r); merge(arr, l, mid, r); } } https://www.geeksforgeeks.org/merge-sort-for-linked-list/ O(nlogn), O(1) merge sort is best for linked list. Node* merge(Node* a, Node* b){ Node* result = NULL; if(a==NULL){ return b; } else if(b==NULL) return a; if(a->data <= b->data){ result = a; result->next = merge(a->next, b); } else{ result=b; result->next = merge(a, b->next); } return result; } void Split(Node* source, Node** a, Node** b){ Node* fast; Node* slow; slow = source; fast = source->next; while(fast!=NULL){ fast = fast->next; while(fast!=NULL){ slow = slow->next; fast = fast->next; } } *a = source; *b = (slow->next); slow->next = NULL; } void mergeSort(Node** headref){ Node* head = *headref; Node* a; Node* b; if((head==NULL)|| (head->next ==NULL)){ return; } Split(head, &a, &b); mergeSort(&a); mergeSort(&b); *headref = merge(a,b); }","title":"Merge Sort"},{"location":"algo/sort/merge.html#merge-sort","text":"","title":"Merge Sort"},{"location":"algo/sort/merge.html#dummy-nodes","text":"The strategy here uses a temporary dummy node as the start of the result list. The pointer Tail always points to the last node in the result list, so appending new nodes is easy. The dummy node gives the tail something to point to initially when the result list is empty. This dummy node is efficient, since it is only temporary, and it is allocated in the stack. The loop proceeds, removing one node from either \u2018a\u2019 or \u2018b\u2019, and adding it to the tail. When We are done, the result is in dummy.next. void MergeSort(Node** headRef) { Node* head = *headRef; Node* a; Node* b; if ((head == NULL) || (head->next == NULL)) { return; } FrontBackSplit(head, &a, &b); MergeSort(&a); MergeSort(&b); *headRef = SortedMerge(a, b); } Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; if (a == NULL) return (b); else if (b == NULL) return (a); if (a->data <= b->data) { result = a; result->next = SortedMerge(a->next, b); } else { result = b; result->next = SortedMerge(a, b->next); } return (result); } void FrontBackSplit(Node* source, Node** frontRef, Node** backRef) { Node* fast; Node* slow; slow = source; fast = source->next; while (fast != NULL) { fast = fast->next; if (fast != NULL) { slow = slow->next; fast = fast->next; } } *frontRef = source; *backRef = slow->next; slow->next = NULL; }","title":"Dummy nodes"},{"location":"algo/sort/merge.html#local-reference","text":"lastPtrRef is same as dummy node (this is tail before was head) Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; Node** lastPtrRef = &result; while(1) { if (a == NULL) { *lastPtrRef = b; break; } else if (b==NULL) { *lastPtrRef = a; break; } if(a->data <= b->data) { MoveNode(lastPtrRef, &a); } else { MoveNode(lastPtrRef, &b); } lastPtrRef = &((*lastPtrRef)->next); } return(result); }","title":"Local Reference"},{"location":"algo/sort/merge.html#recursive-space-is-more-prodution-xxx","text":"Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; if (a == NULL) return(b); else if (b == NULL) return(a); if (a->data <= b->data) { result = a; result->next = SortedMerge(a->next, b); } else { result = b; result->next = SortedMerge(a, b->next); } return(result); } O(n Log n) Merge sort is often preferred for sorting a linked list. The slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible. sorting image Let head be the first node of the linked list to be sorted and headRef be the pointer to head. Note that we need a reference to head in MergeSort() as the below implementation changes next links to sort the linked lists (not data at the nodes), so head node has to be changed if the data at the original head is not the smallest value in the linked list. void MergeSort(Node** headRef) { Node* head = *headRef; Node* a; Node* b; if ((head == NULL) || (head->next == NULL)) { return; } FrontBackSplit(head, &a, &b); MergeSort(&a); MergeSort(&b); *headRef = SortedMerge(a, b); } Node* SortedMerge(Node* a, Node* b) { Node* result = NULL; /* Base cases */ if (a == NULL) return (b); else if (b == NULL) return (a); /* Pick either a or b, and recur */ if (a->data <= b->data) { result = a; result->next = SortedMerge(a->next, b); } else { result = b; result->next = SortedMerge(a, b->next); } return (result); } void FrontBackSplit(Node* source, Node** frontRef, Node** backRef) { Node* fast; Node* slow; slow = source; fast = source->next; while (fast != NULL) { fast = fast->next; if (fast != NULL) { slow = slow->next; fast = fast->next; } } *frontRef = source; *backRef = slow->next; slow->next = NULL; } Time : T(n) = 2T(n/2) + \u03b8(n) : \u03b8(nLogn) in all 3 cases (worst, average and best) Space: O(n) Algorithmic Paradigm: Divide and Conquer Sorting In Place: No in a typical implementation Stable: Yes Applications sorting linked lists -> as Quick Sort requires a lot of direct access Inversion Count Problem Used in External Sorting void merge(int arr[], int l, int m, int r) { int n1 = m - l + 1; int n2 = r - m; int L[n1], R[n2]; for (int i = 0; i < n1; i++) L[i] = arr[l + i]; for (int j = 0; j < n2; j++) R[j] = arr[m + 1 + j]; int i = 0; int j = 0; int k = l; while (i < n1 && j < n2) { if (L[i] <= R[j]) { arr[k] = L[i]; i++; } else { arr[k] = R[j]; j++; } k++; } while (i < n1) { arr[k] = L[i]; i++; k++; } while (j < n2) { arr[k] = R[j]; j++; k++; } } void mergeSort(int arr[],int l,int r){ if(l>=r){ return; } int m = (l+r-1)/2; mergeSort(arr,l,m); mergeSort(arr,m+1,r); merge(arr,l,m,r); } O(nlogn) T(n) = 2T(n/2) + \\Theta(n) , O(n) stable, not inplace generally External sort void merge(int arr[], int l, int mid, int r){ int i=0, j=0, k=0; int n1 = mid-l + 1; int n2 = r-mid; int L[n1]; int R[n2]; while(i<n1){ L[i] = arr[l+i]; i++; } while(j<n2){ R[j] = arr[mid+1+j]; j++; } i =0, j=0, k=l; while(i<n1 && j<n2){ if(L[i]<=R[j]){ arr[k] = L[i]; i++; } else{ arr[k] = R[j]; j++; } k++; } while(i<n1){ arr[k] = L[i]; i++; k++; } while(j<n2){ arr[k] = R[j]; j++; k++; } } void mergesort(int arr[], int l, int r){ if(l<r){ int mid = (l+r)/2; mergesort(arr, l, mid); mergesort(arr, mid+1, r); merge(arr, l, mid, r); } } https://www.geeksforgeeks.org/merge-sort-for-linked-list/ O(nlogn), O(1) merge sort is best for linked list. Node* merge(Node* a, Node* b){ Node* result = NULL; if(a==NULL){ return b; } else if(b==NULL) return a; if(a->data <= b->data){ result = a; result->next = merge(a->next, b); } else{ result=b; result->next = merge(a, b->next); } return result; } void Split(Node* source, Node** a, Node** b){ Node* fast; Node* slow; slow = source; fast = source->next; while(fast!=NULL){ fast = fast->next; while(fast!=NULL){ slow = slow->next; fast = fast->next; } } *a = source; *b = (slow->next); slow->next = NULL; } void mergeSort(Node** headref){ Node* head = *headref; Node* a; Node* b; if((head==NULL)|| (head->next ==NULL)){ return; } Split(head, &a, &b); mergeSort(&a); mergeSort(&b); *headref = merge(a,b); }","title":"Recursive (Space is more... Prodution XXX)"},{"location":"algo/sort/quick.html","text":"Quick Sort \u00b6 T(n) = T(k) + T(n-k-1) + theta(n) Worst Case: greatest or smallest element as pivot-> already sorted in increasing or decreasing order: T(n) = T(n-1) + theta(n) == theta(n^2). Best Case:always picks the middle element as pivot T(n) = 2T(n/2) + theta(n) == theta(nLogn) Average Case: \u00b6 all possible permutation of array and calculate time taken by every permutation which doesn\u2019t look easy. by considering the case when partition puts O(n/9) elements in one set and O(9n/10) elements in other set. T(n) = T(n/9) + T(9n/10) + theta(n) == O(nLogn) Disadvantages \u00b6 worst case -> worse than others merge sort -> when data is huge and stored in external storage. Advantages \u00b6 faster in practice -> inner loop can be efficiently implemented on most architectures and in most real-world data. cache friendly -> good locality of reference when used for arrays. tail recursive, therefore tail call optimizations is done. requires no extra space -> merge sort requires..... averrage O(nlogn) similar Stable \u00b6 nope In-place? \u00b6 space -> recursive function calls but not for manipulating the input. hence in-place 3-Way QuickSort? \u00b6 For redundant data {1, 4, 2, 4, 2, 4, 1, 2, 4, 1, 2, 2, 2, 2, 4, 1, 4, 4, 4} arr[l..i] < pivot. arr[i+1..j-1] == pivot. arr[j..r] > pivot. see Singly Linked List, Doubly Linked List, Iterative Implementation Why MergeSort for Linked Lists? \u00b6 merge sort -> without extra space for linked lists. overhead increases for quick sort in linked list -> as A[i] is required and direct access is costly. When does worst case occur \u00b6 depends on strategy of choosing pivot. when rightmost or leftmost -> array is reverse / sorted -> all same (special case) whenever max , min is chosen Solution? \u00b6 random middle median of first, middle and last int partition (int arr[], int low, int high) { int pivot = arr[high]; int i = (low - 1); for (int j = low; j <= high - 1; j++) { if (arr[j] < pivot) { i++; swap(&arr[i], &arr[j]); } } swap(&arr[i + 1], &arr[high]); return (i + 1); } void quickSort(int arr[], int low, int high) { if (low < high) { int pi = partition(arr, low, high); quickSort(arr, low, pi - 1); quickSort(arr, pi + 1, high); } } T(n) = T(k) + T(n-k-1) + \\theta(n), T(n) = T(n-1) + \\theta(n) --- T(n) = 2T(n/2) + \\theta(n) O(n^2) - O(nlogn), O(1) Although the worst case time complexity of QuickSort is O(n2) which is more than many other sorting algorithms like Merge Sort and Heap Sort, QuickSort is faster in practice, because its inner loop can be efficiently implemented on most architectures, and in most real-world data. QuickSort can be implemented in different ways by changing the choice of pivot, so that the worst case rarely occurs for a given type of data. However, merge sort is generally considered better when data is huge and stored in external storage. inplace, not stable. Related: 3 way quicksort: https:www.geeksforgeeks.org/3-way-quicksort/, iterative, linked list , doubly linked list Quick sort vs merge sort? merge sortrequires O(n) space complexity, casche friendly Randomised Quicksort - average O(nlogn) - worst doesn't occur in patterns like sorted or not. tail recursiv - tail optimisation is done. linked list - merge sort as random access for qsort is not feasable in linked list. int partition (int arr[], int low, int high) { int pivot = arr[high]; int i = (low - 1); for (int j = low; j <= high - 1; j++) { if (arr[j] < pivot) { i++; swap(&arr[i], &arr[j]); } } swap(&arr[i + 1], &arr[high]); return (i + 1); } void quickSort(int arr[], int low, int high) { if (low < high) { int pi = partition(arr, low, high); quickSort(arr, low, pi - 1); quickSort(arr, pi + 1, high); } }","title":"Quick Sort"},{"location":"algo/sort/quick.html#quick-sort","text":"T(n) = T(k) + T(n-k-1) + theta(n) Worst Case: greatest or smallest element as pivot-> already sorted in increasing or decreasing order: T(n) = T(n-1) + theta(n) == theta(n^2). Best Case:always picks the middle element as pivot T(n) = 2T(n/2) + theta(n) == theta(nLogn)","title":"Quick Sort"},{"location":"algo/sort/quick.html#average-case","text":"all possible permutation of array and calculate time taken by every permutation which doesn\u2019t look easy. by considering the case when partition puts O(n/9) elements in one set and O(9n/10) elements in other set. T(n) = T(n/9) + T(9n/10) + theta(n) == O(nLogn)","title":"Average Case:"},{"location":"algo/sort/quick.html#disadvantages","text":"worst case -> worse than others merge sort -> when data is huge and stored in external storage.","title":"Disadvantages"},{"location":"algo/sort/quick.html#advantages","text":"faster in practice -> inner loop can be efficiently implemented on most architectures and in most real-world data. cache friendly -> good locality of reference when used for arrays. tail recursive, therefore tail call optimizations is done. requires no extra space -> merge sort requires..... averrage O(nlogn) similar","title":"Advantages"},{"location":"algo/sort/quick.html#stable","text":"nope","title":"Stable"},{"location":"algo/sort/quick.html#in-place","text":"space -> recursive function calls but not for manipulating the input. hence in-place","title":"In-place?"},{"location":"algo/sort/quick.html#3-way-quicksort","text":"For redundant data {1, 4, 2, 4, 2, 4, 1, 2, 4, 1, 2, 2, 2, 2, 4, 1, 4, 4, 4} arr[l..i] < pivot. arr[i+1..j-1] == pivot. arr[j..r] > pivot. see Singly Linked List, Doubly Linked List, Iterative Implementation","title":"3-Way QuickSort?"},{"location":"algo/sort/quick.html#why-mergesort-for-linked-lists","text":"merge sort -> without extra space for linked lists. overhead increases for quick sort in linked list -> as A[i] is required and direct access is costly.","title":"Why MergeSort for Linked Lists?"},{"location":"algo/sort/quick.html#when-does-worst-case-occur","text":"depends on strategy of choosing pivot. when rightmost or leftmost -> array is reverse / sorted -> all same (special case) whenever max , min is chosen","title":"When does worst case occur"},{"location":"algo/sort/quick.html#solution","text":"random middle median of first, middle and last int partition (int arr[], int low, int high) { int pivot = arr[high]; int i = (low - 1); for (int j = low; j <= high - 1; j++) { if (arr[j] < pivot) { i++; swap(&arr[i], &arr[j]); } } swap(&arr[i + 1], &arr[high]); return (i + 1); } void quickSort(int arr[], int low, int high) { if (low < high) { int pi = partition(arr, low, high); quickSort(arr, low, pi - 1); quickSort(arr, pi + 1, high); } } T(n) = T(k) + T(n-k-1) + \\theta(n), T(n) = T(n-1) + \\theta(n) --- T(n) = 2T(n/2) + \\theta(n) O(n^2) - O(nlogn), O(1) Although the worst case time complexity of QuickSort is O(n2) which is more than many other sorting algorithms like Merge Sort and Heap Sort, QuickSort is faster in practice, because its inner loop can be efficiently implemented on most architectures, and in most real-world data. QuickSort can be implemented in different ways by changing the choice of pivot, so that the worst case rarely occurs for a given type of data. However, merge sort is generally considered better when data is huge and stored in external storage. inplace, not stable. Related: 3 way quicksort: https:www.geeksforgeeks.org/3-way-quicksort/, iterative, linked list , doubly linked list Quick sort vs merge sort? merge sortrequires O(n) space complexity, casche friendly Randomised Quicksort - average O(nlogn) - worst doesn't occur in patterns like sorted or not. tail recursiv - tail optimisation is done. linked list - merge sort as random access for qsort is not feasable in linked list. int partition (int arr[], int low, int high) { int pivot = arr[high]; int i = (low - 1); for (int j = low; j <= high - 1; j++) { if (arr[j] < pivot) { i++; swap(&arr[i], &arr[j]); } } swap(&arr[i + 1], &arr[high]); return (i + 1); } void quickSort(int arr[], int low, int high) { if (low < high) { int pi = partition(arr, low, high); quickSort(arr, low, pi - 1); quickSort(arr, pi + 1, high); } }","title":"Solution?"},{"location":"algo/sort/selection.html","text":"Selection Sort \u00b6 Time : O(n2) Space : O(1) it never makes more than O(n) swap useful when memory write is a costly operation. not stable, in place void selectionSort(int arr[], int n) { int i, j, min_idx; for (i = 0; i < n-1; i++) { min_idx = i; for (j = i+1; j < n; j++) if (arr[j] < arr[min_idx]) min_idx = j; swap(&arr[min_idx], &arr[i]); } } Input : 4A 5 3 2 4B 1 Output : 1 2 3 4B 4A 5 Swapping might impact in pushing a key(let\u2019s say A) to a position greater than the key(let\u2019s say B) which are equal keys. which makes them out of desired order. Stable \u00b6 Selection sort can be made Stable if instead of swapping, the minimum element is placed in its position without swapping i.e. by placing the number in its position by pushing every element one step forward. void stableSelectionSort(int a[], int n) { for (int i = 0; i < n - 1; i++) { int min = i; for (int j = i + 1; j < n; j++) if (a[min] > a[j]) min = j; int key = a[min]; while (min > i) { a[min] = a[min - 1]; min--; } a[i] = key; } }","title":"Selection Sort"},{"location":"algo/sort/selection.html#selection-sort","text":"Time : O(n2) Space : O(1) it never makes more than O(n) swap useful when memory write is a costly operation. not stable, in place void selectionSort(int arr[], int n) { int i, j, min_idx; for (i = 0; i < n-1; i++) { min_idx = i; for (j = i+1; j < n; j++) if (arr[j] < arr[min_idx]) min_idx = j; swap(&arr[min_idx], &arr[i]); } } Input : 4A 5 3 2 4B 1 Output : 1 2 3 4B 4A 5 Swapping might impact in pushing a key(let\u2019s say A) to a position greater than the key(let\u2019s say B) which are equal keys. which makes them out of desired order.","title":"Selection Sort"},{"location":"algo/sort/selection.html#stable","text":"Selection sort can be made Stable if instead of swapping, the minimum element is placed in its position without swapping i.e. by placing the number in its position by pushing every element one step forward. void stableSelectionSort(int a[], int n) { for (int i = 0; i < n - 1; i++) { int min = i; for (int j = i + 1; j < n; j++) if (a[min] > a[j]) min = j; int key = a[min]; while (min > i) { a[min] = a[min - 1]; min--; } a[i] = key; } }","title":"Stable"},{"location":"algo/sort/topological.html","text":"Topological Sort \u00b6 O(V+E) , O(V) Directed Acyclic Graph (DAG) for every directed edge uv, vertex u comes before v in the ordering. first element -> in-degree as 0 (a vertex with no incoming edges). can be more than one sorting different from DFS Application \u00b6 Instruction scheduling ordering of formula cell evaluation when recomputing formula values in spreadsheets logic synthesis, determining the order of compilation tasks to perform in makefiles data serialization resolving symbol dependencies in linkers sortUtil(int v, bool visited[], stack<int> &Stack){ visited[v] = true; list<int>::iterator i; for(i = adj[v].begin(); i!=adj[v].end(); ++i){ if(!visited[v]) sortUtil(v, visited, Stack); } Stack.push(v); } topologicalSort(int v){ stack<int> Stack; bool *visited = new bool[V]; for(int i = 0; i<V; i++) visited[i] = false; for(int i=0;i<V;i++) if(visited[i]==false) sortUtil(i, visited, Stack); while(Stack.empty() == false){ cout << Stack.pop() << \" \"; Stack.pop(); } }","title":"Topological Sort"},{"location":"algo/sort/topological.html#topological-sort","text":"O(V+E) , O(V) Directed Acyclic Graph (DAG) for every directed edge uv, vertex u comes before v in the ordering. first element -> in-degree as 0 (a vertex with no incoming edges). can be more than one sorting different from DFS","title":"Topological Sort"},{"location":"algo/sort/topological.html#application","text":"Instruction scheduling ordering of formula cell evaluation when recomputing formula values in spreadsheets logic synthesis, determining the order of compilation tasks to perform in makefiles data serialization resolving symbol dependencies in linkers sortUtil(int v, bool visited[], stack<int> &Stack){ visited[v] = true; list<int>::iterator i; for(i = adj[v].begin(); i!=adj[v].end(); ++i){ if(!visited[v]) sortUtil(v, visited, Stack); } Stack.push(v); } topologicalSort(int v){ stack<int> Stack; bool *visited = new bool[V]; for(int i = 0; i<V; i++) visited[i] = false; for(int i=0;i<V;i++) if(visited[i]==false) sortUtil(i, visited, Stack); while(Stack.empty() == false){ cout << Stack.pop() << \" \"; Stack.pop(); } }","title":"Application"},{"location":"computer-organisation/intro.html","text":"Computer Architecture and Organisation \u00b6 Digital Electronics \u00b6 MicroProcessor \u00b6 MIPS \u00b6 ARM Assembly \u00b6","title":"Computer Architecture and Organisation"},{"location":"computer-organisation/intro.html#computer-architecture-and-organisation","text":"","title":"Computer Architecture and Organisation"},{"location":"computer-organisation/intro.html#digital-electronics","text":"","title":"Digital Electronics"},{"location":"computer-organisation/intro.html#microprocessor","text":"","title":"MicroProcessor"},{"location":"computer-organisation/intro.html#mips","text":"","title":"MIPS"},{"location":"computer-organisation/intro.html#arm-assembly","text":"","title":"ARM Assembly"},{"location":"css/intro.html","text":"","title":"Intro"},{"location":"dbms/index.html","text":"","title":"Index"},{"location":"dbms/cap.html","text":"CAP theorum \u00b6 Consistency \u00b6 Availability \u00b6 Partition tolerence \u00b6 a distributed database system has to make a tradeoff between Consistency and Availability when a Partition occurs.","title":"CAP theorum"},{"location":"dbms/cap.html#cap-theorum","text":"","title":"CAP theorum"},{"location":"dbms/cap.html#consistency","text":"","title":"Consistency"},{"location":"dbms/cap.html#availability","text":"","title":"Availability"},{"location":"dbms/cap.html#partition-tolerence","text":"a distributed database system has to make a tradeoff between Consistency and Availability when a Partition occurs.","title":"Partition tolerence"},{"location":"dbms/cursors.html","text":"Cursors \u00b6 temp mem or temp Work Station. allocated by server at DML by user. store db. implicit - server explicit - user DECLARE s1 CURSOR FOR SELECT * FROM studDetails OPEN s1 FETCH FIRST FROM s1 FETCH LAST FROM s1 FETCH NEXT FROM s1 FETCH PRIOR FROM s1 FETCH ABSOLUTE 7 FROM s1 FETCH RELATIVE -2 FROM s1 CLOSE s1 DEALLOCATE s1","title":"Cursors"},{"location":"dbms/cursors.html#cursors","text":"temp mem or temp Work Station. allocated by server at DML by user. store db. implicit - server explicit - user DECLARE s1 CURSOR FOR SELECT * FROM studDetails OPEN s1 FETCH FIRST FROM s1 FETCH LAST FROM s1 FETCH NEXT FROM s1 FETCH PRIOR FROM s1 FETCH ABSOLUTE 7 FROM s1 FETCH RELATIVE -2 FROM s1 CLOSE s1 DEALLOCATE s1","title":"Cursors"},{"location":"dbms/deadlock.html","text":"","title":"Deadlock"},{"location":"dbms/implementation.html","text":"Implementation \u00b6","title":"Implementation"},{"location":"dbms/implementation.html#implementation","text":"","title":"Implementation"},{"location":"dbms/in-memory.html","text":"In Memory \u00b6 redis","title":"In Memory"},{"location":"dbms/in-memory.html#in-memory","text":"redis","title":"In Memory"},{"location":"dbms/intro.html","text":"Database \u00b6 Evolution \u00b6 file-based - COBOL, BASIC hierarchal modeling - Information Management System by IBM network modeling Network schema (Database organization) Sub-schema (per user) Data management language (procedural) relational - instance and schema, set theory and predicate logic cloud - DBaaS, snowflake computing, lower costs, automated, accessablity NoSQL - large unstructured distribted data, scale, avalability, no-standard, difficulty in management, no backups, no GUI document - mongo key-value - redis tabular - HBase object-oriented graph - neo4j ACID-CQ \u00b6 Atomicity Consistency Integrity Durability Concurrency Query processing DBMS \u00b6 RDBMS \u00b6 Tiers \u00b6 2 - client/server 3 - user <-> business logic <-> data Schema \u00b6 instance sub-schema data abstraction physical / internal logical / conceptual views","title":"Database"},{"location":"dbms/intro.html#database","text":"","title":"Database"},{"location":"dbms/intro.html#evolution","text":"file-based - COBOL, BASIC hierarchal modeling - Information Management System by IBM network modeling Network schema (Database organization) Sub-schema (per user) Data management language (procedural) relational - instance and schema, set theory and predicate logic cloud - DBaaS, snowflake computing, lower costs, automated, accessablity NoSQL - large unstructured distribted data, scale, avalability, no-standard, difficulty in management, no backups, no GUI document - mongo key-value - redis tabular - HBase object-oriented graph - neo4j","title":"Evolution"},{"location":"dbms/intro.html#acid-cq","text":"Atomicity Consistency Integrity Durability Concurrency Query processing","title":"ACID-CQ"},{"location":"dbms/intro.html#dbms","text":"","title":"DBMS"},{"location":"dbms/intro.html#rdbms","text":"","title":"RDBMS"},{"location":"dbms/intro.html#tiers","text":"2 - client/server 3 - user <-> business logic <-> data","title":"Tiers"},{"location":"dbms/intro.html#schema","text":"instance sub-schema data abstraction physical / internal logical / conceptual views","title":"Schema"},{"location":"dbms/lang.html","text":"Language \u00b6 DDL \u00b6 data defination CREATE DROP ALTER COMMENT RENAME DQL \u00b6 data query SELECT DML \u00b6 data manipulation INSERT UPDATE DELETE LOCK CALL EXPLAIN PLAN DCL \u00b6 data control (user-permissions) GRANT REVOKE TCL \u00b6 transaction control COMMIT ROLLBACK SAVEPOINT SET TRANSACTION SQL \u00b6 interviewbit Misc \u00b6 truncate deletes entire table","title":"Language"},{"location":"dbms/lang.html#language","text":"","title":"Language"},{"location":"dbms/lang.html#ddl","text":"data defination CREATE DROP ALTER COMMENT RENAME","title":"DDL"},{"location":"dbms/lang.html#dql","text":"data query SELECT","title":"DQL"},{"location":"dbms/lang.html#dml","text":"data manipulation INSERT UPDATE DELETE LOCK CALL EXPLAIN PLAN","title":"DML"},{"location":"dbms/lang.html#dcl","text":"data control (user-permissions) GRANT REVOKE","title":"DCL"},{"location":"dbms/lang.html#tcl","text":"transaction control COMMIT ROLLBACK SAVEPOINT SET TRANSACTION","title":"TCL"},{"location":"dbms/lang.html#sql","text":"interviewbit","title":"SQL"},{"location":"dbms/lang.html#misc","text":"truncate deletes entire table","title":"Misc"},{"location":"dbms/normalization.html","text":"Normalization \u00b6","title":"Normalization"},{"location":"dbms/normalization.html#normalization","text":"","title":"Normalization"},{"location":"dbms/rdbms.html","text":"RDBMS \u00b6 B+ trees degree of relation \u00b6 1-1 1-many many-many Constraints \u00b6 cardinality ratio / degree participation constraints Keys \u00b6 candidate - unique, nC(floor(n/2)) super - cadidate + X primary - chosen cadidate alternate foreign ER model \u00b6 Relational algebra \u00b6 Operation(Symbols) Purpose Select(\u03c3) subset of tuples given condition Projection(\u03c0) The projection eliminates all attributes of the input relation but those mentioned in the projection list. Union Operation(\u222a) all in A or/and in B Set Difference(-) A not B Intersection(\u2229) both in A and B Cartesian Product(X) merge columns of 2 tables Inner Join Inner join, includes only those tuples that satisfy the matching criteria. Theta Join(\u03b8) The general case of JOIN operation is called a Theta join. It is denoted by symbol \u03b8. EQUI Join When a theta join uses only equivalence condition, it becomes a equi join. Natural Join(\u22c8) Natural join can only be performed if there is a common attribute (column) between the relations. Outer Join In an outer join, along with tuples that satisfy the matching criteria. Left Outer Join In the left outer join, operation allows keeping all tuple in the left relation. Right Outer join In the right outer join, operation allows keeping all tuple in the right relation. Full Outer Join In a full outer join, all tuples from both relations are included in the result irrespective of the matching condition.","title":"RDBMS"},{"location":"dbms/rdbms.html#rdbms","text":"B+ trees","title":"RDBMS"},{"location":"dbms/rdbms.html#degree-of-relation","text":"1-1 1-many many-many","title":"degree of relation"},{"location":"dbms/rdbms.html#constraints","text":"cardinality ratio / degree participation constraints","title":"Constraints"},{"location":"dbms/rdbms.html#keys","text":"candidate - unique, nC(floor(n/2)) super - cadidate + X primary - chosen cadidate alternate foreign","title":"Keys"},{"location":"dbms/rdbms.html#er-model","text":"","title":"ER model"},{"location":"dbms/rdbms.html#relational-algebra","text":"Operation(Symbols) Purpose Select(\u03c3) subset of tuples given condition Projection(\u03c0) The projection eliminates all attributes of the input relation but those mentioned in the projection list. Union Operation(\u222a) all in A or/and in B Set Difference(-) A not B Intersection(\u2229) both in A and B Cartesian Product(X) merge columns of 2 tables Inner Join Inner join, includes only those tuples that satisfy the matching criteria. Theta Join(\u03b8) The general case of JOIN operation is called a Theta join. It is denoted by symbol \u03b8. EQUI Join When a theta join uses only equivalence condition, it becomes a equi join. Natural Join(\u22c8) Natural join can only be performed if there is a common attribute (column) between the relations. Outer Join In an outer join, along with tuples that satisfy the matching criteria. Left Outer Join In the left outer join, operation allows keeping all tuple in the left relation. Right Outer join In the right outer join, operation allows keeping all tuple in the right relation. Full Outer Join In a full outer join, all tuples from both relations are included in the result irrespective of the matching condition.","title":"Relational algebra"},{"location":"dbms/scale.html","text":"Scaling Query optimization \u00b6 and connection pool implementation Vertical / up \u00b6 CQRS \u00b6 command query responsibility segregation Partition \u00b6 data center wise horizontal \u00b6 Sharding \u00b6 SQL \u00b6 cluster proxy shared proxy NoSQL \u00b6 shards/nodes are equal (no master-slave) gossip/epidemic protocol Master \u00b6 Master-Master: This is similar to Master-Slave architecture, the only difference is that both the nodes are masters and replica at the same time i.e. there will be circular replication between the nodes. It is most recommended to configure both the servers to log the transactions from the replication thread (log-slave-updates) but it ignores its own already replicated transactions (set replicate-same-server-id to 0) to prevent infinite loops in the replication. Master-Slave: In this, as each data has only one master, so consistency is not difficult. Advantages of Master-Master Database: Masters can be distributed across the network that means in several physical sites master can be located . If the one master fails , other masters will start updating the database . Disadvantages of Master-Master Database: This introduces some communication latency, and eager replication systems are complex in this architecture. Multi-Master replication systems will be mostly loosely consistent, i.e. asynchronous, lazy and violating ACID properties. As the number of nodes involved rises and the required latency decreases, so Issues such as conflict resolution can become intractable.","title":"Scale"},{"location":"dbms/scale.html#query-optimization","text":"and connection pool implementation","title":"Query optimization"},{"location":"dbms/scale.html#vertical-up","text":"","title":"Vertical / up"},{"location":"dbms/scale.html#cqrs","text":"command query responsibility segregation","title":"CQRS"},{"location":"dbms/scale.html#partition","text":"data center wise","title":"Partition"},{"location":"dbms/scale.html#horizontal","text":"","title":"horizontal"},{"location":"dbms/scale.html#sharding","text":"","title":"Sharding"},{"location":"dbms/scale.html#sql","text":"cluster proxy shared proxy","title":"SQL"},{"location":"dbms/scale.html#nosql","text":"shards/nodes are equal (no master-slave) gossip/epidemic protocol","title":"NoSQL"},{"location":"dbms/scale.html#master","text":"Master-Master: This is similar to Master-Slave architecture, the only difference is that both the nodes are masters and replica at the same time i.e. there will be circular replication between the nodes. It is most recommended to configure both the servers to log the transactions from the replication thread (log-slave-updates) but it ignores its own already replicated transactions (set replicate-same-server-id to 0) to prevent infinite loops in the replication. Master-Slave: In this, as each data has only one master, so consistency is not difficult. Advantages of Master-Master Database: Masters can be distributed across the network that means in several physical sites master can be located . If the one master fails , other masters will start updating the database . Disadvantages of Master-Master Database: This introduces some communication latency, and eager replication systems are complex in this architecture. Multi-Master replication systems will be mostly loosely consistent, i.e. asynchronous, lazy and violating ACID properties. As the number of nodes involved rises and the required latency decreases, so Issues such as conflict resolution can become intractable.","title":"Master"},{"location":"dbms/storage.html","text":"Storage \u00b6 Types \u00b6 heap file org sequential hash clustered operations \u00b6 update retreival open locate read write close RAID \u00b6 redundant array of independent disks, treat multiple disks as 1. raid 0 1 - mirroring 2 - error correction code using hamming distance 3 - parity bit - byte level striping 4 - parity bit - block level striping 5 - parity bits do not have seperate disks 6 - 2 indipendent parity bits, on same disks","title":"Storage"},{"location":"dbms/storage.html#storage","text":"","title":"Storage"},{"location":"dbms/storage.html#types","text":"heap file org sequential hash clustered","title":"Types"},{"location":"dbms/storage.html#operations","text":"update retreival open locate read write close","title":"operations"},{"location":"dbms/storage.html#raid","text":"redundant array of independent disks, treat multiple disks as 1. raid 0 1 - mirroring 2 - error correction code using hamming distance 3 - parity bit - byte level striping 4 - parity bit - block level striping 5 - parity bits do not have seperate disks 6 - 2 indipendent parity bits, on same disks","title":"RAID"},{"location":"dbms/txn.html","text":"Transaction \u00b6 can't break it. ACID \u00b6 atomicity consistency - after/before durable - latest updates even if system fails isolation - txn should be isolated schedule / serializability \u00b6 Equavalence \u00b6 result - same I/O view Conflict Equivalence View equivalent schedules are view serializable and conflict equivalent schedules are conflict serializable. All conflict serializable schedules are view serializable too. States \u00b6 active partially commited failed commited aborted Concurrent txn \u00b6 lost update dirty read unrepeatable read incorrect summary reduced wait time high throughput high resource utilization Schedule \u00b6 serial complete recoverable cascadeless strict concurrency control \u00b6 shared lock exclusive lock 2 phase locking protocol BASE \u00b6 Basically Available : This constraint states that the system does guarantee the availability of the data as regards CAP Theorem ; there will be a response to any request . But , that response could still be \u2018 failure \u2019 to obtain the requested data or the data may be in an inconsistent or changing state , much like waiting for a check to clear in your bank account . Soft state : The state of the system could change over time , so even during times without input there may be changes going on due to \u2018 eventual consistency , \u2019 thus the state of the system is always \u2018 soft . \u2019 Eventual consistency : The system will eventually become consistent once it stops receiving input . The data will propagate to everywhere it should sooner or later , but the system will continue to receive input and is not checking the consistency of every transaction before it moves onto the next one . Werner Vogel \u2019 s article \u201c Eventually Consistent \u2013 Revisited \u201d covers this topic is much greater detail .","title":"Transaction"},{"location":"dbms/txn.html#transaction","text":"can't break it.","title":"Transaction"},{"location":"dbms/txn.html#acid","text":"atomicity consistency - after/before durable - latest updates even if system fails isolation - txn should be isolated","title":"ACID"},{"location":"dbms/txn.html#schedule-serializability","text":"","title":"schedule / serializability"},{"location":"dbms/txn.html#equavalence","text":"result - same I/O view Conflict Equivalence View equivalent schedules are view serializable and conflict equivalent schedules are conflict serializable. All conflict serializable schedules are view serializable too.","title":"Equavalence"},{"location":"dbms/txn.html#states","text":"active partially commited failed commited aborted","title":"States"},{"location":"dbms/txn.html#concurrent-txn","text":"lost update dirty read unrepeatable read incorrect summary reduced wait time high throughput high resource utilization","title":"Concurrent txn"},{"location":"dbms/txn.html#schedule","text":"serial complete recoverable cascadeless strict","title":"Schedule"},{"location":"dbms/txn.html#concurrency-control","text":"shared lock exclusive lock 2 phase locking protocol","title":"concurrency control"},{"location":"dbms/txn.html#base","text":"Basically Available : This constraint states that the system does guarantee the availability of the data as regards CAP Theorem ; there will be a response to any request . But , that response could still be \u2018 failure \u2019 to obtain the requested data or the data may be in an inconsistent or changing state , much like waiting for a check to clear in your bank account . Soft state : The state of the system could change over time , so even during times without input there may be changes going on due to \u2018 eventual consistency , \u2019 thus the state of the system is always \u2018 soft . \u2019 Eventual consistency : The system will eventually become consistent once it stops receiving input . The data will propagate to everywhere it should sooner or later , but the system will continue to receive input and is not checking the consistency of every transaction before it moves onto the next one . Werner Vogel \u2019 s article \u201c Eventually Consistent \u2013 Revisited \u201d covers this topic is much greater detail .","title":"BASE"},{"location":"ds/array/decay.html","text":"Array Decay \u00b6 Loss of type and dimensions. When we pass the array into function by value or pointer. It sends first address to the array which is a pointer, hence the size of array is not the original one, but the one occupied by the pointer in the memory.","title":"Array Decay"},{"location":"ds/array/decay.html#array-decay","text":"Loss of type and dimensions. When we pass the array into function by value or pointer. It sends first address to the array which is a pointer, hence the size of array is not the original one, but the one occupied by the pointer in the memory.","title":"Array Decay"},{"location":"ds/array/frequency.html","text":"Frequency of a Number \u00b6 simple \u00b6 void countFreq(int arr[], int n) { vector<bool> visited(n, false); for (int i = 0; i < n; i++) { if (visited[i] == true) continue; int count = 1; for (int j = i + 1; j < n; j++) { if (arr[i] == arr[j]) { visited[j] = true; count++; } } cout << arr[i] << \" \" << count << endl; } } Sort then Binary search \u00b6 Hashing \u00b6 void countFreq(int arr[], int n) { unordered_map<int, int> mp; for (int i = 0; i < n; i++) mp[arr[i]]++; for (auto x : mp) cout << x.first << \" \" << x.second << endl; }","title":"Frequency of a Number"},{"location":"ds/array/frequency.html#frequency-of-a-number","text":"","title":"Frequency of a Number"},{"location":"ds/array/frequency.html#simple","text":"void countFreq(int arr[], int n) { vector<bool> visited(n, false); for (int i = 0; i < n; i++) { if (visited[i] == true) continue; int count = 1; for (int j = i + 1; j < n; j++) { if (arr[i] == arr[j]) { visited[j] = true; count++; } } cout << arr[i] << \" \" << count << endl; } }","title":"simple"},{"location":"ds/array/frequency.html#sort-then-binary-search","text":"","title":"Sort then Binary search"},{"location":"ds/array/frequency.html#hashing","text":"void countFreq(int arr[], int n) { unordered_map<int, int> mp; for (int i = 0; i < n; i++) mp[arr[i]]++; for (auto x : mp) cout << x.first << \" \" << x.second << endl; }","title":"Hashing"},{"location":"ds/array/functions.html","text":"Passing Arrays to Functions \u00b6 void myFunction(int *param) {} void myFunction(int param[10]) {} void myFunction(int param[]) {} Return Array from Functions \u00b6 int * myFunction() {} Second point to remember is that C++ does not advocate to return the address of a local variable to outside of the function so you would have to define the local variable as static variable. #include <iostream> #include <ctime> using namespace std; // function to generate and retrun random numbers. int * getRandom( ) { static int r[10]; // set the seed srand( (unsigned)time( NULL ) ); for (int i = 0; i < 10; ++i) { r[i] = rand(); cout << r[i] << endl; } return r; } // main function to call above defined function. int main () { // a pointer to an int. int *p; p = getRandom(); for ( int i = 0; i < 10; i++ ) { cout << \"*(p + \" << i << \") : \"; cout << *(p + i) << endl; } return 0; }","title":"Passing Arrays to Functions"},{"location":"ds/array/functions.html#passing-arrays-to-functions","text":"void myFunction(int *param) {} void myFunction(int param[10]) {} void myFunction(int param[]) {}","title":"Passing Arrays to Functions"},{"location":"ds/array/functions.html#return-array-from-functions","text":"int * myFunction() {} Second point to remember is that C++ does not advocate to return the address of a local variable to outside of the function so you would have to define the local variable as static variable. #include <iostream> #include <ctime> using namespace std; // function to generate and retrun random numbers. int * getRandom( ) { static int r[10]; // set the seed srand( (unsigned)time( NULL ) ); for (int i = 0; i < 10; ++i) { r[i] = rand(); cout << r[i] << endl; } return r; } // main function to call above defined function. int main () { // a pointer to an int. int *p; p = getRandom(); for ( int i = 0; i < 10; i++ ) { cout << \"*(p + \" << i << \") : \"; cout << *(p + i) << endl; } return 0; }","title":"Return Array from Functions"},{"location":"ds/array/intro.html","text":"Introduction \u00b6 collection of similar data items stored at contiguous memory locations random access int, float, double, char, objects, structs, etc Why \u00b6 The idea of an array is to represent many instances in one variable. Declaration \u00b6 int arr1[10]; int n = 10; int arr2[n]; int arr[] = { 10, 20, 30, 40 } int arr[6] = { 10, 20, 30, 40 } // same as int arr[] = {10, 20, 30, 40, 0, 0} Advantages \u00b6 Random access less lines of code Easy access easy Traversal easy Sorting Disadvantages \u00b6 fixed number of elements Insertion and deletion of elements costly Accessing \u00b6 int arr[5]; arr[0] = 5; arr[2] = -10; arr[3 / 2] = 2; // this is same as arr[1] = 2 arr[3] = arr[0]; printf(\"%d %d %d %d\", arr[0], arr[1], arr[2], arr[3]); In C, no index out of bound checking \u00b6 int arr[2]; printf(\"%d \", arr[3]); printf(\"%d \", arr[-2]); int arr[2] = { 10, 20, 30, 40, 50 }; // The program won\u2019t compile in C++. If we save the above program as a .cpp, the program generates compiler error \u201cerror: too many initializers for \u2018int [2]'\u201d. Traversal \u00b6 #include<bits/stdc++.h> using namespace std; int main() { int arr[6]={11,12,13,14,15,16}; // Way -1 for(int i=0;i<6;i++) cout<<arr[i]<<\" \"; cout<<endl; // Way 2 cout<<\"By Other Method:\"<<endl; for(int i=0;i<6;i++) cout<<i[arr]<<\" \"; cout<<endl; return 0; }","title":"Introduction"},{"location":"ds/array/intro.html#introduction","text":"collection of similar data items stored at contiguous memory locations random access int, float, double, char, objects, structs, etc","title":"Introduction"},{"location":"ds/array/intro.html#why","text":"The idea of an array is to represent many instances in one variable.","title":"Why"},{"location":"ds/array/intro.html#declaration","text":"int arr1[10]; int n = 10; int arr2[n]; int arr[] = { 10, 20, 30, 40 } int arr[6] = { 10, 20, 30, 40 } // same as int arr[] = {10, 20, 30, 40, 0, 0}","title":"Declaration"},{"location":"ds/array/intro.html#advantages","text":"Random access less lines of code Easy access easy Traversal easy Sorting","title":"Advantages"},{"location":"ds/array/intro.html#disadvantages","text":"fixed number of elements Insertion and deletion of elements costly","title":"Disadvantages"},{"location":"ds/array/intro.html#accessing","text":"int arr[5]; arr[0] = 5; arr[2] = -10; arr[3 / 2] = 2; // this is same as arr[1] = 2 arr[3] = arr[0]; printf(\"%d %d %d %d\", arr[0], arr[1], arr[2], arr[3]);","title":"Accessing"},{"location":"ds/array/intro.html#in-c-no-index-out-of-bound-checking","text":"int arr[2]; printf(\"%d \", arr[3]); printf(\"%d \", arr[-2]); int arr[2] = { 10, 20, 30, 40, 50 }; // The program won\u2019t compile in C++. If we save the above program as a .cpp, the program generates compiler error \u201cerror: too many initializers for \u2018int [2]'\u201d.","title":"In C, no index out of bound checking"},{"location":"ds/array/intro.html#traversal","text":"#include<bits/stdc++.h> using namespace std; int main() { int arr[6]={11,12,13,14,15,16}; // Way -1 for(int i=0;i<6;i++) cout<<arr[i]<<\" \"; cout<<endl; // Way 2 cout<<\"By Other Method:\"<<endl; for(int i=0;i<6;i++) cout<<i[arr]<<\" \"; cout<<endl; return 0; }","title":"Traversal"},{"location":"ds/array/min-max.html","text":"Min Max \u00b6 Recursive \u00b6 int getMin(int arr[], int n) { return (n == 1) ? arr[0] : min(arr[0], getMin(arr + 1, n - 1)); } int getMax(int arr[], int n) { return (n == 1) ? arr[0] : max(arr[0], getMax(arr + 1, n - 1)); } Library Functions \u00b6 int getMin(int arr[], int n) { return *min_element(arr, arr + n); } int getMax(int arr[], int n) { return *max_element(arr, arr + n); }","title":"Min Max"},{"location":"ds/array/min-max.html#min-max","text":"","title":"Min Max"},{"location":"ds/array/min-max.html#recursive","text":"int getMin(int arr[], int n) { return (n == 1) ? arr[0] : min(arr[0], getMin(arr + 1, n - 1)); } int getMax(int arr[], int n) { return (n == 1) ? arr[0] : max(arr[0], getMax(arr + 1, n - 1)); }","title":"Recursive"},{"location":"ds/array/min-max.html#library-functions","text":"int getMin(int arr[], int n) { return *min_element(arr, arr + n); } int getMax(int arr[], int n) { return *max_element(arr, arr + n); }","title":"Library Functions"},{"location":"ds/array/multi-dimensional.html","text":"Multidimensional Arrays \u00b6 row major (in c++) \u00b6 data_type array_name[size1][size2]....[sizeN]; x[i][j] where i is the row number and \u2018j\u2019 is the column number. 0 to (x-1), 0 to (y-1) int x[3][4] = {0, 1 ,2 ,3 ,4 , 5 , 6 , 7 , 8 , 9 , 10 , 11} int x[3][4] = {{0,1,2,3}, {4,5,6,7}, {8,9,10,11}}; Three-Dimensional Array \u00b6 int x[2][3][4] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}; int x[2][3][4] = { { {0,1,2,3}, {4,5,6,7}, {8,9,10,11} }, { {12,13,14,15}, {16,17,18,19}, {20,21,22,23} } };","title":"Multidimensional Arrays"},{"location":"ds/array/multi-dimensional.html#multidimensional-arrays","text":"","title":"Multidimensional Arrays"},{"location":"ds/array/multi-dimensional.html#row-major-in-c","text":"data_type array_name[size1][size2]....[sizeN]; x[i][j] where i is the row number and \u2018j\u2019 is the column number. 0 to (x-1), 0 to (y-1) int x[3][4] = {0, 1 ,2 ,3 ,4 , 5 , 6 , 7 , 8 , 9 , 10 , 11} int x[3][4] = {{0,1,2,3}, {4,5,6,7}, {8,9,10,11}};","title":"row major (in c++)"},{"location":"ds/array/multi-dimensional.html#three-dimensional-array","text":"int x[2][3][4] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}; int x[2][3][4] = { { {0,1,2,3}, {4,5,6,7}, {8,9,10,11} }, { {12,13,14,15}, {16,17,18,19}, {20,21,22,23} } };","title":"Three-Dimensional Array"},{"location":"ds/array/reverse.html","text":"Reverse Array \u00b6 Iterative \u00b6 void rvereseArray(int arr[], int start, int end) { while (start < end) { int temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; start++; end--; } } Recursive \u00b6 void rvereseArray(int arr[], int start, int end) { if (start >= end) return; int temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; // Recursive Function calling rvereseArray(arr, start + 1, end - 1); }","title":"Reverse Array"},{"location":"ds/array/reverse.html#reverse-array","text":"","title":"Reverse Array"},{"location":"ds/array/reverse.html#iterative","text":"void rvereseArray(int arr[], int start, int end) { while (start < end) { int temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; start++; end--; } }","title":"Iterative"},{"location":"ds/array/reverse.html#recursive","text":"void rvereseArray(int arr[], int start, int end) { if (start >= end) return; int temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; // Recursive Function calling rvereseArray(arr, start + 1, end - 1); }","title":"Recursive"},{"location":"ds/array/vectors.html","text":"Vectors in cpp \u00b6 Iterators \u00b6 begin -> 1st element end -> last element r -> reverse c -> constant begin() end() rbegin() rend() cbegin() \u2013 constant iterator pointing to 1st element cend() crbegin() crend() \u2013 constant reverse iterator, pointing to theoretical element i.e. preceding the 1st element Capacity \u00b6 size() \u2013 number of elements max_size() \u2013 maximum number of elements capacity() \u2013 space currently allocated expressed as number of elements. resize(n) \u2013 Resizes the container to n elements. empty() \u2013 whether is empty. shrink_to_fit() \u2013 Reduces the capacity to fit its size and destroys all elements beyond the capacity. reserve() \u2013 Requests that the vector capacity be at least enough to contain n elements. Element access: \u00b6 reference operator [g] \u2013 reference to the element at position \u2018g\u2019 in the vector at(g) \u2013 reference to the element at position \u2018g\u2019 in the vector front() \u2013 reference to the first element in the vector back() \u2013 reference to the last element in the vector data() \u2013 direct pointer to the memory array used internally by the vector to store its owned elements. Modifiers: \u00b6 assign() \u2013 assigns new value to the elements by replacing old ones push_back() \u2013 push the elements from the back pop_back() \u2013 pop elements from the back. insert() \u2013 inserts new elements before specified position erase() \u2013 remove element at position or range. swap() \u2013 swap one vector with another (size may differ). clear() \u2013 remove all elements. emplace() \u2013 extends the container by inserting new element at position. emplace_back() \u2013 insert new element to the end.","title":"Vectors in cpp"},{"location":"ds/array/vectors.html#vectors-in-cpp","text":"","title":"Vectors in cpp"},{"location":"ds/array/vectors.html#iterators","text":"begin -> 1st element end -> last element r -> reverse c -> constant begin() end() rbegin() rend() cbegin() \u2013 constant iterator pointing to 1st element cend() crbegin() crend() \u2013 constant reverse iterator, pointing to theoretical element i.e. preceding the 1st element","title":"Iterators"},{"location":"ds/array/vectors.html#capacity","text":"size() \u2013 number of elements max_size() \u2013 maximum number of elements capacity() \u2013 space currently allocated expressed as number of elements. resize(n) \u2013 Resizes the container to n elements. empty() \u2013 whether is empty. shrink_to_fit() \u2013 Reduces the capacity to fit its size and destroys all elements beyond the capacity. reserve() \u2013 Requests that the vector capacity be at least enough to contain n elements.","title":"Capacity"},{"location":"ds/array/vectors.html#element-access","text":"reference operator [g] \u2013 reference to the element at position \u2018g\u2019 in the vector at(g) \u2013 reference to the element at position \u2018g\u2019 in the vector front() \u2013 reference to the first element in the vector back() \u2013 reference to the last element in the vector data() \u2013 direct pointer to the memory array used internally by the vector to store its owned elements.","title":"Element access:"},{"location":"ds/array/vectors.html#modifiers","text":"assign() \u2013 assigns new value to the elements by replacing old ones push_back() \u2013 push the elements from the back pop_back() \u2013 pop elements from the back. insert() \u2013 inserts new elements before specified position erase() \u2013 remove element at position or range. swap() \u2013 swap one vector with another (size may differ). clear() \u2013 remove all elements. emplace() \u2013 extends the container by inserting new element at position. emplace_back() \u2013 insert new element to the end.","title":"Modifiers:"},{"location":"ds/array/vs-pointers.html","text":"Array vs Pointers \u00b6 Difference \u00b6 #include <iostream> using namespace std; int main() { int arr[] = { 10, 20, 30, 40, 50, 60 }; int* ptr = arr; // sizof(int) * (number of element in arr[]) is printed cout << \"Size of arr[] \" << sizeof(arr) << \"\\n\"; // sizeof a pointer is printed which is same for all // type of pointers (char *, void *, etc) cout << \"Size of ptr \" << sizeof(ptr); return 0; } Output Size of arr[] 24 Size of ptr 8 Assignment \u00b6 #include <stdio.h> int main() { int arr[] = {10, 20}, x = 10; int *ptr = &x; // This is fine arr = &x; // Compiler Error return 0; } Similarities \u00b6 Array name -> address of 1st element. Members are accessed using pointer arithmetic. ( \u201carr[i]\u201d <=> *(arr + i)) Array parameters are always passed as pointers, even when we use square brackets.","title":"Array vs Pointers"},{"location":"ds/array/vs-pointers.html#array-vs-pointers","text":"","title":"Array vs Pointers"},{"location":"ds/array/vs-pointers.html#difference","text":"#include <iostream> using namespace std; int main() { int arr[] = { 10, 20, 30, 40, 50, 60 }; int* ptr = arr; // sizof(int) * (number of element in arr[]) is printed cout << \"Size of arr[] \" << sizeof(arr) << \"\\n\"; // sizeof a pointer is printed which is same for all // type of pointers (char *, void *, etc) cout << \"Size of ptr \" << sizeof(ptr); return 0; } Output Size of arr[] 24 Size of ptr 8","title":"Difference"},{"location":"ds/array/vs-pointers.html#assignment","text":"#include <stdio.h> int main() { int arr[] = {10, 20}, x = 10; int *ptr = &x; // This is fine arr = &x; // Compiler Error return 0; }","title":"Assignment"},{"location":"ds/array/vs-pointers.html#similarities","text":"Array name -> address of 1st element. Members are accessed using pointer arithmetic. ( \u201carr[i]\u201d <=> *(arr + i)) Array parameters are always passed as pointers, even when we use square brackets.","title":"Similarities"},{"location":"ds/graph/bfs.html","text":"BFS \u00b6 Application: \u00b6 shortest path / mst PSP networks Crawlers social netowwrking sites GPS navigation Broadcasting network In Garbage Collection: Cheney\u2019s algorithm. It is preferred over DFS because of better locality of reference: cycle detection Ford\u2013Fulkerson algorithm bipartite PAth Finding Finding all nodes within one connected component: To print all the vertices, we can modify the BFS function to do traversal starting from all nodes one by one (Like the DFS modified version). O(V+E) O(E*V) void BFS(int s) { bool *visited = new bool[V]; for(int i = 0; i < V; i++) visited[i] = false; list<int> queue; visited[s] = true; queue.push_back(s); list<int>::iterator i; while(!queue.empty()) { s = queue.front(); cout << s << \" \"; queue.pop_front(); for (i = adj[s].begin(); i != adj[s].end(); ++i) { if (!visited[*i]) { visited[*i] = true; queue.push_back(*i); } } } }","title":"BFS"},{"location":"ds/graph/bfs.html#bfs","text":"","title":"BFS"},{"location":"ds/graph/bfs.html#application","text":"shortest path / mst PSP networks Crawlers social netowwrking sites GPS navigation Broadcasting network In Garbage Collection: Cheney\u2019s algorithm. It is preferred over DFS because of better locality of reference: cycle detection Ford\u2013Fulkerson algorithm bipartite PAth Finding Finding all nodes within one connected component: To print all the vertices, we can modify the BFS function to do traversal starting from all nodes one by one (Like the DFS modified version). O(V+E) O(E*V) void BFS(int s) { bool *visited = new bool[V]; for(int i = 0; i < V; i++) visited[i] = false; list<int> queue; visited[s] = true; queue.push_back(s); list<int>::iterator i; while(!queue.empty()) { s = queue.front(); cout << s << \" \"; queue.pop_front(); for (i = adj[s].begin(); i != adj[s].end(); ++i) { if (!visited[*i]) { visited[*i] = true; queue.push_back(*i); } } } }","title":"Application:"},{"location":"ds/graph/dfs-disconnected.html","text":"Disconnected DFS \u00b6 Create a recursive function that takes the index of node and a visited array. Mark the current node as visited and print the node. Traverse all the adjacent and unmarked nodes and call the recursive function with index of adjacent node. Run a loop from 0 to number of vertices and check if the node is unvisited in previous DFS then call the recursive function with current node. O(V + E) , O(V). void Graph::DFSUtil(int v, bool visited[]) { visited[v] = true; cout << v << \" \"; list<int>::iterator i; for(i = adj[v].begin(); i != adj[v].end(); ++i) if(!visited[*i]) DFSUtil(*i, visited); } void Graph::DFS() { bool *visited = new bool[V]; for (int i = 0; i < V; i++) visited[i] = false; for (int i = 0; i < V; i++) if (visited[i] == false) DFSUtil(i, visited); }","title":"Disconnected DFS"},{"location":"ds/graph/dfs-disconnected.html#disconnected-dfs","text":"Create a recursive function that takes the index of node and a visited array. Mark the current node as visited and print the node. Traverse all the adjacent and unmarked nodes and call the recursive function with index of adjacent node. Run a loop from 0 to number of vertices and check if the node is unvisited in previous DFS then call the recursive function with current node. O(V + E) , O(V). void Graph::DFSUtil(int v, bool visited[]) { visited[v] = true; cout << v << \" \"; list<int>::iterator i; for(i = adj[v].begin(); i != adj[v].end(); ++i) if(!visited[*i]) DFSUtil(*i, visited); } void Graph::DFS() { bool *visited = new bool[V]; for (int i = 0; i < V; i++) visited[i] = false; for (int i = 0; i < V; i++) if (visited[i] == false) DFSUtil(i, visited); }","title":"Disconnected DFS"},{"location":"ds/graph/dfs.html","text":"DFS \u00b6 Create a recursive function that takes the index of node and a visited array. Mark the current node as visited and print the node. Traverse all the adjacent and unmarked nodes and call the recursive function with index of adjacent node. Application: \u00b6 mst, shortest path tree detect cycle, if there is back egde in dfs then there is cycle Pth finding: prepare stack when v detect pop it. topological sorting: scheduling jobs from the given dependencies among jobs. bipartite graph test strongly connected graph solving puzzles wiht only one solutiion such as mazes O(V + E) , O(V). void DFSUtil(int v, bool visited[]) { visited[v] = true; cout << v << \" \"; list<int>::iterator i; for (i = adj[v].begin(); i != adj[v].end(); ++i) if (!visited[*i]) DFSUtil(*i, visited); } void DFS(int v) { bool *visited = new bool[V]; for (int i = 0; i < V; i++) visited[i] = false; DFSUtil(v, visited); }","title":"DFS"},{"location":"ds/graph/dfs.html#dfs","text":"Create a recursive function that takes the index of node and a visited array. Mark the current node as visited and print the node. Traverse all the adjacent and unmarked nodes and call the recursive function with index of adjacent node.","title":"DFS"},{"location":"ds/graph/dfs.html#application","text":"mst, shortest path tree detect cycle, if there is back egde in dfs then there is cycle Pth finding: prepare stack when v detect pop it. topological sorting: scheduling jobs from the given dependencies among jobs. bipartite graph test strongly connected graph solving puzzles wiht only one solutiion such as mazes O(V + E) , O(V). void DFSUtil(int v, bool visited[]) { visited[v] = true; cout << v << \" \"; list<int>::iterator i; for (i = adj[v].begin(); i != adj[v].end(); ++i) if (!visited[*i]) DFSUtil(*i, visited); } void DFS(int v) { bool *visited = new bool[V]; for (int i = 0; i < V; i++) visited[i] = false; DFSUtil(v, visited); }","title":"Application:"},{"location":"ds/graph/intro.html","text":"Graph \u00b6 Properties \u00b6 finite set of vertices -> nodes set of ordered pair (u,v) telling aedge bw vertex v and u. f(u,v) weight/value/cost. Applications \u00b6 networks -> facebook, node has person info. Representation \u00b6 Adjacency List Adjacency Matrix Incidence Matrix Incidence List Depends on use case -> type of operations Adjacency matrix \u00b6 2D array of size V x V adj[i][j] = 1, adj[i][j] = w Pros \u00b6 deletion, isThereAnEdge -> O(1) Cons \u00b6 space -> O(V^2) even matrix is sparse inserting an edge -> O(V^2) Adjacency List \u00b6 array[i] -> list of vertices adjacent to ith vertex. Weights -> as lists of pairs Pros \u00b6 Saves space O(|V|+|E|) worst case -> O(V^2) Adding a vertex is easier Cons \u00b6 isThereAnEdge -> O(V) #include <iostream> #include <vector> using namespace std; void addEdge(vector<int> adj[], int u, int v) { adj[u].push_back(v); adj[v].push_back(u); } void printGraph(vector<int> adj[], int V){ for (int v = 0; v < V; ++v) { cout << \"\\n Adjacency list of vertex \" << v << \"\\n head \"; for (auto x : adj[v]) cout << \"-> \" << x; printf(\"\\n\"); } } int main(){ int V = 5; vector<int> adj[V]; addEdge(adj, 0, 1); addEdge(adj, 0, 4); addEdge(adj, 1, 2); addEdge(adj, 1, 3); addEdge(adj, 1, 4); addEdge(adj, 2, 3); addEdge(adj, 3, 4); printGraph(adj, V); return 0; }","title":"Graph"},{"location":"ds/graph/intro.html#graph","text":"","title":"Graph"},{"location":"ds/graph/intro.html#properties","text":"finite set of vertices -> nodes set of ordered pair (u,v) telling aedge bw vertex v and u. f(u,v) weight/value/cost.","title":"Properties"},{"location":"ds/graph/intro.html#applications","text":"networks -> facebook, node has person info.","title":"Applications"},{"location":"ds/graph/intro.html#representation","text":"Adjacency List Adjacency Matrix Incidence Matrix Incidence List Depends on use case -> type of operations","title":"Representation"},{"location":"ds/graph/intro.html#adjacency-matrix","text":"2D array of size V x V adj[i][j] = 1, adj[i][j] = w","title":"Adjacency matrix"},{"location":"ds/graph/intro.html#pros","text":"deletion, isThereAnEdge -> O(1)","title":"Pros"},{"location":"ds/graph/intro.html#cons","text":"space -> O(V^2) even matrix is sparse inserting an edge -> O(V^2)","title":"Cons"},{"location":"ds/graph/intro.html#adjacency-list","text":"array[i] -> list of vertices adjacent to ith vertex. Weights -> as lists of pairs","title":"Adjacency List"},{"location":"ds/graph/intro.html#pros_1","text":"Saves space O(|V|+|E|) worst case -> O(V^2) Adding a vertex is easier","title":"Pros"},{"location":"ds/graph/intro.html#cons_1","text":"isThereAnEdge -> O(V) #include <iostream> #include <vector> using namespace std; void addEdge(vector<int> adj[], int u, int v) { adj[u].push_back(v); adj[v].push_back(u); } void printGraph(vector<int> adj[], int V){ for (int v = 0; v < V; ++v) { cout << \"\\n Adjacency list of vertex \" << v << \"\\n head \"; for (auto x : adj[v]) cout << \"-> \" << x; printf(\"\\n\"); } } int main(){ int V = 5; vector<int> adj[V]; addEdge(adj, 0, 1); addEdge(adj, 0, 4); addEdge(adj, 1, 2); addEdge(adj, 1, 3); addEdge(adj, 1, 4); addEdge(adj, 2, 3); addEdge(adj, 3, 4); printGraph(adj, V); return 0; }","title":"Cons"},{"location":"ds/heap/fibonacci-heap.html","text":"Fibonacci Heap \u00b6 Find Min: \u0398(1) [Same as both Binary and Binomial] Delete Min: O(Log n) [\u0398(Log n) in both Binary and Binomial] Insert: \u0398(1) [\u0398(Log n) in Binary and \u0398(1) in Binomial] Decrease-Key: \u0398(1) [\u0398(Log n) in both Binary and Binomial] Merge: \u0398(1) [\u0398(m Log n) or \u0398(m+n) in Binary and \u0398(Log n) in Binomial] Like Binomial Heap, Fibonacci Heap is a collection of trees with min-heap or max-heap property. In Fibonacci Heap, trees can can have any shape even all trees can be single nodes (This is unlike Binomial Heap where every tree has to be Binomial Tree). Fibonacci Heap maintains a pointer to minimum value (which is root of a tree). All tree roots are connected using circular doubly linked list, so all of them can be accessed using single \u2018min\u2019 pointer. The main idea is to execute operations in \u201clazy\u201d way. For example merge operation simply links two heaps, insert operation simply adds a new tree with single node. The operation extract minimum is the most complicated operation. It does delayed work of consolidating trees. This makes delete also complicated as delete first decreases key to minus infinite, then calls extract minimum. Below are some interesting facts about Fibonacci Heap The reduced time complexity of Decrease-Key has importance in Dijkstra and Prim algorithms. With Binary Heap, time complexity of these algorithms is O(VLogV + ELogV). If Fibonacci Heap is used, then time complexity is improved to O(VLogV + E) Although Fibonacci Heap looks promising time complexity wise, it has been found slow in practice as hidden constants are high (Source Wiki). Fibonacci heap are mainly called so because Fibonacci numbers are used in the running time analysis. Also, every node in Fibonacci Heap has degree at most O(log n) and the size of a subtree rooted in a node of degree k is at least F^k+2, where F^k is the kth Fibonacci number.","title":"Fibonacci Heap"},{"location":"ds/heap/fibonacci-heap.html#fibonacci-heap","text":"Find Min: \u0398(1) [Same as both Binary and Binomial] Delete Min: O(Log n) [\u0398(Log n) in both Binary and Binomial] Insert: \u0398(1) [\u0398(Log n) in Binary and \u0398(1) in Binomial] Decrease-Key: \u0398(1) [\u0398(Log n) in both Binary and Binomial] Merge: \u0398(1) [\u0398(m Log n) or \u0398(m+n) in Binary and \u0398(Log n) in Binomial] Like Binomial Heap, Fibonacci Heap is a collection of trees with min-heap or max-heap property. In Fibonacci Heap, trees can can have any shape even all trees can be single nodes (This is unlike Binomial Heap where every tree has to be Binomial Tree). Fibonacci Heap maintains a pointer to minimum value (which is root of a tree). All tree roots are connected using circular doubly linked list, so all of them can be accessed using single \u2018min\u2019 pointer. The main idea is to execute operations in \u201clazy\u201d way. For example merge operation simply links two heaps, insert operation simply adds a new tree with single node. The operation extract minimum is the most complicated operation. It does delayed work of consolidating trees. This makes delete also complicated as delete first decreases key to minus infinite, then calls extract minimum. Below are some interesting facts about Fibonacci Heap The reduced time complexity of Decrease-Key has importance in Dijkstra and Prim algorithms. With Binary Heap, time complexity of these algorithms is O(VLogV + ELogV). If Fibonacci Heap is used, then time complexity is improved to O(VLogV + E) Although Fibonacci Heap looks promising time complexity wise, it has been found slow in practice as hidden constants are high (Source Wiki). Fibonacci heap are mainly called so because Fibonacci numbers are used in the running time analysis. Also, every node in Fibonacci Heap has degree at most O(log n) and the size of a subtree rooted in a node of degree k is at least F^k+2, where F^k is the kth Fibonacci number.","title":"Fibonacci Heap"},{"location":"ds/heap/heap-sort.html","text":"TODO","title":"Heap sort"},{"location":"ds/heap/intro.html","text":"Binary Heap \u00b6 Complete Binary Tree Max Heap AND Min Heap Representaion \u00b6 Arrays \u00b6 root -> Arr[0]. For Arr[i]: Arr[(i-1)/2] -> parent node Arr[(2 i)+1] -> left child node Arr[(2 i)+2] -> right child node Traversal \u00b6 Level order aka Breadth First as storage is same. Operations \u00b6 getMin(): returns root. extractMin(): Removes minimum. as it calls heapify() after removing root. decreaseKey(): Decreases value of key. IF key value > parent THEN do anything. ELSE, traverse up to fix the violated heap property. insert() : New key Add a new key at the end of the tree. IF new key is greater than its parent, do nothing. Otherwise, traverse up to maintain heap property. delete(): Deleting a key We replace the key to be deleted with minum infinite by calling decreaseKey(). After decreaseKey(), the minus infinite value must reach root, so we call extractMin() to remove the key. Application \u00b6 Heap Sort : Heap Sort uses Binary Heap to sort an array in O(nLogn) time. Priority Queue : Priority queues can be efficiently implemented using Binary Heap because it supports insert(), delete() and extractmax(), decreaseKey() operations in O(logn) time. Binomoial Heap and Fibonacci Heap are variations of Binary Heap. These variations perform union also efficiently. Graph Algorithms : priority queues -> Dijkstra\u2019s Shortest Path, Prim\u2019s Minimum Spanning Tree. 5. a. K\u2019th Largest Element in an array. b. Sort an almost sorted array / Merge K Sorted Arrays. Why is Binary Heap Preferred over BST for Priority Queue? \u00b6 A typical Priority Queue requires following operations to be efficient. Get Top Priority Element (Get minimum or maximum) Insert an element Remove top priority element Decrease Key A Binary Heap supports above operations with following time complexities: O(1) O(Logn) O(Logn) O(Logn) A Self Balancing Binary Search Tree like AVL Tree, Red-Black Tree, etc can also support above operations with same time complexities. Finding minimum and maximum are not naturally O(1), but can be easily implemented in O(1) by keeping an extra pointer to minimum or maximum and updating the pointer with insertion and deletion if required. With deletion we can update by finding inorder predecessor or successor. Inserting an element is naturally O(Logn) Removing maximum or minimum are also O(Logn) Decrease key can be done in O(Logn) by doing a deletion followed by insertion. So why is Binary Heap Preferred for Priority Queue? arrays-> locallity of refernence -> cache friendliness constants in Binary Search Tree are higher. constructing Binary Heap -> O(n) Self Balancing BSTs -> O(nLogn) Binary Heap doesn\u2019t require extra space for pointers. Binary Heap is easier to implement. TFibonacci Heap -> insert and decrease-key -> \u0398(1) time Is Binary Heap always better? Although Binary Heap is for Priority Queue, BSTs have their own advantages and the list of advantages is in-fact bigger compared to binary heap. Searching an element in self-balancing BST is O(Logn) which is O(n) in Binary Heap. We can print all elements of BST in sorted order in O(n) time, but Binary Heap requires O(nLogn) time. Floor and ceil can be found in O(Logn) time. K\u2019th largest/smallest element be found in O(Logn) time by augmenting tree with an additional field. // 0(logn) , O(n) #include<iostream> #include<climits> using namespace std; void swap(int *x, int *y); class MinHeap { int *harr; int capacity; int heap_size; public: MinHeap(int capacity); void MinHeapify(int ); int parent(int i) { return (i-1)/2; } int left(int i) { return (2*i + 1); } int right(int i) { return (2*i + 2); } int extractMin(); void decreaseKey(int i, int new_val); int getMin() { return harr[0]; } void deleteKey(int i); void insertKey(int k); }; MinHeap::MinHeap(int cap) { heap_size = 0; capacity = cap; harr = new int[cap]; } void MinHeap::insertKey(int k) // O(logn) { if (heap_size == capacity) { cout << \"\\nOverflow: Could not insertKey\\n\"; return; } heap_size++; int i = heap_size - 1; harr[i] = k; while (i != 0 && harr[parent(i)] > harr[i]) { swap(&harr[i], &harr[parent(i)]); i = parent(i); } } void MinHeap::decreaseKey(int i, int new_val) // O(logn) { harr[i] = new_val; while (i != 0 && harr[parent(i)] > harr[i]) { swap(&harr[i], &harr[parent(i)]); i = parent(i); } } int MinHeap::extractMin() // O(logn) { if (heap_size <= 0) return INT_MAX; if (heap_size == 1) { heap_size--; return harr[0]; } int root = harr[0]; harr[0] = harr[heap_size-1]; heap_size--; MinHeapify(0); return root; } void MinHeap::deleteKey(int i) // O(logn) { decreaseKey(i, INT_MIN); extractMin(); } void MinHeap::MinHeapify(int i) // O(logn) { int l = left(i); int r = right(i); int smallest = i; if (l < heap_size && harr[l] < harr[i]) smallest = l; if (r < heap_size && harr[r] < harr[smallest]) smallest = r; if (smallest != i) { swap(&harr[i], &harr[smallest]); MinHeapify(smallest); } } void swap(int *x, int *y) { int temp = *x; *x = *y; *y = temp; } int main() { MinHeap h(11); h.insertKey(3); h.insertKey(2); h.deleteKey(1); h.insertKey(15); h.insertKey(5); h.insertKey(4); h.insertKey(45); cout << h.extractMin() << \" \"; cout << h.getMin() << \" \"; h.decreaseKey(2, 1); cout << h.getMin(); return 0; }","title":"Binary Heap"},{"location":"ds/heap/intro.html#binary-heap","text":"Complete Binary Tree Max Heap AND Min Heap","title":"Binary Heap"},{"location":"ds/heap/intro.html#representaion","text":"","title":"Representaion"},{"location":"ds/heap/intro.html#arrays","text":"root -> Arr[0]. For Arr[i]: Arr[(i-1)/2] -> parent node Arr[(2 i)+1] -> left child node Arr[(2 i)+2] -> right child node","title":"Arrays"},{"location":"ds/heap/intro.html#traversal","text":"Level order aka Breadth First as storage is same.","title":"Traversal"},{"location":"ds/heap/intro.html#operations","text":"getMin(): returns root. extractMin(): Removes minimum. as it calls heapify() after removing root. decreaseKey(): Decreases value of key. IF key value > parent THEN do anything. ELSE, traverse up to fix the violated heap property. insert() : New key Add a new key at the end of the tree. IF new key is greater than its parent, do nothing. Otherwise, traverse up to maintain heap property. delete(): Deleting a key We replace the key to be deleted with minum infinite by calling decreaseKey(). After decreaseKey(), the minus infinite value must reach root, so we call extractMin() to remove the key.","title":"Operations"},{"location":"ds/heap/intro.html#application","text":"Heap Sort : Heap Sort uses Binary Heap to sort an array in O(nLogn) time. Priority Queue : Priority queues can be efficiently implemented using Binary Heap because it supports insert(), delete() and extractmax(), decreaseKey() operations in O(logn) time. Binomoial Heap and Fibonacci Heap are variations of Binary Heap. These variations perform union also efficiently. Graph Algorithms : priority queues -> Dijkstra\u2019s Shortest Path, Prim\u2019s Minimum Spanning Tree. 5. a. K\u2019th Largest Element in an array. b. Sort an almost sorted array / Merge K Sorted Arrays.","title":"Application"},{"location":"ds/heap/intro.html#why-is-binary-heap-preferred-over-bst-for-priority-queue","text":"A typical Priority Queue requires following operations to be efficient. Get Top Priority Element (Get minimum or maximum) Insert an element Remove top priority element Decrease Key A Binary Heap supports above operations with following time complexities: O(1) O(Logn) O(Logn) O(Logn) A Self Balancing Binary Search Tree like AVL Tree, Red-Black Tree, etc can also support above operations with same time complexities. Finding minimum and maximum are not naturally O(1), but can be easily implemented in O(1) by keeping an extra pointer to minimum or maximum and updating the pointer with insertion and deletion if required. With deletion we can update by finding inorder predecessor or successor. Inserting an element is naturally O(Logn) Removing maximum or minimum are also O(Logn) Decrease key can be done in O(Logn) by doing a deletion followed by insertion. So why is Binary Heap Preferred for Priority Queue? arrays-> locallity of refernence -> cache friendliness constants in Binary Search Tree are higher. constructing Binary Heap -> O(n) Self Balancing BSTs -> O(nLogn) Binary Heap doesn\u2019t require extra space for pointers. Binary Heap is easier to implement. TFibonacci Heap -> insert and decrease-key -> \u0398(1) time Is Binary Heap always better? Although Binary Heap is for Priority Queue, BSTs have their own advantages and the list of advantages is in-fact bigger compared to binary heap. Searching an element in self-balancing BST is O(Logn) which is O(n) in Binary Heap. We can print all elements of BST in sorted order in O(n) time, but Binary Heap requires O(nLogn) time. Floor and ceil can be found in O(Logn) time. K\u2019th largest/smallest element be found in O(Logn) time by augmenting tree with an additional field. // 0(logn) , O(n) #include<iostream> #include<climits> using namespace std; void swap(int *x, int *y); class MinHeap { int *harr; int capacity; int heap_size; public: MinHeap(int capacity); void MinHeapify(int ); int parent(int i) { return (i-1)/2; } int left(int i) { return (2*i + 1); } int right(int i) { return (2*i + 2); } int extractMin(); void decreaseKey(int i, int new_val); int getMin() { return harr[0]; } void deleteKey(int i); void insertKey(int k); }; MinHeap::MinHeap(int cap) { heap_size = 0; capacity = cap; harr = new int[cap]; } void MinHeap::insertKey(int k) // O(logn) { if (heap_size == capacity) { cout << \"\\nOverflow: Could not insertKey\\n\"; return; } heap_size++; int i = heap_size - 1; harr[i] = k; while (i != 0 && harr[parent(i)] > harr[i]) { swap(&harr[i], &harr[parent(i)]); i = parent(i); } } void MinHeap::decreaseKey(int i, int new_val) // O(logn) { harr[i] = new_val; while (i != 0 && harr[parent(i)] > harr[i]) { swap(&harr[i], &harr[parent(i)]); i = parent(i); } } int MinHeap::extractMin() // O(logn) { if (heap_size <= 0) return INT_MAX; if (heap_size == 1) { heap_size--; return harr[0]; } int root = harr[0]; harr[0] = harr[heap_size-1]; heap_size--; MinHeapify(0); return root; } void MinHeap::deleteKey(int i) // O(logn) { decreaseKey(i, INT_MIN); extractMin(); } void MinHeap::MinHeapify(int i) // O(logn) { int l = left(i); int r = right(i); int smallest = i; if (l < heap_size && harr[l] < harr[i]) smallest = l; if (r < heap_size && harr[r] < harr[smallest]) smallest = r; if (smallest != i) { swap(&harr[i], &harr[smallest]); MinHeapify(smallest); } } void swap(int *x, int *y) { int temp = *x; *x = *y; *y = temp; } int main() { MinHeap h(11); h.insertKey(3); h.insertKey(2); h.deleteKey(1); h.insertKey(15); h.insertKey(5); h.insertKey(4); h.insertKey(45); cout << h.extractMin() << \" \"; cout << h.getMin() << \" \"; h.decreaseKey(2, 1); cout << h.getMin(); return 0; }","title":"Why is Binary Heap Preferred over BST for Priority Queue?"},{"location":"ds/linked-list/bin-tree-cdll.html","text":"Binary Tree to Circular DLL \u00b6 (In Place) CDLL \u00b6 struct Node { struct Node *left, *right; int data; }; Concatenate two CDLL's \u00b6 Node *concatenate(Node *leftList, Node *rightList) { if (leftList == NULL) return rightList; if (rightList == NULL) return leftList; Node *leftLast = leftList->left; Node *rightLast = rightList->left; leftLast->right = rightList; rightList->left = leftLast; leftList->left = rightLast; rightLast->right = leftList; return leftList; } Main Function \u00b6 Node *bTreeToCList(Node *root) { if (root == NULL) return NULL; Node *left = bTreeToCList(root->left); Node *right = bTreeToCList(root->right); root->left = root->right = root; return concatenate(concatenate(left, root), right); } Output: Circular Linked List is : 25 12 30 10 36 15","title":"Binary Tree to Circular DLL"},{"location":"ds/linked-list/bin-tree-cdll.html#binary-tree-to-circular-dll","text":"(In Place)","title":"Binary Tree to Circular DLL"},{"location":"ds/linked-list/bin-tree-cdll.html#cdll","text":"struct Node { struct Node *left, *right; int data; };","title":"CDLL"},{"location":"ds/linked-list/bin-tree-cdll.html#concatenate-two-cdlls","text":"Node *concatenate(Node *leftList, Node *rightList) { if (leftList == NULL) return rightList; if (rightList == NULL) return leftList; Node *leftLast = leftList->left; Node *rightLast = rightList->left; leftLast->right = rightList; rightList->left = leftLast; leftList->left = rightLast; rightLast->right = leftList; return leftList; }","title":"Concatenate two CDLL's"},{"location":"ds/linked-list/bin-tree-cdll.html#main-function","text":"Node *bTreeToCList(Node *root) { if (root == NULL) return NULL; Node *left = bTreeToCList(root->left); Node *right = bTreeToCList(root->right); root->left = root->right = root; return concatenate(concatenate(left, root), right); } Output: Circular Linked List is : 25 12 30 10 36 15","title":"Main Function"},{"location":"ds/linked-list/cll.html","text":"Circular LL \u00b6 end node points to start single or doubly you can traverse from anywhere, take note of starting node good for queues. usefl for cyclation. eg cpu shceduling fibonici heap Representation \u00b6 class Node { public: int data; Node *next; }; Traversal \u00b6 void printList(struct Node *first) { struct Node *temp = first; if (first != NULL) { do { printf(\"%d \", temp->data); temp = temp->next; } while (temp != first); } } Insertion \u00b6 have ext pointer pointing at end empty list beginning end in between struct Node *addToEmpty(struct Node *last, int data) { if (last != NULL) return last; struct Node *temp = (struct Node*)malloc(sizeof(struct Node)); temp -> data = data; last = temp; last -> next = last; return last; } struct Node *addBegin(struct Node *last, int data) { if (last == NULL) return addToEmpty(last, data); struct Node *temp = (struct Node *)malloc(sizeof(struct Node)); temp -> data = data; temp -> next = last -> next; last -> next = temp; return last; } struct Node *addEnd(struct Node *last, int data) { if (last == NULL) return addToEmpty(last, data); struct Node *temp = (struct Node *)malloc(sizeof(struct Node)); temp -> data = data; temp -> next = last -> next; last -> next = temp; last = temp; return last; } struct Node *addAfter(struct Node *last, int data, int item) { if (last == NULL) return NULL; struct Node *temp, *p; p = last -> next; do { if (p ->data == item) { temp = (struct Node *)malloc(sizeof(struct Node)); temp -> data = data; temp -> next = p -> next; p -> next = temp; if (p == last) last = temp; return last; } p = p -> next; } while(p != last -> next); cout << item << \" not present in the list.\" << endl; return last; } Problems: \u00b6 split in halve -> tortoise and haire cll sorted insert -> find and insert cases Case 2 of the above algorithm/code can be optimized. To implement the suggested change we need to modify the case 2 to following. // Case 2 of the above algo else if (current->data >= new_node->data) { // swap the data part of head node and new node // assuming that we have a function swap(int *, int *) swap(&(current->data), &(new_node->data)); new_node->next = (*head_ref)->next; (*head_ref)->next = new_node; } CLL Sorted Insert \u00b6 void sortedInsert(Node** head_ref, Node* new_node) { Node* current = *head_ref; if (current == NULL) { new_node->next = new_node; *head_ref = new_node; } else if (current->data >= new_node->data) { while(current->next != *head_ref) current = current->next; current->next = new_node; new_node->next = *head_ref; *head_ref = new_node; } else { while (current->next!= *head_ref && current->next->data < new_node->data) current = current->next; new_node->next = current->next; current->next = new_node; } }","title":"Circular LL"},{"location":"ds/linked-list/cll.html#circular-ll","text":"end node points to start single or doubly you can traverse from anywhere, take note of starting node good for queues. usefl for cyclation. eg cpu shceduling fibonici heap","title":"Circular LL"},{"location":"ds/linked-list/cll.html#representation","text":"class Node { public: int data; Node *next; };","title":"Representation"},{"location":"ds/linked-list/cll.html#traversal","text":"void printList(struct Node *first) { struct Node *temp = first; if (first != NULL) { do { printf(\"%d \", temp->data); temp = temp->next; } while (temp != first); } }","title":"Traversal"},{"location":"ds/linked-list/cll.html#insertion","text":"have ext pointer pointing at end empty list beginning end in between struct Node *addToEmpty(struct Node *last, int data) { if (last != NULL) return last; struct Node *temp = (struct Node*)malloc(sizeof(struct Node)); temp -> data = data; last = temp; last -> next = last; return last; } struct Node *addBegin(struct Node *last, int data) { if (last == NULL) return addToEmpty(last, data); struct Node *temp = (struct Node *)malloc(sizeof(struct Node)); temp -> data = data; temp -> next = last -> next; last -> next = temp; return last; } struct Node *addEnd(struct Node *last, int data) { if (last == NULL) return addToEmpty(last, data); struct Node *temp = (struct Node *)malloc(sizeof(struct Node)); temp -> data = data; temp -> next = last -> next; last -> next = temp; last = temp; return last; } struct Node *addAfter(struct Node *last, int data, int item) { if (last == NULL) return NULL; struct Node *temp, *p; p = last -> next; do { if (p ->data == item) { temp = (struct Node *)malloc(sizeof(struct Node)); temp -> data = data; temp -> next = p -> next; p -> next = temp; if (p == last) last = temp; return last; } p = p -> next; } while(p != last -> next); cout << item << \" not present in the list.\" << endl; return last; }","title":"Insertion"},{"location":"ds/linked-list/cll.html#problems","text":"split in halve -> tortoise and haire cll sorted insert -> find and insert cases Case 2 of the above algorithm/code can be optimized. To implement the suggested change we need to modify the case 2 to following. // Case 2 of the above algo else if (current->data >= new_node->data) { // swap the data part of head node and new node // assuming that we have a function swap(int *, int *) swap(&(current->data), &(new_node->data)); new_node->next = (*head_ref)->next; (*head_ref)->next = new_node; }","title":"Problems:"},{"location":"ds/linked-list/cll.html#cll-sorted-insert","text":"void sortedInsert(Node** head_ref, Node* new_node) { Node* current = *head_ref; if (current == NULL) { new_node->next = new_node; *head_ref = new_node; } else if (current->data >= new_node->data) { while(current->next != *head_ref) current = current->next; current->next = new_node; new_node->next = *head_ref; *head_ref = new_node; } else { while (current->next!= *head_ref && current->next->data < new_node->data) current = current->next; new_node->next = current->next; current->next = new_node; } }","title":"CLL Sorted Insert"},{"location":"ds/linked-list/count.html","text":"Counting \u00b6 O(n) Iterative \u00b6 int getCount(Node* head) { int count = 0; Node* current = head; while (current != NULL) { count++; current = current->next; } return count; } Recursive \u00b6 int getCount(struct Node* head) { if (head == NULL) return 0; return 1 + getCount(head->next); }","title":"Counting"},{"location":"ds/linked-list/count.html#counting","text":"O(n)","title":"Counting"},{"location":"ds/linked-list/count.html#iterative","text":"int getCount(Node* head) { int count = 0; Node* current = head; while (current != NULL) { count++; current = current->next; } return count; }","title":"Iterative"},{"location":"ds/linked-list/count.html#recursive","text":"int getCount(struct Node* head) { if (head == NULL) return 0; return 1 + getCount(head->next); }","title":"Recursive"},{"location":"ds/linked-list/deletion.html","text":"Deletion \u00b6 First \u00b6 void deleteFirst(struct Node **head_ref) { if(*head_ref != NULL) { struct Node *temp = *head_ref; *head_ref = (*head_ref)->next; free(temp); } } Any \u00b6 void deleteNode(struct Node **head_ref, int key) { struct Node* temp = *head_ref, *prev; if (temp != NULL && temp->data == key) { *head_ref = temp->next; free(temp); return; } while (temp != NULL && temp->data != key) { prev = temp; temp = temp->next; } if (temp == NULL) return; prev->next = temp->next; free(temp); }","title":"Deletion"},{"location":"ds/linked-list/deletion.html#deletion","text":"","title":"Deletion"},{"location":"ds/linked-list/deletion.html#first","text":"void deleteFirst(struct Node **head_ref) { if(*head_ref != NULL) { struct Node *temp = *head_ref; *head_ref = (*head_ref)->next; free(temp); } }","title":"First"},{"location":"ds/linked-list/deletion.html#any","text":"void deleteNode(struct Node **head_ref, int key) { struct Node* temp = *head_ref, *prev; if (temp != NULL && temp->data == key) { *head_ref = temp->next; free(temp); return; } while (temp != NULL && temp->data != key) { prev = temp; temp = temp->next; } if (temp == NULL) return; prev->next = temp->next; free(temp); }","title":"Any"},{"location":"ds/linked-list/dll.html","text":"Doubly LL \u00b6 struct Node { int data; struct Node* next; struct Node* prev; }; Advantages \u00b6 traversal -> both directions deletion is easy as prev pointer is easily available quickly insert Disadvantages \u00b6 more mem -> mem efficient is XOR XOR linked list: data npx = addr(next) ^ addr(prev) to trverse we need prev addr then prev XOR npx == next addr every manipulation -> more steps Insertion \u00b6 At the front of the DLL After a given node. At the end of the DLL Before a given node. void push(Node** head_ref, int new_data) { Node* new_node = new Node(); new_node->data = new_data; new_node->next = (*head_ref); new_node->prev = NULL; if ((*head_ref) != NULL) (*head_ref)->prev = new_node; (*head_ref) = new_node; } void insertAfter(Node* prev_node, int new_data) { if (prev_node == NULL) { cout<<\"the given previous node cannot be NULL\"; return; } Node* new_node = new Node(); new_node->data = new_data; new_node->next = prev_node->next; prev_node->next = new_node; new_node->prev = prev_node; if (new_node->next != NULL) new_node->next->prev = new_node; } void append(Node** head_ref, int new_data) { Node* new_node = new Node(); Node* last = *head_ref; new_node->data = new_data; new_node->next = NULL; if (*head_ref == NULL) { new_node->prev = NULL; *head_ref = new_node; return; } while (last->next != NULL) last = last->next; last->next = new_node; new_node->prev = last; return; } Deletion \u00b6 Time Complexity: O(1). Space Complexity: O(1). void deleteNode(Node** head_ref, Node* del) { if (*head_ref == NULL || del == NULL) return; if (*head_ref == del) *head_ref = del->next; if (del->next != NULL) del->next->prev = del->prev; if (del->prev != NULL) del->prev->next = del->next; free(del); return; } reverse \u00b6 void reverse(Node **head_ref) { Node *temp = NULL; Node *current = *head_ref; while (current != NULL) { temp = current->prev; current->prev = current->next; current->next = temp; current = current->prev; } if(temp != NULL ) *head_ref = temp->prev; } TODO Swapping","title":"Doubly LL"},{"location":"ds/linked-list/dll.html#doubly-ll","text":"struct Node { int data; struct Node* next; struct Node* prev; };","title":"Doubly LL"},{"location":"ds/linked-list/dll.html#advantages","text":"traversal -> both directions deletion is easy as prev pointer is easily available quickly insert","title":"Advantages"},{"location":"ds/linked-list/dll.html#disadvantages","text":"more mem -> mem efficient is XOR XOR linked list: data npx = addr(next) ^ addr(prev) to trverse we need prev addr then prev XOR npx == next addr every manipulation -> more steps","title":"Disadvantages"},{"location":"ds/linked-list/dll.html#insertion","text":"At the front of the DLL After a given node. At the end of the DLL Before a given node. void push(Node** head_ref, int new_data) { Node* new_node = new Node(); new_node->data = new_data; new_node->next = (*head_ref); new_node->prev = NULL; if ((*head_ref) != NULL) (*head_ref)->prev = new_node; (*head_ref) = new_node; } void insertAfter(Node* prev_node, int new_data) { if (prev_node == NULL) { cout<<\"the given previous node cannot be NULL\"; return; } Node* new_node = new Node(); new_node->data = new_data; new_node->next = prev_node->next; prev_node->next = new_node; new_node->prev = prev_node; if (new_node->next != NULL) new_node->next->prev = new_node; } void append(Node** head_ref, int new_data) { Node* new_node = new Node(); Node* last = *head_ref; new_node->data = new_data; new_node->next = NULL; if (*head_ref == NULL) { new_node->prev = NULL; *head_ref = new_node; return; } while (last->next != NULL) last = last->next; last->next = new_node; new_node->prev = last; return; }","title":"Insertion"},{"location":"ds/linked-list/dll.html#deletion","text":"Time Complexity: O(1). Space Complexity: O(1). void deleteNode(Node** head_ref, Node* del) { if (*head_ref == NULL || del == NULL) return; if (*head_ref == del) *head_ref = del->next; if (del->next != NULL) del->next->prev = del->prev; if (del->prev != NULL) del->prev->next = del->next; free(del); return; }","title":"Deletion"},{"location":"ds/linked-list/dll.html#reverse","text":"void reverse(Node **head_ref) { Node *temp = NULL; Node *current = *head_ref; while (current != NULL) { temp = current->prev; current->prev = current->next; current->next = temp; current = current->prev; } if(temp != NULL ) *head_ref = temp->prev; } TODO Swapping","title":"reverse"},{"location":"ds/linked-list/functions.html","text":"Functional Passing \u00b6 modify head global head multiple linked lists? unpredictable behaviour http://wiki.c2.com/?GlobalVariablesAreBad return head forgets about assigning it to head? double pointer","title":"Functional Passing"},{"location":"ds/linked-list/functions.html#functional-passing","text":"modify head global head multiple linked lists? unpredictable behaviour http://wiki.c2.com/?GlobalVariablesAreBad return head forgets about assigning it to head? double pointer","title":"Functional Passing"},{"location":"ds/linked-list/generic.html","text":"Generic Implementation in c \u00b6 Unlike C++ and Java, C doesn\u2019t support generics. Hence we use void pointer. struct Node { void *data; struct Node *next; }; void push(struct Node** head_ref, void *new_data, size_t data_size) { struct Node* new_node = (struct Node*)malloc(sizeof(struct Node)); new_node->data = malloc(data_size); new_node->next = (*head_ref); int i; for (i=0; i<data_size; i++) *(char *)(new_node->data + i) = *(char *)(new_data + i); (*head_ref) = new_node; } void printList(struct Node *node, void (*fptr)(void *)) { while (node != NULL) { (*fptr)(node->data); node = node->next; } }","title":"Generic Implementation in c"},{"location":"ds/linked-list/generic.html#generic-implementation-in-c","text":"Unlike C++ and Java, C doesn\u2019t support generics. Hence we use void pointer. struct Node { void *data; struct Node *next; }; void push(struct Node** head_ref, void *new_data, size_t data_size) { struct Node* new_node = (struct Node*)malloc(sizeof(struct Node)); new_node->data = malloc(data_size); new_node->next = (*head_ref); int i; for (i=0; i<data_size; i++) *(char *)(new_node->data + i) = *(char *)(new_data + i); (*head_ref) = new_node; } void printList(struct Node *node, void (*fptr)(void *)) { while (node != NULL) { (*fptr)(node->data); node = node->next; } }","title":"Generic Implementation in c"},{"location":"ds/linked-list/insertion.html","text":"Insertion \u00b6 does not modify head Front (O(1)) \u00b6 void push(Node** head_ref, int new_data) { Node* new_node = new Node(); new_node->data = new_data; new_node->next = (*head_ref); (*head_ref) = new_node; } After Node (O(1)) \u00b6 void insertAfter(Node* prev_node, int new_data) { if (prev_node == NULL) { cout << \"the given previous node cannot be NULL\"; return; } Node* new_node = new Node(); new_node->data = new_data; new_node->next = prev_node->next; prev_node->next = new_node; } Last (O(n)) \u00b6 void append(Node** head_ref, int new_data) { Node* new_node = new Node(); Node *last = *head_ref; new_node->data = new_data; new_node->next = NULL; if (*head_ref == NULL) { *head_ref = new_node; return; } while (last->next != NULL) last = last->next; last->next = new_node; return; }","title":"Insertion"},{"location":"ds/linked-list/insertion.html#insertion","text":"does not modify head","title":"Insertion"},{"location":"ds/linked-list/insertion.html#front-o1","text":"void push(Node** head_ref, int new_data) { Node* new_node = new Node(); new_node->data = new_data; new_node->next = (*head_ref); (*head_ref) = new_node; }","title":"Front (O(1))"},{"location":"ds/linked-list/insertion.html#after-node-o1","text":"void insertAfter(Node* prev_node, int new_data) { if (prev_node == NULL) { cout << \"the given previous node cannot be NULL\"; return; } Node* new_node = new Node(); new_node->data = new_data; new_node->next = prev_node->next; prev_node->next = new_node; }","title":"After Node (O(1))"},{"location":"ds/linked-list/insertion.html#last-on","text":"void append(Node** head_ref, int new_data) { Node* new_node = new Node(); Node *last = *head_ref; new_node->data = new_data; new_node->next = NULL; if (*head_ref == NULL) { *head_ref = new_node; return; } while (last->next != NULL) last = last->next; last->next = new_node; return; }","title":"Last (O(n))"},{"location":"ds/linked-list/intro.html","text":"Linked List \u00b6 Linear DS Non-contagious Pointers Why ? \u00b6 dynamic size inserting / deleting -> less expensive Drawbacks \u00b6 Random Access not allowed extra memory for pointer not cache friendly -> no locallity of refernce -> non contagious Representation \u00b6 Data + next pointer -> node Linked List -> Meta data + Start class Node { public: int data; Node* next; }; Traversal \u00b6 void printList(Node* n) { while (n != NULL) { cout << n->data << \" \"; n = n->next; } }","title":"Linked List"},{"location":"ds/linked-list/intro.html#linked-list","text":"Linear DS Non-contagious Pointers","title":"Linked List"},{"location":"ds/linked-list/intro.html#why","text":"dynamic size inserting / deleting -> less expensive","title":"Why ?"},{"location":"ds/linked-list/intro.html#drawbacks","text":"Random Access not allowed extra memory for pointer not cache friendly -> no locallity of refernce -> non contagious","title":"Drawbacks"},{"location":"ds/linked-list/intro.html#representation","text":"Data + next pointer -> node Linked List -> Meta data + Start class Node { public: int data; Node* next; };","title":"Representation"},{"location":"ds/linked-list/intro.html#traversal","text":"void printList(Node* n) { while (n != NULL) { cout << n->data << \" \"; n = n->next; } }","title":"Traversal"},{"location":"ds/linked-list/reverse.html","text":"Reverse LL \u00b6 Iterative \u00b6 Time O(n) Space O(1) void reverse() { Node* current = head; Node *prev = NULL, *next = NULL; while (current != NULL) { next = current->next; current->next = prev; prev = current; current = next; } head = prev; } Recursive \u00b6 Time O(n) Space O(1) Node* reverse(Node* head) { if (head == NULL || head->next == NULL) return head; Node* rest = reverse(head->next); head->next->next = head; head->next = NULL; return rest; } Tail Recursive method \u00b6 void reverse(Node** head) { if (!head) return; reverseUtil(*head, NULL, head); } void reverseUtil(Node* curr, Node* prev, Node** head) { if (!curr->next) { *head = curr; curr->next = prev; return; } Node* next = curr->next; curr->next = prev; reverseUtil(next, curr, head); } Stack \u00b6 void reverseLL(Node** head) { stack<Node*> s; Node* temp = *head; while (temp->next != NULL) { s.push(temp); temp = temp->next; } *head = temp; while (!s.empty()) { temp->next = s.top(); s.pop(); temp = temp->next; } temp->next = NULL; }","title":"Reverse LL"},{"location":"ds/linked-list/reverse.html#reverse-ll","text":"","title":"Reverse LL"},{"location":"ds/linked-list/reverse.html#iterative","text":"Time O(n) Space O(1) void reverse() { Node* current = head; Node *prev = NULL, *next = NULL; while (current != NULL) { next = current->next; current->next = prev; prev = current; current = next; } head = prev; }","title":"Iterative"},{"location":"ds/linked-list/reverse.html#recursive","text":"Time O(n) Space O(1) Node* reverse(Node* head) { if (head == NULL || head->next == NULL) return head; Node* rest = reverse(head->next); head->next->next = head; head->next = NULL; return rest; }","title":"Recursive"},{"location":"ds/linked-list/reverse.html#tail-recursive-method","text":"void reverse(Node** head) { if (!head) return; reverseUtil(*head, NULL, head); } void reverseUtil(Node* curr, Node* prev, Node** head) { if (!curr->next) { *head = curr; curr->next = prev; return; } Node* next = curr->next; curr->next = prev; reverseUtil(next, curr, head); }","title":"Tail Recursive method"},{"location":"ds/linked-list/reverse.html#stack","text":"void reverseLL(Node** head) { stack<Node*> s; Node* temp = *head; while (temp->next != NULL) { s.push(temp); temp = temp->next; } *head = temp; while (!s.empty()) { temp->next = s.top(); s.pop(); temp = temp->next; } temp->next = NULL; }","title":"Stack"},{"location":"ds/linked-list/search.html","text":"Search \u00b6 LinkedListNode* search(LinkedListNode* head, int val){ LinkedListNode* temp = head; while(temp!=NULL){ if(temp->value==val){ cout << \"Found it\"; return temp; } temp = temp->next; } cout << \"Not Found\"; return NULL; }","title":"Search"},{"location":"ds/linked-list/search.html#search","text":"LinkedListNode* search(LinkedListNode* head, int val){ LinkedListNode* temp = head; while(temp!=NULL){ if(temp->value==val){ cout << \"Found it\"; return temp; } temp = temp->next; } cout << \"Not Found\"; return NULL; }","title":"Search"},{"location":"ds/linked-list/swap.html","text":"Swapping \u00b6 Possibilities x and y may or may not be adjacent. Either x or y may be a head node. Either x or y may be the last node. x and/or y may not be present in the linked list. void swap(Node*& a, Node*& b) { Node* temp = a; a = b; b = temp; } void swapNodes(Node** head_ref, int x, int y) { if (x == y) return; Node **a = NULL, **b = NULL; while (*head_ref) { if ((*head_ref)->data == x) { a = head_ref; } else if ((*head_ref)->data == y) { b = head_ref; } head_ref = &((*head_ref)->next); } if (a && b) { swap(*a, *b); swap(((*a)->next), ((*b)->next)); } }","title":"Swapping"},{"location":"ds/linked-list/swap.html#swapping","text":"Possibilities x and y may or may not be adjacent. Either x or y may be a head node. Either x or y may be the last node. x and/or y may not be present in the linked list. void swap(Node*& a, Node*& b) { Node* temp = a; a = b; b = temp; } void swapNodes(Node** head_ref, int x, int y) { if (x == y) return; Node **a = NULL, **b = NULL; while (*head_ref) { if ((*head_ref)->data == x) { a = head_ref; } else if ((*head_ref)->data == y) { b = head_ref; } head_ref = &((*head_ref)->next); } if (a && b) { swap(*a, *b); swap(((*a)->next), ((*b)->next)); } }","title":"Swapping"},{"location":"ds/linked-list/vs-array.html","text":"Linked list vs Arrays \u00b6 Array \u00b6 contagious linear colection of similar datatype indexes -> direct retrival (O(1)) mem aloc -> compile time FROM Data section (e.g. global array) or Stack section (e.g. local array). static wastage of memory For dynamic alloc(FROM heap) random access of array + runtime alloc of linked list int * dynArr = (int *)malloc(sizeof(int)*arrSize); __advantage__ -> reduce code-size!! (but other factors e.g. program format etc.) Assuming we aren't allowed to get mem. from heap (eg. embedded systems) due to performance, malloc is costly we have to do module specific memory management. (not system provided API's) How to do it? struct sllNode { int dataInt; int nextIndex; }; struct sllNode arrayLL[5]; //__this__ 0x500 -> 0x508 -> 0x510 -> 0x518. [(1),1] [(2),2] [(3),3] [(4),-2] [(0),-1] 0x500 0x508 0x510 0x518 0x520 -2 -> end of linked list delete 2nd node 0x500 -> 0x510 -> 0x518 [(1),2] [(0),-1] [(3),3] [(4),-2] [(0),-1] 0x500 0x508 0x510 0x518 0x520 2nd node's mem still there inserting \u00b6 0x500 -> 0x508 -> 0x518 -> 0x520 [(1),1] [(2),3] [(0),-1] [(4),4] [(5),-2] 0x500 0x508 0x510 0x518 0x520 insert a new node with data 8 [(1),1] [(2),3] [(8),0] [(4),4] [(5),-2] 0x500 0x508 0x510 0x518 0x520 0x510 -> 0x500 -> 0x508 -> 0x518 -> 0x520 How to optimise \u00b6 Maintain to linked lists (with data and other that is empty) Linked List \u00b6 non-primitive , unordered linked elements (Nodes) traversal from head (O(n)) mem aloc -> runtime FROM Heap section (e.g. using malloc() etc.) dynamic","title":"Linked list vs Arrays"},{"location":"ds/linked-list/vs-array.html#linked-list-vs-arrays","text":"","title":"Linked list vs Arrays"},{"location":"ds/linked-list/vs-array.html#array","text":"contagious linear colection of similar datatype indexes -> direct retrival (O(1)) mem aloc -> compile time FROM Data section (e.g. global array) or Stack section (e.g. local array). static wastage of memory For dynamic alloc(FROM heap) random access of array + runtime alloc of linked list int * dynArr = (int *)malloc(sizeof(int)*arrSize); __advantage__ -> reduce code-size!! (but other factors e.g. program format etc.) Assuming we aren't allowed to get mem. from heap (eg. embedded systems) due to performance, malloc is costly we have to do module specific memory management. (not system provided API's) How to do it? struct sllNode { int dataInt; int nextIndex; }; struct sllNode arrayLL[5]; //__this__ 0x500 -> 0x508 -> 0x510 -> 0x518. [(1),1] [(2),2] [(3),3] [(4),-2] [(0),-1] 0x500 0x508 0x510 0x518 0x520 -2 -> end of linked list delete 2nd node 0x500 -> 0x510 -> 0x518 [(1),2] [(0),-1] [(3),3] [(4),-2] [(0),-1] 0x500 0x508 0x510 0x518 0x520 2nd node's mem still there","title":"Array"},{"location":"ds/linked-list/vs-array.html#inserting","text":"0x500 -> 0x508 -> 0x518 -> 0x520 [(1),1] [(2),3] [(0),-1] [(4),4] [(5),-2] 0x500 0x508 0x510 0x518 0x520 insert a new node with data 8 [(1),1] [(2),3] [(8),0] [(4),4] [(5),-2] 0x500 0x508 0x510 0x518 0x520 0x510 -> 0x500 -> 0x508 -> 0x518 -> 0x520","title":"inserting"},{"location":"ds/linked-list/vs-array.html#how-to-optimise","text":"Maintain to linked lists (with data and other that is empty)","title":"How to optimise"},{"location":"ds/linked-list/vs-array.html#linked-list","text":"non-primitive , unordered linked elements (Nodes) traversal from head (O(n)) mem aloc -> runtime FROM Heap section (e.g. using malloc() etc.) dynamic","title":"Linked List"},{"location":"ds/linked-list/xor-dll.html","text":"XOR DLL \u00b6 Representation \u00b6 class Node { public: int data; Node* npx; /* XOR of next and previous node */ }; XOR Utility \u00b6 Node* XOR (Node *a, Node *b) { return reinterpret_cast<Node *>( reinterpret_cast<uintptr_t>(a) ^ reinterpret_cast<uintptr_t>(b)); } Insert \u00b6 void insert(Node **head_ref, int data) { Node *new_node = new Node(); new_node->data = data; new_node->npx = *head_ref; if (*head_ref != NULL) { (*head_ref)->npx = XOR(new_node, (*head_ref)->npx); } *head_ref = new_node; } Print \u00b6 void printList (Node *head) { Node *curr = head; Node *prev = NULL; Node *next; cout << \"Following are the nodes of Linked List: \\n\"; while (curr != NULL) { cout<<curr->data<<\" \"; next = XOR (prev, curr->npx); prev = curr; curr = next; } }","title":"XOR DLL"},{"location":"ds/linked-list/xor-dll.html#xor-dll","text":"","title":"XOR DLL"},{"location":"ds/linked-list/xor-dll.html#representation","text":"class Node { public: int data; Node* npx; /* XOR of next and previous node */ };","title":"Representation"},{"location":"ds/linked-list/xor-dll.html#xor-utility","text":"Node* XOR (Node *a, Node *b) { return reinterpret_cast<Node *>( reinterpret_cast<uintptr_t>(a) ^ reinterpret_cast<uintptr_t>(b)); }","title":"XOR Utility"},{"location":"ds/linked-list/xor-dll.html#insert","text":"void insert(Node **head_ref, int data) { Node *new_node = new Node(); new_node->data = data; new_node->npx = *head_ref; if (*head_ref != NULL) { (*head_ref)->npx = XOR(new_node, (*head_ref)->npx); } *head_ref = new_node; }","title":"Insert"},{"location":"ds/linked-list/xor-dll.html#print","text":"void printList (Node *head) { Node *curr = head; Node *prev = NULL; Node *next; cout << \"Following are the nodes of Linked List: \\n\"; while (curr != NULL) { cout<<curr->data<<\" \"; next = XOR (prev, curr->npx); prev = curr; curr = next; } }","title":"Print"},{"location":"ds/queue/dequeue.html","text":"Dequeue \u00b6 generalised version of queue -> insert and delete at both ends Operations \u00b6 insertFront deleteFront insertRear deleteRear isFull isEmpty getFront getRear Application \u00b6 used as stack and queues. clockwise and anticlockwise rotations in O(1) time Also, the problems where elements need to be removed and or added both ends can be efficiently solved using Deque. For example see Maximum of all subarrays of size k problem., 0-1 BFS and Find the first circular tour that visits all petrol pumps. A-Steal job scheduling algorithm where Deque is used as deletions operation is required at both ends. Implementation \u00b6 all ops -> O(1) doubly linked list \u00b6 TODO circular array. \u00b6 #define MAX 100 class Deque { int arr[MAX]; int front; int rear; int size; public : Deque(int size) { front = -1; rear = 0; this->size = size; } void insertfront(int key); void insertrear(int key); void deletefront(); void deleterear(); bool isFull(); bool isEmpty(); int getFront(); int getRear(); }; bool Deque::isFull() { return ((front == 0 && rear == size-1)|| front == rear+1); } bool Deque::isEmpty () { return (front == -1); } void Deque::insertfront(int key) { if (isFull()) { cout << \"Overflow\\n\" << endl; return; } if (front == -1) { front = 0; rear = 0; } else if (front == 0) front = size - 1 ; else front = front-1; arr[front] = key ; } void Deque ::insertrear(int key) { if (isFull()) { cout << \" Overflow\\n \" << endl; return; } if (front == -1) { front = 0; rear = 0; } else if (rear == size-1) rear = 0; else rear = rear+1; arr[rear] = key ; } void Deque ::deletefront() { if (isEmpty()) { cout << \"Queue Underflow\\n\" << endl; return ; } if (front == rear) { front = -1; rear = -1; } else if (front == size -1) front = 0; else front = front+1; } void Deque::deleterear() { if (isEmpty()) { cout << \" Underflow\\n\" << endl ; return ; } if (front == rear) { front = -1; rear = -1; } else if (rear == 0) rear = size-1; else rear = rear-1; } int Deque::getFront() { if (isEmpty()) { cout << \" Underflow\\n\" << endl; return -1 ; } return arr[front]; } int Deque::getRear() { if(isEmpty() || rear < 0) { cout << \" Underflow\\n\" << endl; return -1 ; } return arr[rear]; }","title":"Dequeue"},{"location":"ds/queue/dequeue.html#dequeue","text":"generalised version of queue -> insert and delete at both ends","title":"Dequeue"},{"location":"ds/queue/dequeue.html#operations","text":"insertFront deleteFront insertRear deleteRear isFull isEmpty getFront getRear","title":"Operations"},{"location":"ds/queue/dequeue.html#application","text":"used as stack and queues. clockwise and anticlockwise rotations in O(1) time Also, the problems where elements need to be removed and or added both ends can be efficiently solved using Deque. For example see Maximum of all subarrays of size k problem., 0-1 BFS and Find the first circular tour that visits all petrol pumps. A-Steal job scheduling algorithm where Deque is used as deletions operation is required at both ends.","title":"Application"},{"location":"ds/queue/dequeue.html#implementation","text":"all ops -> O(1)","title":"Implementation"},{"location":"ds/queue/dequeue.html#doubly-linked-list","text":"TODO","title":"doubly linked list"},{"location":"ds/queue/dequeue.html#circular-array","text":"#define MAX 100 class Deque { int arr[MAX]; int front; int rear; int size; public : Deque(int size) { front = -1; rear = 0; this->size = size; } void insertfront(int key); void insertrear(int key); void deletefront(); void deleterear(); bool isFull(); bool isEmpty(); int getFront(); int getRear(); }; bool Deque::isFull() { return ((front == 0 && rear == size-1)|| front == rear+1); } bool Deque::isEmpty () { return (front == -1); } void Deque::insertfront(int key) { if (isFull()) { cout << \"Overflow\\n\" << endl; return; } if (front == -1) { front = 0; rear = 0; } else if (front == 0) front = size - 1 ; else front = front-1; arr[front] = key ; } void Deque ::insertrear(int key) { if (isFull()) { cout << \" Overflow\\n \" << endl; return; } if (front == -1) { front = 0; rear = 0; } else if (rear == size-1) rear = 0; else rear = rear+1; arr[rear] = key ; } void Deque ::deletefront() { if (isEmpty()) { cout << \"Queue Underflow\\n\" << endl; return ; } if (front == rear) { front = -1; rear = -1; } else if (front == size -1) front = 0; else front = front+1; } void Deque::deleterear() { if (isEmpty()) { cout << \" Underflow\\n\" << endl ; return ; } if (front == rear) { front = -1; rear = -1; } else if (rear == 0) rear = size-1; else rear = rear-1; } int Deque::getFront() { if (isEmpty()) { cout << \" Underflow\\n\" << endl; return -1 ; } return arr[front]; } int Deque::getRear() { if(isEmpty() || rear < 0) { cout << \" Underflow\\n\" << endl; return -1 ; } return arr[rear]; }","title":"circular array."},{"location":"ds/queue/intro.html","text":"Queue \u00b6 FIFO Operations \u00b6 Enqueue: Adds or overflow. Dequeue: Removes or underflow. Front Rear Application \u00b6 Used when elements does not have to be processed immediately When a resource is shared among multiple consumers. Examples include CPU scheduling, Disk Scheduling. When data is transferred asynchronously (data not necessarily received at same rate as sent) between two processes. Examples include IO Buffers, pipes, file IO, etc. Implementation \u00b6 Array \u00b6 Complexity Analysis: Time Complexity: Operations Complexity Enque(insertion) O(1) Deque(deletion) O(1) Front(Get front) O(1) Rear(Get Rear) O(1) Auxiliary Space: O(N). N is the size of array for storing elements. Pros of Array Implementation: Easy to implement. Cons of Array Implementation: Static Data Structure, fixed size. If the queue has a large number of enqueue and dequeue operations, at some point we may not we able to insert elements in the queue even if the queue is empty (this problem is avoided by using circular queue). class Queue { public: int front, rear, size; unsigned capacity; int* array; }; Queue* createQueue(unsigned capacity) { Queue* queue = new Queue(); queue->capacity = capacity; queue->front = queue->size = 0; queue->rear = capacity - 1; queue->array = new int[( queue->capacity * sizeof(int))]; return queue; } int isFull(Queue* queue) { return (queue->size == queue->capacity); } int isEmpty(Queue* queue) { return (queue->size == 0); } void enqueue(Queue* queue, int item) { if (isFull(queue)) return; queue->rear = (queue->rear + 1) % queue->capacity; queue->array[queue->rear] = item; queue->size = queue->size + 1; cout << item << \" enqueued to queue\\n\"; } int dequeue(Queue* queue) { if (isEmpty(queue)) return INT_MIN; int item = queue->array[queue->front]; queue->front = (queue->front + 1) % queue->capacity; queue->size = queue->size - 1; return item; } int front(Queue* queue) { if (isEmpty(queue)) return INT_MIN; return queue->array[queue->front]; } int rear(Queue* queue) { if (isEmpty(queue)) return INT_MIN; return queue->array[queue->rear]; } Linked List \u00b6 Time Complexity: Time complexity of both operations enqueue() and dequeue() is O(1) as we only change few pointers in both operations. There is no loop in any of the operations. struct QNode { int data; QNode* next; QNode(int d) { data = d; next = NULL; } }; struct Queue { QNode *front, *rear; Queue() { front = rear = NULL; } void enQueue(int x) { QNode* temp = new QNode(x); if (rear == NULL) { front = rear = temp; return; } rear->next = temp; rear = temp; } void deQueue() { if (front == NULL) return; QNode* temp = front; front = front->next; if (front == NULL) rear = NULL; delete (temp); } };","title":"Queue"},{"location":"ds/queue/intro.html#queue","text":"FIFO","title":"Queue"},{"location":"ds/queue/intro.html#operations","text":"Enqueue: Adds or overflow. Dequeue: Removes or underflow. Front Rear","title":"Operations"},{"location":"ds/queue/intro.html#application","text":"Used when elements does not have to be processed immediately When a resource is shared among multiple consumers. Examples include CPU scheduling, Disk Scheduling. When data is transferred asynchronously (data not necessarily received at same rate as sent) between two processes. Examples include IO Buffers, pipes, file IO, etc.","title":"Application"},{"location":"ds/queue/intro.html#implementation","text":"","title":"Implementation"},{"location":"ds/queue/intro.html#array","text":"Complexity Analysis: Time Complexity: Operations Complexity Enque(insertion) O(1) Deque(deletion) O(1) Front(Get front) O(1) Rear(Get Rear) O(1) Auxiliary Space: O(N). N is the size of array for storing elements. Pros of Array Implementation: Easy to implement. Cons of Array Implementation: Static Data Structure, fixed size. If the queue has a large number of enqueue and dequeue operations, at some point we may not we able to insert elements in the queue even if the queue is empty (this problem is avoided by using circular queue). class Queue { public: int front, rear, size; unsigned capacity; int* array; }; Queue* createQueue(unsigned capacity) { Queue* queue = new Queue(); queue->capacity = capacity; queue->front = queue->size = 0; queue->rear = capacity - 1; queue->array = new int[( queue->capacity * sizeof(int))]; return queue; } int isFull(Queue* queue) { return (queue->size == queue->capacity); } int isEmpty(Queue* queue) { return (queue->size == 0); } void enqueue(Queue* queue, int item) { if (isFull(queue)) return; queue->rear = (queue->rear + 1) % queue->capacity; queue->array[queue->rear] = item; queue->size = queue->size + 1; cout << item << \" enqueued to queue\\n\"; } int dequeue(Queue* queue) { if (isEmpty(queue)) return INT_MIN; int item = queue->array[queue->front]; queue->front = (queue->front + 1) % queue->capacity; queue->size = queue->size - 1; return item; } int front(Queue* queue) { if (isEmpty(queue)) return INT_MIN; return queue->array[queue->front]; } int rear(Queue* queue) { if (isEmpty(queue)) return INT_MIN; return queue->array[queue->rear]; }","title":"Array"},{"location":"ds/queue/intro.html#linked-list","text":"Time Complexity: Time complexity of both operations enqueue() and dequeue() is O(1) as we only change few pointers in both operations. There is no loop in any of the operations. struct QNode { int data; QNode* next; QNode(int d) { data = d; next = NULL; } }; struct Queue { QNode *front, *rear; Queue() { front = rear = NULL; } void enQueue(int x) { QNode* temp = new QNode(x); if (rear == NULL) { front = rear = temp; return; } rear->next = temp; rear = temp; } void deQueue() { if (front == NULL) return; QNode* temp = front; front = front->next; if (front == NULL) rear = NULL; delete (temp); } };","title":"Linked List"},{"location":"ds/queue/print-binary.html","text":"Print Binary \u00b6 simple \u00b6 loop from 1 to n.. then call toBin() Using Queues \u00b6 O(n * logn) void generatePrintBinary(int n) { queue<string> q; q.push(\"1\"); // BFS while (n--) { string s1 = q.front(); q.pop(); cout << s1 << \"\\n\"; string s2 = s1; q.push(s1.append(\"0\")); q.push(s2.append(\"1\")); } }","title":"Print Binary"},{"location":"ds/queue/print-binary.html#print-binary","text":"","title":"Print Binary"},{"location":"ds/queue/print-binary.html#simple","text":"loop from 1 to n.. then call toBin()","title":"simple"},{"location":"ds/queue/print-binary.html#using-queues","text":"O(n * logn) void generatePrintBinary(int n) { queue<string> q; q.push(\"1\"); // BFS while (n--) { string s1 = q.front(); q.pop(); cout << s1 << \"\\n\"; string s2 = s1; q.push(s1.append(\"0\")); q.push(s2.append(\"1\")); } }","title":"Using Queues"},{"location":"ds/queue/priority-queue.html","text":"Priority Queue \u00b6 Queue with properties: \u00b6 every element has apriority dequeue removes element with highest priority 2 elements with same priority... deletes in the order of queueing Operations \u00b6 insert(item, priority) getHighestPriority(): deleteHighestPriority() Implementation \u00b6 struct item { int item; int priority; } Arrays \u00b6 insert -> O(1) getHighest -> linear traversal (O(n)) deleteHighestPriority -> need to move elements O(n) Linked List \u00b6 delete is efficient as no need to move Heap \u00b6 best insert, delete -> O(logn) highest -> O(1) Applications \u00b6 CPU Scheduling Graph algorithms like Dijkstra\u2019s shortest path algorithm, Prim\u2019s Minimum Spanning Tree, etc All queue applications where priority is involved.","title":"Priority Queue"},{"location":"ds/queue/priority-queue.html#priority-queue","text":"","title":"Priority Queue"},{"location":"ds/queue/priority-queue.html#queue-with-properties","text":"every element has apriority dequeue removes element with highest priority 2 elements with same priority... deletes in the order of queueing","title":"Queue with properties:"},{"location":"ds/queue/priority-queue.html#operations","text":"insert(item, priority) getHighestPriority(): deleteHighestPriority()","title":"Operations"},{"location":"ds/queue/priority-queue.html#implementation","text":"struct item { int item; int priority; }","title":"Implementation"},{"location":"ds/queue/priority-queue.html#arrays","text":"insert -> O(1) getHighest -> linear traversal (O(n)) deleteHighestPriority -> need to move elements O(n)","title":"Arrays"},{"location":"ds/queue/priority-queue.html#linked-list","text":"delete is efficient as no need to move","title":"Linked List"},{"location":"ds/queue/priority-queue.html#heap","text":"best insert, delete -> O(logn) highest -> O(1)","title":"Heap"},{"location":"ds/queue/priority-queue.html#applications","text":"CPU Scheduling Graph algorithms like Dijkstra\u2019s shortest path algorithm, Prim\u2019s Minimum Spanning Tree, etc All queue applications where priority is involved.","title":"Applications"},{"location":"ds/queue/using-stack.html","text":"Queue using Stack \u00b6 Costly Enqueue \u00b6 Push O(N): In the worst case we have empty whole of stack 1 into stack 2. Pop operation: O(1). Auxiliary Space: O(N). struct Queue { stack<int> s1, s2; void enQueue(int x) { while (!s1.empty()) { s2.push(s1.top()); s1.pop(); } s1.push(x); while (!s2.empty()) { s1.push(s2.top()); s2.pop(); } } int deQueue() { if (s1.empty()) { cout << \"Q is Empty\"; exit(0); } int x = s1.top(); s1.pop(); return x; } }; Costly Dequeue \u00b6 Push operation: O(1). Pop operation: O(N): In the worst case we have empty whole of stack 1 into stack 2 Auxiliary Space: O(N). Method 2 is definitely better than method 1. Method 1 moves all the elements twice in enQueue operation, while method 2 (in deQueue operation) moves the elements once and moves elements only if stack2 empty. So, the amortized complexity of the dequeue operation becomes \\Theta (1) . struct Queue { stack<int> s1, s2; void enQueue(int x) { s1.push(x); } int deQueue() { if (s1.empty() && s2.empty()) { cout << \"Q is empty\"; exit(0); } if (s2.empty()) { while (!s1.empty()) { s2.push(s1.top()); s1.pop(); } } int x = s2.top(); s2.pop(); return x; } }; Using one user stack and function call stack (recurion) \u00b6 Push operation : O(1). Pop operation : O(N). The difference from above method is that in this method element is returned and all elements are restored back in a single call. Auxiliary Space: O(N). struct Queue { stack<int> s; void enQueue(int x) { s.push(x); } int deQueue() { if (s.empty()) { cout << \"Q is empty\"; exit(0); } int x = s.top(); s.pop(); if (s.empty()) return x; int item = deQueue(); s.push(x); return item; } };","title":"Queue using Stack"},{"location":"ds/queue/using-stack.html#queue-using-stack","text":"","title":"Queue using Stack"},{"location":"ds/queue/using-stack.html#costly-enqueue","text":"Push O(N): In the worst case we have empty whole of stack 1 into stack 2. Pop operation: O(1). Auxiliary Space: O(N). struct Queue { stack<int> s1, s2; void enQueue(int x) { while (!s1.empty()) { s2.push(s1.top()); s1.pop(); } s1.push(x); while (!s2.empty()) { s1.push(s2.top()); s2.pop(); } } int deQueue() { if (s1.empty()) { cout << \"Q is Empty\"; exit(0); } int x = s1.top(); s1.pop(); return x; } };","title":"Costly Enqueue"},{"location":"ds/queue/using-stack.html#costly-dequeue","text":"Push operation: O(1). Pop operation: O(N): In the worst case we have empty whole of stack 1 into stack 2 Auxiliary Space: O(N). Method 2 is definitely better than method 1. Method 1 moves all the elements twice in enQueue operation, while method 2 (in deQueue operation) moves the elements once and moves elements only if stack2 empty. So, the amortized complexity of the dequeue operation becomes \\Theta (1) . struct Queue { stack<int> s1, s2; void enQueue(int x) { s1.push(x); } int deQueue() { if (s1.empty() && s2.empty()) { cout << \"Q is empty\"; exit(0); } if (s2.empty()) { while (!s1.empty()) { s2.push(s1.top()); s1.pop(); } } int x = s2.top(); s2.pop(); return x; } };","title":"Costly Dequeue"},{"location":"ds/queue/using-stack.html#using-one-user-stack-and-function-call-stack-recurion","text":"Push operation : O(1). Pop operation : O(N). The difference from above method is that in this method element is returned and all elements are restored back in a single call. Auxiliary Space: O(N). struct Queue { stack<int> s; void enQueue(int x) { s.push(x); } int deQueue() { if (s.empty()) { cout << \"Q is empty\"; exit(0); } int x = s.top(); s.pop(); if (s.empty()) return x; int item = deQueue(); s.push(x); return item; } };","title":"Using one user stack and function call stack (recurion)"},{"location":"ds/set-map/map.html","text":"Maps \u00b6 key-value pairs can't have duplicate keys pair insert(keyvalue, mapvalue): Adds a new element to the map MultiMap \u00b6 can have same keys. key-value not unique sorted order pair<int,int> insert(keyvalue,multimapvalue) \u2013 Adds a new element to the multimap multimap key_comp(): Returns the object that determines how the elements in the multimap are ordered (\u2018<\u2018 by default).","title":"Maps"},{"location":"ds/set-map/map.html#maps","text":"key-value pairs can't have duplicate keys pair insert(keyvalue, mapvalue): Adds a new element to the map","title":"Maps"},{"location":"ds/set-map/map.html#multimap","text":"can have same keys. key-value not unique sorted order pair<int,int> insert(keyvalue,multimapvalue) \u2013 Adds a new element to the multimap multimap key_comp(): Returns the object that determines how the elements in the multimap are ordered (\u2018<\u2018 by default).","title":"MultiMap"},{"location":"ds/set-map/multiset.html","text":"MultiSet \u00b6 non-unique values Functions \u00b6 In adition to set's operations it has: a.erase(): Remove all instance of element from multiset having same value a.erase(a.find()): Remove only one instance of element from multiset having same value erase(const g): Removes the value \u2018g\u2019 from the multiset. find(const g): Returns an iterator to the element \u2018g\u2019 in the multiset if found, else returns the iterator to end. count(const g): Returns the number of matches to element \u2018g\u2019 in the multiset. swap(): This function is used to exchange the contents of two multisets but the sets must be of same type, although sizes may differ.","title":"MultiSet"},{"location":"ds/set-map/multiset.html#multiset","text":"non-unique values","title":"MultiSet"},{"location":"ds/set-map/multiset.html#functions","text":"In adition to set's operations it has: a.erase(): Remove all instance of element from multiset having same value a.erase(a.find()): Remove only one instance of element from multiset having same value erase(const g): Removes the value \u2018g\u2019 from the multiset. find(const g): Returns an iterator to the element \u2018g\u2019 in the multiset if found, else returns the iterator to end. count(const g): Returns the number of matches to element \u2018g\u2019 in the multiset. swap(): This function is used to exchange the contents of two multisets but the sets must be of same type, although sizes may differ.","title":"Functions"},{"location":"ds/set-map/set-types.html","text":"Set Types \u00b6 set multiset unordered_set unordered_multiset sorted order sorted any order any order unique vallues non-unique unique non-unique start-end erase start-end erase only 1 erase only 1 erase BST BST hash-table hash-table Conclusion \u00b6 set is a container that stores sorted and unique elements. If unordered is added means elements are not sorted. If multiset is added means duplicate elements storage is allowed.","title":"Set Types"},{"location":"ds/set-map/set-types.html#set-types","text":"set multiset unordered_set unordered_multiset sorted order sorted any order any order unique vallues non-unique unique non-unique start-end erase start-end erase only 1 erase only 1 erase BST BST hash-table hash-table","title":"Set Types"},{"location":"ds/set-map/set-types.html#conclusion","text":"set is a container that stores sorted and unique elements. If unordered is added means elements are not sorted. If multiset is added means duplicate elements storage is allowed.","title":"Conclusion"},{"location":"ds/set-map/set.html","text":"Set \u00b6 unique elements immutable though can be deleted and added Functions \u00b6 r -> reverse c -> constant begin() end() size() max_size() empty() rbegin() rend(): Returns a reverse iterator pointing to the theoretical element right before the first element in the set container. crbegin() crend(): Returns a constant iterator pointing to the position just before the first element in the container. cbegin() cend() Manipulation \u00b6 insert(const g) iterator insert (iterator position, const g) \u2013 \u2018g\u2019 at the position pointed by iterator. erase(iterator position) \u2013 Removes the element at the position pointed by the iterator. erase(const g)\u2013 Removes the value \u2018g\u2019 from the set. clear() \u2013 Removes all the elements from the set. key_comp() / value_comp() \u2013 Returns the object that determines how the elements in the set are ordered (\u2018<\u2018 by default). find(const g) \u2013 Returns an iterator to the element \u2018g\u2019 in the set if found, else returns the iterator to end. count(const g) \u2013 Returns 1 or 0 based on the element \u2018g\u2019 is present in the set or not. lower_bound(const g) \u2013 Returns an iterator to the first element that is equivalent to \u2018g\u2019 or definitely will not go before the element \u2018g\u2019 in the set. upper_bound(const g) \u2013 Returns an iterator to the first element that will go after the element \u2018g\u2019 in the set. equal_range()\u2013 The function returns an iterator of pairs. (key_comp). The pair refers to the range that includes all the elements in the container which have a key equivalent to k. emplace()\u2013 This function is used to insert a new element into the set container, only if the element to be inserted is unique and does not already exists in the set. emplace_hint()\u2013 Returns an iterator pointing to the position where the insertion is done. If the element passed in the parameter already exists, then it returns an iterator pointing to the position where the existing element is. swap()\u2013 This function is used to exchange the contents of two sets but the sets must be of same type, although sizes may differ. operator= \u2013 The \u2018=\u2019 is an operator in C++ STL which copies (or moves) a set to another set and set::operator= is the corresponding operator function. get_allocator()\u2013 Returns the copy of the allocator object associated with the set.","title":"Set"},{"location":"ds/set-map/set.html#set","text":"unique elements immutable though can be deleted and added","title":"Set"},{"location":"ds/set-map/set.html#functions","text":"r -> reverse c -> constant begin() end() size() max_size() empty() rbegin() rend(): Returns a reverse iterator pointing to the theoretical element right before the first element in the set container. crbegin() crend(): Returns a constant iterator pointing to the position just before the first element in the container. cbegin() cend()","title":"Functions"},{"location":"ds/set-map/set.html#manipulation","text":"insert(const g) iterator insert (iterator position, const g) \u2013 \u2018g\u2019 at the position pointed by iterator. erase(iterator position) \u2013 Removes the element at the position pointed by the iterator. erase(const g)\u2013 Removes the value \u2018g\u2019 from the set. clear() \u2013 Removes all the elements from the set. key_comp() / value_comp() \u2013 Returns the object that determines how the elements in the set are ordered (\u2018<\u2018 by default). find(const g) \u2013 Returns an iterator to the element \u2018g\u2019 in the set if found, else returns the iterator to end. count(const g) \u2013 Returns 1 or 0 based on the element \u2018g\u2019 is present in the set or not. lower_bound(const g) \u2013 Returns an iterator to the first element that is equivalent to \u2018g\u2019 or definitely will not go before the element \u2018g\u2019 in the set. upper_bound(const g) \u2013 Returns an iterator to the first element that will go after the element \u2018g\u2019 in the set. equal_range()\u2013 The function returns an iterator of pairs. (key_comp). The pair refers to the range that includes all the elements in the container which have a key equivalent to k. emplace()\u2013 This function is used to insert a new element into the set container, only if the element to be inserted is unique and does not already exists in the set. emplace_hint()\u2013 Returns an iterator pointing to the position where the insertion is done. If the element passed in the parameter already exists, then it returns an iterator pointing to the position where the existing element is. swap()\u2013 This function is used to exchange the contents of two sets but the sets must be of same type, although sizes may differ. operator= \u2013 The \u2018=\u2019 is an operator in C++ STL which copies (or moves) a set to another set and set::operator= is the corresponding operator function. get_allocator()\u2013 Returns the copy of the allocator object associated with the set.","title":"Manipulation"},{"location":"ds/stack/intro.html","text":"Stack \u00b6 push(), pop(), isEmpty() and peek() all take O(1) follow LIFO/FILO order. real life -> stack of plates Applications of stack: \u00b6 Balancing of symbols Infix to Postfix / Prefix conversion Redo-undo Forward and backward feature in web browsers Used in many algorithms like Tower of Hanoi, tree traversals, stock span problem, histogram problem. Other applications can be Backtracking, Knight tour problem, rat in a maze, N queen problem and sudoku solver In Graph Algorithms like Topological Sorting and Strongly Connected Components Implementation \u00b6 Array \u00b6 Pros: Easy to implement. Memory is saved as pointers are not involved. Cons: It is not dynamic. It doesn\u2019t grow and shrink depending on needs at runtime. #define MAX 1000 class Stack { int top; public: int a[MAX]; // Maximum size of Stack Stack() { top = -1; } bool push(int x); int pop(); int peek(); bool isEmpty(); }; bool Stack::push(int x) { if (top >= (MAX - 1)) { cout << \"Stack Overflow\"; return false; } else { a[++top] = x; cout << x << \" pushed into stack\\n\"; return true; } } int Stack::pop() { if (top < 0) { cout << \"Stack Underflow\"; return 0; } else { int x = a[top--]; return x; } } int Stack::peek() { if (top < 0) { cout << \"Stack is Empty\"; return 0; } else { int x = a[top]; return x; } } bool Stack::isEmpty() { return (top < 0); } int main() { class Stack s; s.push(10); s.push(20); s.push(30); cout << s.pop() << \" Popped from stack\\n\"; return 0; } Linked List \u00b6 Pros: The linked list implementation of stack can grow and shrink according to the needs at runtime. Cons: Requires extra memory due to involvement of pointers. class StackNode { public: int data; StackNode* next; }; StackNode* newNode(int data) { StackNode* stackNode = new StackNode(); stackNode->data = data; stackNode->next = NULL; return stackNode; } int isEmpty(StackNode* root) { return !root; } void push(StackNode** root, int data) { StackNode* stackNode = newNode(data); stackNode->next = *root; *root = stackNode; cout << data << \" pushed to stack\\n\"; } int pop(StackNode** root) { if (isEmpty(*root)) return INT_MIN; StackNode* temp = *root; *root = (*root)->next; int popped = temp->data; free(temp); return popped; } int peek(StackNode* root) { if (isEmpty(root)) return INT_MIN; return root->data; } int main() { StackNode* root = NULL; push(&root, 10); push(&root, 20); push(&root, 30); cout << pop(&root) << \" popped from stack\\n\"; cout << \"Top element is \" << peek(root) << endl; return 0; } Problems \u00b6 reverse a string -> push push... pop pop (better -> just swap the pointers) 2 stacks in an array: divide space into two (space inefficient) start from extremes check for balanced parenthesis -> push if open the pop if closed and check the equality next grater element two loops (o(n^2)) using stacks (O(n)) Push the first element to stack. Pick rest of the elements one by one and follow the following steps in loop. Mark the current element as next. If stack is not empty, compare top element of stack with next. If next is greater than the top element,Pop element from stack. next is the next greater element for the popped element. Keep popping from the stack while the popped element is smaller than next. next becomes the next greater element for all such popped elements Finally, push the next in the stack. After the loop in step 2 is over, pop all the elements from stack and print -1 as next element for them. Time Complexity: O(n). The worst case occurs when all elements are sorted in decreasing order. If elements are sorted in decreasing order, then every element is processed at most 4 times. Initially pushed to the stack. Popped from the stack when next element is being processed. Pushed back to the stack because the next element is smaller. Popped from the stack in step 3 of algorithm. in the same order : void printNGE(int arr[], int n) { stack<int> s; int arr1[n]; for (int i = n - 1; i >= 0; i--) { while (!s.empty() && s.top() <= arr[i]) s.pop(); if (s.empty()) arr1[i] = -1; else arr1[i] = s.top(); s.push(arr[i]); } for (int i = 0; i < n; i++) cout << arr[i] << \" ---> \" << arr1[i] << endl; }","title":"Stack"},{"location":"ds/stack/intro.html#stack","text":"push(), pop(), isEmpty() and peek() all take O(1) follow LIFO/FILO order. real life -> stack of plates","title":"Stack"},{"location":"ds/stack/intro.html#applications-of-stack","text":"Balancing of symbols Infix to Postfix / Prefix conversion Redo-undo Forward and backward feature in web browsers Used in many algorithms like Tower of Hanoi, tree traversals, stock span problem, histogram problem. Other applications can be Backtracking, Knight tour problem, rat in a maze, N queen problem and sudoku solver In Graph Algorithms like Topological Sorting and Strongly Connected Components","title":"Applications of stack:"},{"location":"ds/stack/intro.html#implementation","text":"","title":"Implementation"},{"location":"ds/stack/intro.html#array","text":"Pros: Easy to implement. Memory is saved as pointers are not involved. Cons: It is not dynamic. It doesn\u2019t grow and shrink depending on needs at runtime. #define MAX 1000 class Stack { int top; public: int a[MAX]; // Maximum size of Stack Stack() { top = -1; } bool push(int x); int pop(); int peek(); bool isEmpty(); }; bool Stack::push(int x) { if (top >= (MAX - 1)) { cout << \"Stack Overflow\"; return false; } else { a[++top] = x; cout << x << \" pushed into stack\\n\"; return true; } } int Stack::pop() { if (top < 0) { cout << \"Stack Underflow\"; return 0; } else { int x = a[top--]; return x; } } int Stack::peek() { if (top < 0) { cout << \"Stack is Empty\"; return 0; } else { int x = a[top]; return x; } } bool Stack::isEmpty() { return (top < 0); } int main() { class Stack s; s.push(10); s.push(20); s.push(30); cout << s.pop() << \" Popped from stack\\n\"; return 0; }","title":"Array"},{"location":"ds/stack/intro.html#linked-list","text":"Pros: The linked list implementation of stack can grow and shrink according to the needs at runtime. Cons: Requires extra memory due to involvement of pointers. class StackNode { public: int data; StackNode* next; }; StackNode* newNode(int data) { StackNode* stackNode = new StackNode(); stackNode->data = data; stackNode->next = NULL; return stackNode; } int isEmpty(StackNode* root) { return !root; } void push(StackNode** root, int data) { StackNode* stackNode = newNode(data); stackNode->next = *root; *root = stackNode; cout << data << \" pushed to stack\\n\"; } int pop(StackNode** root) { if (isEmpty(*root)) return INT_MIN; StackNode* temp = *root; *root = (*root)->next; int popped = temp->data; free(temp); return popped; } int peek(StackNode* root) { if (isEmpty(root)) return INT_MIN; return root->data; } int main() { StackNode* root = NULL; push(&root, 10); push(&root, 20); push(&root, 30); cout << pop(&root) << \" popped from stack\\n\"; cout << \"Top element is \" << peek(root) << endl; return 0; }","title":"Linked List"},{"location":"ds/stack/intro.html#problems","text":"reverse a string -> push push... pop pop (better -> just swap the pointers) 2 stacks in an array: divide space into two (space inefficient) start from extremes check for balanced parenthesis -> push if open the pop if closed and check the equality next grater element two loops (o(n^2)) using stacks (O(n)) Push the first element to stack. Pick rest of the elements one by one and follow the following steps in loop. Mark the current element as next. If stack is not empty, compare top element of stack with next. If next is greater than the top element,Pop element from stack. next is the next greater element for the popped element. Keep popping from the stack while the popped element is smaller than next. next becomes the next greater element for all such popped elements Finally, push the next in the stack. After the loop in step 2 is over, pop all the elements from stack and print -1 as next element for them. Time Complexity: O(n). The worst case occurs when all elements are sorted in decreasing order. If elements are sorted in decreasing order, then every element is processed at most 4 times. Initially pushed to the stack. Popped from the stack when next element is being processed. Pushed back to the stack because the next element is smaller. Popped from the stack in step 3 of algorithm. in the same order : void printNGE(int arr[], int n) { stack<int> s; int arr1[n]; for (int i = n - 1; i >= 0; i--) { while (!s.empty() && s.top() <= arr[i]) s.pop(); if (s.empty()) arr1[i] = -1; else arr1[i] = s.top(); s.push(arr[i]); } for (int i = 0; i < n; i++) cout << arr[i] << \" ---> \" << arr1[i] << endl; }","title":"Problems"},{"location":"ds/stack/k-stacks.html","text":"K Stacks \u00b6 simple \u00b6 divide array into n/k stack overflow even if space is available extra 2 integer arrays \u00b6 1) top[k] -> top elements of stacks 2) next[] -> class kStacks { int *arr; int *top; int *next; int n, k; int free; public: kStacks(int k, int n); bool isFull() { return (free == -1); } void push(int item, int sn); int pop(int sn); bool isEmpty(int sn) { return (top[sn] == -1); } }; kStacks::kStacks(int k1, int n1) { k = k1, n = n1; arr = new int[n]; top = new int[k]; next = new int[n]; for (int i = 0; i < k; i++) top[i] = -1; free = 0; for (int i=0; i<n-1; i++) next[i] = i+1; next[n-1] = -1; } void kStacks::push(int item, int sn) { if (isFull()) { cout << \"\\nStack Overflow\\n\"; return; } int i = free; free = next[i]; next[i] = top[sn]; top[sn] = i; arr[i] = item; } int kStacks::pop(int sn) { if (isEmpty(sn)) { cout << \"\\nStack Underflow\\n\"; return INT_MAX; } int i = top[sn]; top[sn] = next[i]; next[i] = free; free = i; return arr[i]; }","title":"K Stacks"},{"location":"ds/stack/k-stacks.html#k-stacks","text":"","title":"K Stacks"},{"location":"ds/stack/k-stacks.html#simple","text":"divide array into n/k stack overflow even if space is available","title":"simple"},{"location":"ds/stack/k-stacks.html#extra-2-integer-arrays","text":"1) top[k] -> top elements of stacks 2) next[] -> class kStacks { int *arr; int *top; int *next; int n, k; int free; public: kStacks(int k, int n); bool isFull() { return (free == -1); } void push(int item, int sn); int pop(int sn); bool isEmpty(int sn) { return (top[sn] == -1); } }; kStacks::kStacks(int k1, int n1) { k = k1, n = n1; arr = new int[n]; top = new int[k]; next = new int[n]; for (int i = 0; i < k; i++) top[i] = -1; free = 0; for (int i=0; i<n-1; i++) next[i] = i+1; next[n-1] = -1; } void kStacks::push(int item, int sn) { if (isFull()) { cout << \"\\nStack Overflow\\n\"; return; } int i = free; free = next[i]; next[i] = top[sn]; top[sn] = i; arr[i] = item; } int kStacks::pop(int sn) { if (isEmpty(sn)) { cout << \"\\nStack Underflow\\n\"; return INT_MAX; } int i = top[sn]; top[sn] = next[i]; next[i] = free; free = i; return arr[i]; }","title":"extra 2 integer arrays"},{"location":"ds/stack/medium-special.html","text":"Medium Special \u00b6 findMiddle() which will return middle element of the stack. deleteMiddle() which will delete the middle element. void push(myStack *ms, int new_data) { DLLNode* new_DLLNode = new DLLNode(); new_DLLNode->data = new_data; new_DLLNode->prev = NULL; new_DLLNode->next = ms->head; ms->count += 1; if (ms->count == 1) { ms->mid = new_DLLNode; } else { ms->head->prev = new_DLLNode; if(!(ms->count & 1)) // Update mid if ms->count is even ms->mid = ms->mid->prev; } ms->head = new_DLLNode; } int pop(myStack *ms) { if (ms->count == 0) { cout<<\"Stack is empty\\n\"; return -1; } DLLNode *head = ms->head; int item = head->data; ms->head = head->next; if (ms->head != NULL) ms->head->prev = NULL; ms->count -= 1; if ((ms->count) & 1 ) ms->mid = ms->mid->next; free(head); return item; }","title":"Medium Special"},{"location":"ds/stack/medium-special.html#medium-special","text":"findMiddle() which will return middle element of the stack. deleteMiddle() which will delete the middle element. void push(myStack *ms, int new_data) { DLLNode* new_DLLNode = new DLLNode(); new_DLLNode->data = new_data; new_DLLNode->prev = NULL; new_DLLNode->next = ms->head; ms->count += 1; if (ms->count == 1) { ms->mid = new_DLLNode; } else { ms->head->prev = new_DLLNode; if(!(ms->count & 1)) // Update mid if ms->count is even ms->mid = ms->mid->prev; } ms->head = new_DLLNode; } int pop(myStack *ms) { if (ms->count == 0) { cout<<\"Stack is empty\\n\"; return -1; } DLLNode *head = ms->head; int item = head->data; ms->head = head->next; if (ms->head != NULL) ms->head->prev = NULL; ms->count -= 1; if ((ms->count) & 1 ) ms->mid = ms->mid->next; free(head); return item; }","title":"Medium Special"},{"location":"ds/stack/min-special.html","text":"Min Special \u00b6 void SpecialStack::push(int x) { if (isEmpty() == true) { Stack::push(x); min.push(x); } else { Stack::push(x); int y = min.pop(); min.push(y); if (x < y) min.push(x); else min.push(y); } } /* SpecialStack's member method to remove an element from it. This method removes top element from min stack also. */ int SpecialStack::pop() { int x = Stack::pop(); min.pop(); return x; } operations -> push, pop, isempty, getMin -> O(1) aux. space -> O(n) Optimised space for 2 stacks: \u00b6 void SpecialStack::push(int x) { if (isEmpty() == true) { Stack::push(x); min.push(x); } else { Stack::push(x); int y = min.pop(); min.push(y); if (x <= y) min.push(x); } } int SpecialStack::pop() { int x = Stack::pop(); int y = min.pop(); if (y != x) min.push(y); return x; } operations -> push, pop, isempty, getMin -> O(1) aux. space -> O(n) (Worst case -> decreasing order)","title":"Min Special"},{"location":"ds/stack/min-special.html#min-special","text":"void SpecialStack::push(int x) { if (isEmpty() == true) { Stack::push(x); min.push(x); } else { Stack::push(x); int y = min.pop(); min.push(y); if (x < y) min.push(x); else min.push(y); } } /* SpecialStack's member method to remove an element from it. This method removes top element from min stack also. */ int SpecialStack::pop() { int x = Stack::pop(); min.pop(); return x; } operations -> push, pop, isempty, getMin -> O(1) aux. space -> O(n)","title":"Min Special"},{"location":"ds/stack/min-special.html#optimised-space-for-2-stacks","text":"void SpecialStack::push(int x) { if (isEmpty() == true) { Stack::push(x); min.push(x); } else { Stack::push(x); int y = min.pop(); min.push(y); if (x <= y) min.push(x); } } int SpecialStack::pop() { int x = Stack::pop(); int y = min.pop(); if (y != x) min.push(y); return x; } operations -> push, pop, isempty, getMin -> O(1) aux. space -> O(n) (Worst case -> decreasing order)","title":"Optimised space for 2 stacks:"},{"location":"ds/stack/reverse.html","text":"Reverse \u00b6 Recursion char insert_at_bottom(char x) { if(st.size() == 0) st.push(x); else { char a = st.top(); st.pop(); insert_at_bottom(x); st.push(a); } } char reverse() { if(st.size()>0) { char x = st.top(); st.pop(); reverse(); insert_at_bottom(x); } }","title":"Reverse"},{"location":"ds/stack/reverse.html#reverse","text":"Recursion char insert_at_bottom(char x) { if(st.size() == 0) st.push(x); else { char a = st.top(); st.pop(); insert_at_bottom(x); st.push(a); } } char reverse() { if(st.size()>0) { char x = st.top(); st.pop(); reverse(); insert_at_bottom(x); } }","title":"Reverse"},{"location":"ds/stack/sort.html","text":"Sort \u00b6 Recursion Time Complexity: O(n2). In the worst case for every sortstack(), sortedinsert() is called for \u2018N\u2019 times recursively for putting element to the right place Auxiliary Space: O(N) void sortedInsert(struct stack** s, int x) { if (isEmpty(*s) or x > top(*s)) { push(s, x); return; } int temp = pop(s); sortedInsert(s, x); push(s, temp); } void sortStack(struct stack** s) { if (!isEmpty(*s)) { int x = pop(s); sortStack(s); sortedInsert(s, x); } }","title":"Sort"},{"location":"ds/stack/sort.html#sort","text":"Recursion Time Complexity: O(n2). In the worst case for every sortstack(), sortedinsert() is called for \u2018N\u2019 times recursively for putting element to the right place Auxiliary Space: O(N) void sortedInsert(struct stack** s, int x) { if (isEmpty(*s) or x > top(*s)) { push(s, x); return; } int temp = pop(s); sortedInsert(s, x); push(s, temp); } void sortStack(struct stack** s) { if (!isEmpty(*s)) { int x = pop(s); sortStack(s); sortedInsert(s, x); } }","title":"Sort"},{"location":"ds/stack/stock-span.html","text":"Stock Span \u00b6 Naive \u00b6 void calculateSpan(int price[], int n, int S[]) { S[0] = 1; for (int i = 1; i < n; i++) { S[i] = 1; for (int j = i - 1; (j >= 0) && (price[i] >= price[j]); j--) S[i]++; } } O(n^2), O(n) using Stack \u00b6 void calculateSpan(int price[], int n, int S[]) { stack<int> st; st.push(0); S[0] = 1; for (int i = 1; i < n; i++) { while (!st.empty() && price[st.top()] <= price[i]) st.pop(); S[i] = (st.empty()) ? (i + 1) : (i - st.top()); st.push(i); } } O(2n), O(n) Without stack \u00b6 void calculateSpan(int A[], int n, int ans[]) { ans[0] = 1; for (int i = 1; i < n; i++) { int counter = 1; while ((i - counter) >= 0 && A[i] >= A[i - counter]) { counter += ans[i - counter]; } ans[i] = counter; } }","title":"Stock Span"},{"location":"ds/stack/stock-span.html#stock-span","text":"","title":"Stock Span"},{"location":"ds/stack/stock-span.html#naive","text":"void calculateSpan(int price[], int n, int S[]) { S[0] = 1; for (int i = 1; i < n; i++) { S[i] = 1; for (int j = i - 1; (j >= 0) && (price[i] >= price[j]); j--) S[i]++; } } O(n^2), O(n)","title":"Naive"},{"location":"ds/stack/stock-span.html#using-stack","text":"void calculateSpan(int price[], int n, int S[]) { stack<int> st; st.push(0); S[0] = 1; for (int i = 1; i < n; i++) { while (!st.empty() && price[st.top()] <= price[i]) st.pop(); S[i] = (st.empty()) ? (i + 1) : (i - st.top()); st.push(i); } } O(2n), O(n)","title":"using Stack"},{"location":"ds/stack/stock-span.html#without-stack","text":"void calculateSpan(int A[], int n, int ans[]) { ans[0] = 1; for (int i = 1; i < n; i++) { int counter = 1; while ((i - counter) >= 0 && A[i] >= A[i - counter]) { counter += ans[i - counter]; } ans[i] = counter; } }","title":"Without stack"},{"location":"ds/stack/using-queue.html","text":"Stack using Queue \u00b6 Costly Push \u00b6 void push(int x) { curr_size++; q2.push(x); while (!q1.empty()) { q2.push(q1.front()); q1.pop(); } queue<int> q = q1; q1 = q2; q2 = q; } void pop() { if (q1.empty()) return; q1.pop(); curr_size--; } Costly Pop \u00b6 void pop() { if (q1.empty()) return; while (q1.size() != 1) { q2.push(q1.front()); q1.pop(); } q1.pop(); curr_size--; queue<int> q = q1; q1 = q2; q2 = q; } void push(int x) { q1.push(x); curr_size++; }","title":"Stack using Queue"},{"location":"ds/stack/using-queue.html#stack-using-queue","text":"","title":"Stack using Queue"},{"location":"ds/stack/using-queue.html#costly-push","text":"void push(int x) { curr_size++; q2.push(x); while (!q1.empty()) { q2.push(q1.front()); q1.pop(); } queue<int> q = q1; q1 = q2; q2 = q; } void pop() { if (q1.empty()) return; q1.pop(); curr_size--; }","title":"Costly Push"},{"location":"ds/stack/using-queue.html#costly-pop","text":"void pop() { if (q1.empty()) return; while (q1.size() != 1) { q2.push(q1.front()); q1.pop(); } q1.pop(); curr_size--; queue<int> q = q1; q1 = q2; q2 = q; } void push(int x) { q1.push(x); curr_size++; }","title":"Costly Pop"},{"location":"ds/string/duplicate.html","text":"Duplicate \u00b6 string s = \"abca\"; int count[26]; memset(count, 0, sizeof(count)); for(int i =0;i<s.size();i++){ count[s[i]-'a']++; } for(int i = 0; i<26;i++){ if(count[i]>1) cout << char('a'+i) << endl; }","title":"Duplicate"},{"location":"ds/string/duplicate.html#duplicate","text":"string s = \"abca\"; int count[26]; memset(count, 0, sizeof(count)); for(int i =0;i<s.size();i++){ count[s[i]-'a']++; } for(int i = 0; i<26;i++){ if(count[i]>1) cout << char('a'+i) << endl; }","title":"Duplicate"},{"location":"ds/string/frequency.html","text":"Frequency \u00b6 string s = \"abcdddssod\"; int count[26]; memset(count, 0, sizeof(count)); for(int i = 0; i<s.size();i++) count[s[i]-'a']++; for(int i =0; i<26; i++) if(count[i]>0){ cout << char('a' + i) << count[i] << endl; }","title":"Frequency"},{"location":"ds/string/frequency.html#frequency","text":"string s = \"abcdddssod\"; int count[26]; memset(count, 0, sizeof(count)); for(int i = 0; i<s.size();i++) count[s[i]-'a']++; for(int i =0; i<26; i++) if(count[i]>0){ cout << char('a' + i) << count[i] << endl; }","title":"Frequency"},{"location":"ds/string/intro.html","text":"String \u00b6 characters as a sequence of bytes allowing access to single byte character. std::string vs Character Array \u00b6 array of characters terminated by a null character, static allocation -> array class which defines objects, dynamic alocation -> string threat of array decay. As strings are represented as objects, no array decay occurs. Strings are slower when compared to implementation than character array. string has many inbuilt operations. Operations on strings \u00b6 Input \u00b6 getline() : store a stream of characters by the user in the object memory. push_back() : input a character at the end of the string. pop_back() : delete the last character from the string. Capacity \u00b6 capacity() : which can be equal to or more than the size. resize() : size can be increased or decreased. length(): length of the string. shrink_to_fit(): decreases the capacity of the string and makes it equal to the minimum capacity of the string. Iterator \u00b6 begin(): iterator to beginning of the string. end(): iterator to end of the string. rbegin(): reverse iterator pointing at the end of string. rend(): reverse iterator pointing at beginning of string. Manipulating \u00b6 copy(\u201cchar array\u201d, len, pos) :- This function copies the substring in target character array mentioned in its arguments. It takes 3 arguments, target char array, length to be copied and starting position in string to start copying. swap() strcpy(s1, s2): Copies string s2 into string s1. strcat(s1, s2): Concatenates string s2 onto the end of string s1. strlen(s1) strcmp(s1, s2): 0 if s1 == s2; less than 0 if s1<s2; greater than 0 if s1>s2. strchr(s1, ch): pointer to the first occurrence of character ch in string s1. strstr(s1, s2): pointer to the first occurrence of string s2 in string s1.","title":"String"},{"location":"ds/string/intro.html#string","text":"characters as a sequence of bytes allowing access to single byte character.","title":"String"},{"location":"ds/string/intro.html#stdstring-vs-character-array","text":"array of characters terminated by a null character, static allocation -> array class which defines objects, dynamic alocation -> string threat of array decay. As strings are represented as objects, no array decay occurs. Strings are slower when compared to implementation than character array. string has many inbuilt operations.","title":"std::string vs Character Array"},{"location":"ds/string/intro.html#operations-on-strings","text":"","title":"Operations on strings"},{"location":"ds/string/intro.html#input","text":"getline() : store a stream of characters by the user in the object memory. push_back() : input a character at the end of the string. pop_back() : delete the last character from the string.","title":"Input"},{"location":"ds/string/intro.html#capacity","text":"capacity() : which can be equal to or more than the size. resize() : size can be increased or decreased. length(): length of the string. shrink_to_fit(): decreases the capacity of the string and makes it equal to the minimum capacity of the string.","title":"Capacity"},{"location":"ds/string/intro.html#iterator","text":"begin(): iterator to beginning of the string. end(): iterator to end of the string. rbegin(): reverse iterator pointing at the end of string. rend(): reverse iterator pointing at beginning of string.","title":"Iterator"},{"location":"ds/string/intro.html#manipulating","text":"copy(\u201cchar array\u201d, len, pos) :- This function copies the substring in target character array mentioned in its arguments. It takes 3 arguments, target char array, length to be copied and starting position in string to start copying. swap() strcpy(s1, s2): Copies string s2 into string s1. strcat(s1, s2): Concatenates string s2 onto the end of string s1. strlen(s1) strcmp(s1, s2): 0 if s1 == s2; less than 0 if s1<s2; greater than 0 if s1>s2. strchr(s1, ch): pointer to the first occurrence of character ch in string s1. strstr(s1, s2): pointer to the first occurrence of string s2 in string s1.","title":"Manipulating"},{"location":"ds/string/remove-duplicates.html","text":"Remove Duplicates \u00b6 char * removeDuplicate(char str[], int n){ int index = 0; for(int i = 0; i< n ; i++){ int j; for(j = 0; j<n; j++) if(str[i]==str[j]) break; if(i==j) str[index++] = str[i]; } return str; }","title":"Remove Duplicates"},{"location":"ds/string/remove-duplicates.html#remove-duplicates","text":"char * removeDuplicate(char str[], int n){ int index = 0; for(int i = 0; i< n ; i++){ int j; for(j = 0; j<n; j++) if(str[i]==str[j]) break; if(i==j) str[index++] = str[i]; } return str; }","title":"Remove Duplicates"},{"location":"ds/string/reverse.html","text":"Reverse \u00b6 void reverseStr(string& str) { int n = str.length(); for (int i = 0; i < n / 2; i++) swap(str[i], str[n - i - 1]); }","title":"Reverse"},{"location":"ds/string/reverse.html#reverse","text":"void reverseStr(string& str) { int n = str.length(); for (int i = 0; i < n / 2; i++) swap(str[i], str[n - i - 1]); }","title":"Reverse"},{"location":"ds/string/rotation.html","text":"Rotation \u00b6 if 2 strings are rotation of each other. string s = \"ABCDE\"; string temp = s + s; string str2 = \"BCDEA\"; cout << (temp.find(str2) != string::npos);","title":"Rotation"},{"location":"ds/string/rotation.html#rotation","text":"if 2 strings are rotation of each other. string s = \"ABCDE\"; string temp = s + s; string str2 = \"BCDEA\"; cout << (temp.find(str2) != string::npos);","title":"Rotation"},{"location":"ds/string/string-to-no.html","text":"Strings to Numbers \u00b6 Using stringstream class \u00b6 #include <iostream> #include <sstream> using namespace std; int main() { string s = \"12345\"; stringstream geek(s); int x = 0; geek >> x; cout << \"Value of x : \" << x; return 0; } sscanf() \u00b6 sscanf() is a C style function similar to scanf(). It reads input from a string rather that standard input. const char *str = \"12345\"; int x; sscanf(str, \"%d\", &x); printf(\"\\nThe value of x : %d\", x); Functions \u00b6 int stoi (const string& str, size_t* index = 0, int base = 10): string to value int atoi (const char * str) : character array or string literal tp value. stoi() vs atoi() \u00b6 atoi stoi legacy c style function c++11 c char and string literal c++ string 1 parameter 3 parameters: string, start index, base String to Double \u00b6 atof(): the converted integral number as an int value. If no valid conversion could be performed, it returns zero.","title":"Strings to Numbers"},{"location":"ds/string/string-to-no.html#strings-to-numbers","text":"","title":"Strings to Numbers"},{"location":"ds/string/string-to-no.html#using-stringstream-class","text":"#include <iostream> #include <sstream> using namespace std; int main() { string s = \"12345\"; stringstream geek(s); int x = 0; geek >> x; cout << \"Value of x : \" << x; return 0; }","title":"Using stringstream class"},{"location":"ds/string/string-to-no.html#sscanf","text":"sscanf() is a C style function similar to scanf(). It reads input from a string rather that standard input. const char *str = \"12345\"; int x; sscanf(str, \"%d\", &x); printf(\"\\nThe value of x : %d\", x);","title":"sscanf()"},{"location":"ds/string/string-to-no.html#functions","text":"int stoi (const string& str, size_t* index = 0, int base = 10): string to value int atoi (const char * str) : character array or string literal tp value.","title":"Functions"},{"location":"ds/string/string-to-no.html#stoi-vs-atoi","text":"atoi stoi legacy c style function c++11 c char and string literal c++ string 1 parameter 3 parameters: string, start index, base","title":"stoi() vs atoi()"},{"location":"ds/string/string-to-no.html#string-to-double","text":"atof(): the converted integral number as an int value. If no valid conversion could be performed, it returns zero.","title":"String to Double"},{"location":"ds/string/white-space.html","text":"White Space \u00b6 void removeSpaces(string &str) { int n = str.length(); int i = 0, j = -1; bool spaceFound = false; while (++j < n && str[j] == ' '); while (j < n) { if (str[j] != ' ') { if ((str[j] == '.' || str[j] == ',' || str[j] == '?') && i - 1 >= 0 && str[i - 1] == ' ') str[i - 1] = str[j++]; else str[i++] = str[j++]; spaceFound = false; } else if (str[j++] == ' ') { if (!spaceFound) { str[i++] = ' '; spaceFound = true; } } } if (i <= 1) str.erase(str.begin() + i, str.end()); else str.erase(str.begin() + i - 1, str.end()); }","title":"White Space"},{"location":"ds/string/white-space.html#white-space","text":"void removeSpaces(string &str) { int n = str.length(); int i = 0, j = -1; bool spaceFound = false; while (++j < n && str[j] == ' '); while (j < n) { if (str[j] != ' ') { if ((str[j] == '.' || str[j] == ',' || str[j] == '?') && i - 1 >= 0 && str[i - 1] == ' ') str[i - 1] = str[j++]; else str[i++] = str[j++]; spaceFound = false; } else if (str[j++] == ' ') { if (!spaceFound) { str[i++] = ' '; spaceFound = true; } } } if (i <= 1) str.erase(str.begin() + i, str.end()); else str.erase(str.begin() + i - 1, str.end()); }","title":"White Space"},{"location":"ds/tree/binary-tree.html","text":"Binary Tree \u00b6 Each node -> 2 children -> left and right Representaion \u00b6 A node have: Data Pointer to left child Pointer to right child struct node { int data; struct node *left; struct node *right; }; Example \u00b6 #include<stdio.h> #include<stdlib.h> struct node { int data; struct node *left; struct node *right; }; struct node* newNode(int data) { struct node* node = (struct node*)malloc(sizeof(struct node)); node->data = data; node->left = NULL; node->right = NULL; return(node); } int main() { struct node *root = newNode(1); /* 1 / \\ NULL NULL */ root->left = newNode(2); root->right = newNode(3); /* 1 / \\ 2 3 / \\ / \\ NULL NULL NULL NULL */ root->left->left = newNode(4); /* 1 / \\ 2 3 / \\ / \\ 4 NULL NULL NULL / \\ NULL NULL */ getchar(); return 0; } Properties \u00b6 Nomenclature l -> level h -> height n -> nodes L -> Levels max n at l is 2^l max n at h -> 2^h -1 (when root's h = 1) as 1 + 2 + 4 + .. + 2^(h-1) , 2^(h+1) -1 (when root's h = 0) min h in Binary tree = Log2(N+1) reverse of point 2 A Binary Tree with L leaves has at least Log2L + 1 levels L <= 2^l-1 [From Point 1] l = Log2L + 1 where l is the minimum number of levels. Full Binary Tree \u00b6 Only 0 or 2 children i.e. only leaf nodes have 0 children. L = I + 1 Where L = Number of leaf nodes, I = Number of internal nodes PROOF: https://www.geeksforgeeks.org/handshaking-lemma-and-interesting-tree-properties/ Complete Binary Tree \u00b6 All levels filled except last level -> all nodes are as left as possible APPLICATION: binary-heap Perfect Binary Tree \u00b6 all nodes have 2 children except leaf -> 0 All levels filled n -> 2^h - 1 n with root's h = 1 Balanced Binary Tree \u00b6 h = log2n AVL - h(left sub tree) - h(right sub tree) \u2248 1 Red-Black trees - Black nodes on every root to leaf paths is the same and there are no adjacent red nodes // TODO don't know O(log n) time for search, insert and delete. A degenerate (or pathological) tree \u00b6 internal node -> 1 child Performance = Linked List Full \u00b6 18 / \\ 15 20 / \\ 40 50 / \\ 30 50 Complete \u00b6 18 / \\ 15 30 / \\ / \\ 40 50 100 40 / \\ / 8 7 9 Perfect \u00b6 18 / \\ 15 30 / \\ / \\ 40 50 100 40 Balanced \u00b6 // TODO example Degenerate \u00b6 10 / 20 \\ 30 \\ 40 Applications \u00b6 Store hierarchical data, like folder structure, organization structure, XML/HTML data. Binary Search Tree is a tree that allows fast search, insert, delete on a sorted data. It also allows finding closest item Heap is a tree data structure which is implemented using arrays and used to implement priority queues. B-Tree and B+ Tree : They are used to implement indexing in databases. Syntax Tree: Used in Compilers. K-D Tree: A space partitioning tree used to organize points in K dimensional space. Trie : Used to implement dictionaries with prefix lookup. Suffix Tree : For quick pattern searching in a fixed text. Spanning Trees and shortest path trees are used in routers and bridges respectively in computer networks As a workflow for compositing digital images for visual effects.","title":"Binary Tree"},{"location":"ds/tree/binary-tree.html#binary-tree","text":"Each node -> 2 children -> left and right","title":"Binary Tree"},{"location":"ds/tree/binary-tree.html#representaion","text":"A node have: Data Pointer to left child Pointer to right child struct node { int data; struct node *left; struct node *right; };","title":"Representaion"},{"location":"ds/tree/binary-tree.html#example","text":"#include<stdio.h> #include<stdlib.h> struct node { int data; struct node *left; struct node *right; }; struct node* newNode(int data) { struct node* node = (struct node*)malloc(sizeof(struct node)); node->data = data; node->left = NULL; node->right = NULL; return(node); } int main() { struct node *root = newNode(1); /* 1 / \\ NULL NULL */ root->left = newNode(2); root->right = newNode(3); /* 1 / \\ 2 3 / \\ / \\ NULL NULL NULL NULL */ root->left->left = newNode(4); /* 1 / \\ 2 3 / \\ / \\ 4 NULL NULL NULL / \\ NULL NULL */ getchar(); return 0; }","title":"Example"},{"location":"ds/tree/binary-tree.html#properties","text":"Nomenclature l -> level h -> height n -> nodes L -> Levels max n at l is 2^l max n at h -> 2^h -1 (when root's h = 1) as 1 + 2 + 4 + .. + 2^(h-1) , 2^(h+1) -1 (when root's h = 0) min h in Binary tree = Log2(N+1) reverse of point 2 A Binary Tree with L leaves has at least Log2L + 1 levels L <= 2^l-1 [From Point 1] l = Log2L + 1 where l is the minimum number of levels.","title":"Properties"},{"location":"ds/tree/binary-tree.html#full-binary-tree","text":"Only 0 or 2 children i.e. only leaf nodes have 0 children. L = I + 1 Where L = Number of leaf nodes, I = Number of internal nodes PROOF: https://www.geeksforgeeks.org/handshaking-lemma-and-interesting-tree-properties/","title":"Full Binary Tree"},{"location":"ds/tree/binary-tree.html#complete-binary-tree","text":"All levels filled except last level -> all nodes are as left as possible APPLICATION: binary-heap","title":"Complete Binary Tree"},{"location":"ds/tree/binary-tree.html#perfect-binary-tree","text":"all nodes have 2 children except leaf -> 0 All levels filled n -> 2^h - 1 n with root's h = 1","title":"Perfect Binary Tree"},{"location":"ds/tree/binary-tree.html#balanced-binary-tree","text":"h = log2n AVL - h(left sub tree) - h(right sub tree) \u2248 1 Red-Black trees - Black nodes on every root to leaf paths is the same and there are no adjacent red nodes // TODO don't know O(log n) time for search, insert and delete.","title":"Balanced Binary Tree"},{"location":"ds/tree/binary-tree.html#a-degenerate-or-pathological-tree","text":"internal node -> 1 child Performance = Linked List","title":"A degenerate (or pathological) tree"},{"location":"ds/tree/binary-tree.html#full","text":"18 / \\ 15 20 / \\ 40 50 / \\ 30 50","title":"Full"},{"location":"ds/tree/binary-tree.html#complete","text":"18 / \\ 15 30 / \\ / \\ 40 50 100 40 / \\ / 8 7 9","title":"Complete"},{"location":"ds/tree/binary-tree.html#perfect","text":"18 / \\ 15 30 / \\ / \\ 40 50 100 40","title":"Perfect"},{"location":"ds/tree/binary-tree.html#balanced","text":"// TODO example","title":"Balanced"},{"location":"ds/tree/binary-tree.html#degenerate","text":"10 / 20 \\ 30 \\ 40","title":"Degenerate"},{"location":"ds/tree/binary-tree.html#applications","text":"Store hierarchical data, like folder structure, organization structure, XML/HTML data. Binary Search Tree is a tree that allows fast search, insert, delete on a sorted data. It also allows finding closest item Heap is a tree data structure which is implemented using arrays and used to implement priority queues. B-Tree and B+ Tree : They are used to implement indexing in databases. Syntax Tree: Used in Compilers. K-D Tree: A space partitioning tree used to organize points in K dimensional space. Trie : Used to implement dictionaries with prefix lookup. Suffix Tree : For quick pattern searching in a fixed text. Spanning Trees and shortest path trees are used in routers and bridges respectively in computer networks As a workflow for compositing digital images for visual effects.","title":"Applications"},{"location":"ds/tree/bst-check.html","text":"Checking BST \u00b6 METHOD 1 (Simple but Wrong) \u00b6 int isBST(struct node* node) { if (node == NULL) return 1; if (node->left != NULL && node->left->data > node->data) return 0; if (node->right != NULL && node->right->data < node->data) return 0; if (!isBST(node->left) || !isBST(node->right)) return 0; return 1; } But it returns true if a node is grater than root but is present in left most leaf METHOD 2 (Correct but not efficient) \u00b6 int isBST(struct node* node) { if (node == NULL) return 1; if (node->left!=NULL && maxValue(node->left) > node->data) return 0; if (node->right!=NULL && minValue(node->right) < node->data) return 0; if (!isBST(node->left) || !isBST(node->right)) return 0; return 1; } METHOD 3 (Correct and Efficient): \u00b6 int isBST(node* node) { return(isBSTUtil(node, INT_MIN, INT_MAX)); } int isBSTUtil(node* node, int min, int max) { if (node==NULL) return 1; if (node->data < min || node->data > max) return 0; return isBSTUtil(node->left, min, node->data-1) && isBSTUtil(node->right, node->data+1, max); } Time Complexity: O(n) Auxiliary Space : O(1) if Function Call Stack size is not considered, otherwise O(n) Simplified Method 3 \u00b6 bool isBST(Node* root, Node* l=NULL, Node* r=NULL) { if (root == NULL) return true; if (l != NULL and root->data <= l->data) return false; if (r != NULL and root->data >= r->data) return false; return isBST(root->left, l, root) and isBST(root->right, root, r); } METHOD 4(Using In-Order Traversal) \u00b6 Do In-Order Traversal of the given tree and store the result in a temp array. Check if the temp array is sorted in ascending order, if it is, then the tree is BST. bool isBST(node* root) { static node *prev = NULL; if (root) { if (!isBST(root->left)) return false; if (prev != NULL && root->data <= prev->data) return false; prev = root; return isBST(root->right); } return true; }","title":"Checking BST"},{"location":"ds/tree/bst-check.html#checking-bst","text":"","title":"Checking BST"},{"location":"ds/tree/bst-check.html#method-1-simple-but-wrong","text":"int isBST(struct node* node) { if (node == NULL) return 1; if (node->left != NULL && node->left->data > node->data) return 0; if (node->right != NULL && node->right->data < node->data) return 0; if (!isBST(node->left) || !isBST(node->right)) return 0; return 1; } But it returns true if a node is grater than root but is present in left most leaf","title":"METHOD 1 (Simple but Wrong)"},{"location":"ds/tree/bst-check.html#method-2-correct-but-not-efficient","text":"int isBST(struct node* node) { if (node == NULL) return 1; if (node->left!=NULL && maxValue(node->left) > node->data) return 0; if (node->right!=NULL && minValue(node->right) < node->data) return 0; if (!isBST(node->left) || !isBST(node->right)) return 0; return 1; }","title":"METHOD 2 (Correct but not efficient)"},{"location":"ds/tree/bst-check.html#method-3-correct-and-efficient","text":"int isBST(node* node) { return(isBSTUtil(node, INT_MIN, INT_MAX)); } int isBSTUtil(node* node, int min, int max) { if (node==NULL) return 1; if (node->data < min || node->data > max) return 0; return isBSTUtil(node->left, min, node->data-1) && isBSTUtil(node->right, node->data+1, max); } Time Complexity: O(n) Auxiliary Space : O(1) if Function Call Stack size is not considered, otherwise O(n)","title":"METHOD 3 (Correct and Efficient):"},{"location":"ds/tree/bst-check.html#simplified-method-3","text":"bool isBST(Node* root, Node* l=NULL, Node* r=NULL) { if (root == NULL) return true; if (l != NULL and root->data <= l->data) return false; if (r != NULL and root->data >= r->data) return false; return isBST(root->left, l, root) and isBST(root->right, root, r); }","title":"Simplified Method 3"},{"location":"ds/tree/bst-check.html#method-4using-in-order-traversal","text":"Do In-Order Traversal of the given tree and store the result in a temp array. Check if the temp array is sorted in ascending order, if it is, then the tree is BST. bool isBST(node* root) { static node *prev = NULL; if (root) { if (!isBST(root->left)) return false; if (prev != NULL && root->data <= prev->data) return false; prev = root; return isBST(root->right); } return true; }","title":"METHOD 4(Using In-Order Traversal)"},{"location":"ds/tree/bst-merge.html","text":"Merge BST \u00b6 Stack Related Stuff \u00b6 class snode { public: node *t; snode *next; }; void push(snode **s, node *k) { snode *tmp = new snode(); tmp->t = k; tmp->next = *s; (*s) = tmp; } node *pop(snode **s) { node *t; snode *st; st=*s; (*s) = (*s)->next; t = st->t; free(st); return t; } int isEmpty(snode *s) { if (s == NULL ) return 1; return 0; } Merge \u00b6 void merge(node *root1, node *root2) { snode *s1 = NULL; node *current1 = root1; snode *s2 = NULL; node *current2 = root2; if (root1 == NULL) { inorder(root2); return; } if (root2 == NULL) { inorder(root1); return ; } while (current1 != NULL || !isEmpty(s1) || current2 != NULL || !isEmpty(s2)) { if (current1 != NULL || current2 != NULL ) { if (current1 != NULL) { push(&s1, current1); current1 = current1->left; } if (current2 != NULL) { push(&s2, current2); current2 = current2->left; } } else { if (isEmpty(s1)) { while (!isEmpty(s2)) { current2 = pop (&s2); current2->left = NULL; inorder(current2); } return ; } if (isEmpty(s2)) { while (!isEmpty(s1)) { current1 = pop (&s1); current1->left = NULL; inorder(current1); } return ; } current1 = pop(&s1); current2 = pop(&s2); if (current1->data < current2->data) { cout<<current1->data<<\" \"; current1 = current1->right; push(&s2, current2); current2 = NULL; } else { cout<<current2->data<<\" \"; current2 = current2->right; push(&s1, current1); current1 = NULL; } } } }","title":"Merge BST"},{"location":"ds/tree/bst-merge.html#merge-bst","text":"","title":"Merge BST"},{"location":"ds/tree/bst-merge.html#stack-related-stuff","text":"class snode { public: node *t; snode *next; }; void push(snode **s, node *k) { snode *tmp = new snode(); tmp->t = k; tmp->next = *s; (*s) = tmp; } node *pop(snode **s) { node *t; snode *st; st=*s; (*s) = (*s)->next; t = st->t; free(st); return t; } int isEmpty(snode *s) { if (s == NULL ) return 1; return 0; }","title":"Stack Related Stuff"},{"location":"ds/tree/bst-merge.html#merge","text":"void merge(node *root1, node *root2) { snode *s1 = NULL; node *current1 = root1; snode *s2 = NULL; node *current2 = root2; if (root1 == NULL) { inorder(root2); return; } if (root2 == NULL) { inorder(root1); return ; } while (current1 != NULL || !isEmpty(s1) || current2 != NULL || !isEmpty(s2)) { if (current1 != NULL || current2 != NULL ) { if (current1 != NULL) { push(&s1, current1); current1 = current1->left; } if (current2 != NULL) { push(&s2, current2); current2 = current2->left; } } else { if (isEmpty(s1)) { while (!isEmpty(s2)) { current2 = pop (&s2); current2->left = NULL; inorder(current2); } return ; } if (isEmpty(s2)) { while (!isEmpty(s1)) { current1 = pop (&s1); current1->left = NULL; inorder(current1); } return ; } current1 = pop(&s1); current2 = pop(&s2); if (current1->data < current2->data) { cout<<current1->data<<\" \"; current1 = current1->right; push(&s2, current2); current2 = NULL; } else { cout<<current2->data<<\" \"; current2 = current2->right; push(&s1, current1); current1 = NULL; } } } }","title":"Merge"},{"location":"ds/tree/bst.html","text":"Binary Search Tree \u00b6 Insertion \u00b6 struct node* insert(struct node* node, int key) { if (node == NULL) return newNode(key); if (key < node->key) node->left = insert(node->left, key); else node->right = insert(node->right, key); return node; } Min Value \u00b6 struct node* minValueNode(struct node* node) { struct node* current = node; while (current && current->left != NULL) current = current->left; return current; } Deletion \u00b6 struct node* deleteNode(struct node* root, int key) { if (root == NULL) return root; if (key < root->key) root->left = deleteNode(root->left, key); else if (key > root->key) root->right = deleteNode(root->right, key); else { if (root.left==NULL and root.right==NULL): return NULL elif (root->left == NULL) { struct node* temp = root->right; free(root); return temp; } else if (root->right == NULL) { struct node* temp = root->left; free(root); return temp; } struct node* temp = minValueNode(root->right); root->key = temp->key; root->right = deleteNode(root->right, temp->key); } return root; } node-based binary tree data structure The left subtree of a node contains only nodes with keys lesser than the node\u2019s key. The right subtree of a node contains only nodes with keys greater than the node\u2019s key. The left and right subtree -> binary search tree. no duplicate nodes. Facts \u00b6 Inorder traversal -> sorted output. We can construct a BST with only Preorder or Postorder or Level Order traversal. Note that we can always get inorder traversal by sorting the only given traversal. Number of unique BSTs with n distinct keys is Catalan Number Each node (item in the tree) has a distinct key. Search \u00b6 struct node* search(struct node* root, int key) { if (root == NULL || root->key == key) return root; if (root->key < key) return search(root->right, key); return search(root->left, key); } Insert \u00b6 BST* BST ::Insert(BST* root, int value) { if (!root) { return new BST(value); } if (value > root->data) { root->right = Insert(root->right, value); } else { root->left = Insert(root->left, value); } return root; } Deletion \u00b6 leaf -> simply delete one child -> put it in the deleted place two child -> place the inorder successor or predeccesor in its place The important thing to note is, inorder successor is needed only when right child is not empty. In this particular case, inorder successor can be obtained by finding the minimum value in right child of the node. struct node* deleteNode(struct node* root, int key) { if (root == NULL) return root; if (key < root->key) root->left = deleteNode(root->left, key); else if (key > root->key) root->right = deleteNode(root->right, key); else { if (root->left == NULL) { struct node *temp = root->right; free(root); return temp; } else if (root->right == NULL) { struct node *temp = root->left; free(root); return temp; } struct node* temp = minValueNode(root->right); root->key = temp->key; root->right = deleteNode(root->right, temp->key); } return root; } Time Complexity: O(h)... skewed tree -> O(n) Optimization In the above recursive code, we recursively call delete() for successor. We can avoid recursive call by keeping track of parent node of successor so that we can simply remove the successor by making child of parent as NULL. We know that successor would always be a leaf node. Node* deleteNode(Node* root, int k) { if (root == NULL) return root; if (root->key > k) { root->left = deleteNode(root->left, k); return root; } else if (root->key < k) { root->right = deleteNode(root->right, k); return root; } if (root->left == NULL) { Node* temp = root->right; delete root; return temp; } else if (root->right == NULL) { Node* temp = root->left; delete root; return temp; } else { Node* succParent = root; Node *succ = root->right; while (succ->left != NULL) { succParent = succ; succ = succ->left; } if (succParent != root) succParent->left = succ->right; else succParent->right = succ->right; root->key = succ->key; delete succ; return root; } } MinValue \u00b6 O(n) Worst case happens for left skewed trees. Similarly we can get the maximum value by recursively traversing the right node of a binary search tree. struct node * minValueNode(struct node* node) { struct node* current = node; while (current && current->left != NULL) current = current->left; return current; } Inorder predecessor and successor for a given key in BST \u00b6 void findPreSuc(Node* root, Node*& pre, Node*& suc, int key) { if (root == NULL) return ; if (root->key == key) { if (root->left != NULL) { Node* tmp = root->left; while (tmp->right) tmp = tmp->right; pre = tmp ; } if (root->right != NULL) { Node* tmp = root->right ; while (tmp->left) tmp = tmp->left ; suc = tmp ; } return ; } if (root->key > key) { suc = root ; findPreSuc(root->left, pre, suc, key) ; } else // go to right subtree { pre = root ; findPreSuc(root->right, pre, suc, key) ; } } Another Approach : We can also find the inorder successor and inorder predecessor using inorder traversal . Check if the current node is smaller than the given key for predecessor and for successor, check if it is greater than the given key . If it is greater than the given key then, check if it is smaller than the already stored value in successor then, update it . At last, get the predecessor and successor stored in q(successor) and p(predecessor). void find_p_s(Node* root,int a, Node** p, Node** q) { if(!root) return ; find_p_s(root->left, a, p, q); if(root&&root->data > a) { if((!*q) || (*q) && (*q)->data > root->data) *q = root; } else if(root && root->data < a) { *p = root; } find_p_s(root->right, a, p, q); }","title":"Binary Search Tree"},{"location":"ds/tree/bst.html#binary-search-tree","text":"","title":"Binary Search Tree"},{"location":"ds/tree/bst.html#insertion","text":"struct node* insert(struct node* node, int key) { if (node == NULL) return newNode(key); if (key < node->key) node->left = insert(node->left, key); else node->right = insert(node->right, key); return node; }","title":"Insertion"},{"location":"ds/tree/bst.html#min-value","text":"struct node* minValueNode(struct node* node) { struct node* current = node; while (current && current->left != NULL) current = current->left; return current; }","title":"Min Value"},{"location":"ds/tree/bst.html#deletion","text":"struct node* deleteNode(struct node* root, int key) { if (root == NULL) return root; if (key < root->key) root->left = deleteNode(root->left, key); else if (key > root->key) root->right = deleteNode(root->right, key); else { if (root.left==NULL and root.right==NULL): return NULL elif (root->left == NULL) { struct node* temp = root->right; free(root); return temp; } else if (root->right == NULL) { struct node* temp = root->left; free(root); return temp; } struct node* temp = minValueNode(root->right); root->key = temp->key; root->right = deleteNode(root->right, temp->key); } return root; } node-based binary tree data structure The left subtree of a node contains only nodes with keys lesser than the node\u2019s key. The right subtree of a node contains only nodes with keys greater than the node\u2019s key. The left and right subtree -> binary search tree. no duplicate nodes.","title":"Deletion"},{"location":"ds/tree/bst.html#facts","text":"Inorder traversal -> sorted output. We can construct a BST with only Preorder or Postorder or Level Order traversal. Note that we can always get inorder traversal by sorting the only given traversal. Number of unique BSTs with n distinct keys is Catalan Number Each node (item in the tree) has a distinct key.","title":"Facts"},{"location":"ds/tree/bst.html#search","text":"struct node* search(struct node* root, int key) { if (root == NULL || root->key == key) return root; if (root->key < key) return search(root->right, key); return search(root->left, key); }","title":"Search"},{"location":"ds/tree/bst.html#insert","text":"BST* BST ::Insert(BST* root, int value) { if (!root) { return new BST(value); } if (value > root->data) { root->right = Insert(root->right, value); } else { root->left = Insert(root->left, value); } return root; }","title":"Insert"},{"location":"ds/tree/bst.html#deletion_1","text":"leaf -> simply delete one child -> put it in the deleted place two child -> place the inorder successor or predeccesor in its place The important thing to note is, inorder successor is needed only when right child is not empty. In this particular case, inorder successor can be obtained by finding the minimum value in right child of the node. struct node* deleteNode(struct node* root, int key) { if (root == NULL) return root; if (key < root->key) root->left = deleteNode(root->left, key); else if (key > root->key) root->right = deleteNode(root->right, key); else { if (root->left == NULL) { struct node *temp = root->right; free(root); return temp; } else if (root->right == NULL) { struct node *temp = root->left; free(root); return temp; } struct node* temp = minValueNode(root->right); root->key = temp->key; root->right = deleteNode(root->right, temp->key); } return root; } Time Complexity: O(h)... skewed tree -> O(n) Optimization In the above recursive code, we recursively call delete() for successor. We can avoid recursive call by keeping track of parent node of successor so that we can simply remove the successor by making child of parent as NULL. We know that successor would always be a leaf node. Node* deleteNode(Node* root, int k) { if (root == NULL) return root; if (root->key > k) { root->left = deleteNode(root->left, k); return root; } else if (root->key < k) { root->right = deleteNode(root->right, k); return root; } if (root->left == NULL) { Node* temp = root->right; delete root; return temp; } else if (root->right == NULL) { Node* temp = root->left; delete root; return temp; } else { Node* succParent = root; Node *succ = root->right; while (succ->left != NULL) { succParent = succ; succ = succ->left; } if (succParent != root) succParent->left = succ->right; else succParent->right = succ->right; root->key = succ->key; delete succ; return root; } }","title":"Deletion"},{"location":"ds/tree/bst.html#minvalue","text":"O(n) Worst case happens for left skewed trees. Similarly we can get the maximum value by recursively traversing the right node of a binary search tree. struct node * minValueNode(struct node* node) { struct node* current = node; while (current && current->left != NULL) current = current->left; return current; }","title":"MinValue"},{"location":"ds/tree/bst.html#inorder-predecessor-and-successor-for-a-given-key-in-bst","text":"void findPreSuc(Node* root, Node*& pre, Node*& suc, int key) { if (root == NULL) return ; if (root->key == key) { if (root->left != NULL) { Node* tmp = root->left; while (tmp->right) tmp = tmp->right; pre = tmp ; } if (root->right != NULL) { Node* tmp = root->right ; while (tmp->left) tmp = tmp->left ; suc = tmp ; } return ; } if (root->key > key) { suc = root ; findPreSuc(root->left, pre, suc, key) ; } else // go to right subtree { pre = root ; findPreSuc(root->right, pre, suc, key) ; } } Another Approach : We can also find the inorder successor and inorder predecessor using inorder traversal . Check if the current node is smaller than the given key for predecessor and for successor, check if it is greater than the given key . If it is greater than the given key then, check if it is smaller than the already stored value in successor then, update it . At last, get the predecessor and successor stored in q(successor) and p(predecessor). void find_p_s(Node* root,int a, Node** p, Node** q) { if(!root) return ; find_p_s(root->left, a, p, q); if(root&&root->data > a) { if((!*q) || (*q) && (*q)->data > root->data) *q = root; } else if(root && root->data < a) { *p = root; } find_p_s(root->right, a, p, q); }","title":"Inorder predecessor and successor for a given key in BST"},{"location":"ds/tree/bt-deletion.html","text":"Deletion \u00b6 void deletDeepest(struct Node* root, struct Node* d_node) { queue<struct Node*> q; q.push(root); struct Node* temp; while (!q.empty()) { temp = q.front(); q.pop(); if (temp == d_node) { temp = NULL; delete (d_node); return; } if (temp->right) { if (temp->right == d_node) { temp->right = NULL; delete (d_node); return; } else q.push(temp->right); } if (temp->left) { if (temp->left == d_node) { temp->left = NULL; delete (d_node); return; } else q.push(temp->left); } } } Node* deletion(struct Node* root, int key) { if (root == NULL) return NULL; if (root->left == NULL && root->right == NULL) { if (root->key == key) return NULL; else return root; } queue<struct Node*> q; q.push(root); struct Node* temp; struct Node* key_node = NULL; while (!q.empty()) { temp = q.front(); q.pop(); if (temp->key == key) key_node = temp; if (temp->left) q.push(temp->left); if (temp->right) q.push(temp->right); } if (key_node != NULL) { int x = temp->key; deletDeepest(root, temp); key_node->key = x; } return root; }","title":"Deletion"},{"location":"ds/tree/bt-deletion.html#deletion","text":"void deletDeepest(struct Node* root, struct Node* d_node) { queue<struct Node*> q; q.push(root); struct Node* temp; while (!q.empty()) { temp = q.front(); q.pop(); if (temp == d_node) { temp = NULL; delete (d_node); return; } if (temp->right) { if (temp->right == d_node) { temp->right = NULL; delete (d_node); return; } else q.push(temp->right); } if (temp->left) { if (temp->left == d_node) { temp->left = NULL; delete (d_node); return; } else q.push(temp->left); } } } Node* deletion(struct Node* root, int key) { if (root == NULL) return NULL; if (root->left == NULL && root->right == NULL) { if (root->key == key) return NULL; else return root; } queue<struct Node*> q; q.push(root); struct Node* temp; struct Node* key_node = NULL; while (!q.empty()) { temp = q.front(); q.pop(); if (temp->key == key) key_node = temp; if (temp->left) q.push(temp->left); if (temp->right) q.push(temp->right); } if (key_node != NULL) { int x = temp->key; deletDeepest(root, temp); key_node->key = x; } return root; }","title":"Deletion"},{"location":"ds/tree/bt-enumeration.html","text":"Binary Tree Enumeration \u00b6 unlabeled trees \u00b6 o o / \\ / \\ o o o o n\u2019th Catalan Numbers For example, let T(n) be count for n nodes. T(0) = 1 [There is only 1 empty tree] T(1) = 1 T(2) = 2 T(3) = T(0)*T(2) + T(1)*T(1) + T(2)*T(0) = 1*2 + 1*1 + 2*1 = 5 T(4) = T(0)*T(3) + T(1)*T(2) + T(2)*T(1) + T(3)*T(0) = 1*5 + 1*2 + 2*1 + 5*1 = 14 T(n) = sum(T(i-1) * T(n-i)) = sum(T(n-i-1) * T(i)) = Cn T(i-1) represents number of nodes on the left-sub-tree T(n\u2212i-1) represents number of nodes on the right-sub-tree T(n) = (2n)! / (n+1)!n! labeled trees \u00b6 A C / \\ / \\ B C A B T(n) = (Number of unlabeled trees) * n! = [(2n)! / (n+1)!n!] \u00d7 n!","title":"Binary Tree Enumeration"},{"location":"ds/tree/bt-enumeration.html#binary-tree-enumeration","text":"","title":"Binary Tree Enumeration"},{"location":"ds/tree/bt-enumeration.html#unlabeled-trees","text":"o o / \\ / \\ o o o o n\u2019th Catalan Numbers For example, let T(n) be count for n nodes. T(0) = 1 [There is only 1 empty tree] T(1) = 1 T(2) = 2 T(3) = T(0)*T(2) + T(1)*T(1) + T(2)*T(0) = 1*2 + 1*1 + 2*1 = 5 T(4) = T(0)*T(3) + T(1)*T(2) + T(2)*T(1) + T(3)*T(0) = 1*5 + 1*2 + 2*1 + 5*1 = 14 T(n) = sum(T(i-1) * T(n-i)) = sum(T(n-i-1) * T(i)) = Cn T(i-1) represents number of nodes on the left-sub-tree T(n\u2212i-1) represents number of nodes on the right-sub-tree T(n) = (2n)! / (n+1)!n!","title":"unlabeled trees"},{"location":"ds/tree/bt-enumeration.html#labeled-trees","text":"A C / \\ / \\ B C A B T(n) = (Number of unlabeled trees) * n! = [(2n)! / (n+1)!n!] \u00d7 n!","title":"labeled trees"},{"location":"ds/tree/bt-height.html","text":"Height \u00b6 int height(node* node) { if (node == NULL) return 0; else { int lheight = height(node->left); int rheight = height(node->right); if (lheight > rheight) return(lheight + 1); else return(rheight + 1); } }","title":"Height"},{"location":"ds/tree/bt-height.html#height","text":"int height(node* node) { if (node == NULL) return 0; else { int lheight = height(node->left); int rheight = height(node->right); if (lheight > rheight) return(lheight + 1); else return(rheight + 1); } }","title":"Height"},{"location":"ds/tree/bt-insertion.html","text":"Insertion \u00b6 Node* InsertNode(Node* root, int data) { if (root == NULL) { root = CreateNode(data); return root; } queue<Node*> q; q.push(root); while (!q.empty()) { Node* temp = q.front(); q.pop(); if (temp->left != NULL) q.push(temp->left); else { temp->left = CreateNode(data); return root; } if (temp->right != NULL) q.push(temp->right); else { temp->right = CreateNode(data); return root; } } }","title":"Insertion"},{"location":"ds/tree/bt-insertion.html#insertion","text":"Node* InsertNode(Node* root, int data) { if (root == NULL) { root = CreateNode(data); return root; } queue<Node*> q; q.push(root); while (!q.empty()) { Node* temp = q.front(); q.pop(); if (temp->left != NULL) q.push(temp->left); else { temp->left = CreateNode(data); return root; } if (temp->right != NULL) q.push(temp->right); else { temp->right = CreateNode(data); return root; } } }","title":"Insertion"},{"location":"ds/tree/bt-level-order.html","text":"Level Order \u00b6 Queue \u00b6 void printLevelOrderWithQueue(Node *root) { if (root == NULL) return; queue<Node *> q; q.push(root); while (q.empty() == false) { Node *node = q.front(); cout << node->data << \" \"; q.pop(); if (node->left != NULL) q.push(node->left); if (node->right != NULL) q.push(node->right); } } Reverse \u00b6 void reverseLevelOrder(node* root) { int h = height(root); int i; for (i=h; i>=1; i--) //THE ONLY LINE DIFFERENT FROM NORMAL LEVEL ORDER printGivenLevel(root, i); } void reverseLevelOrderWithStack(node* root) { stack <node *> S; queue <node *> Q; Q.push(root); while (Q.empty() == false) { root = Q.front(); Q.pop(); S.push(root); if (root->right) Q.push(root->right); if (root->left) Q.push(root->left); } while (S.empty() == false) { root = S.top(); cout << root->data << \" \"; S.pop(); } } Given Level \u00b6 void printLevelOrder(node* root) { int h = height(root); int i; for (i = 1; i <= h; i++) printGivenLevel(root, i); } void printGivenLevel(node* root, int level) { if (root == NULL) return; if (level == 1) cout << root->data << \" \"; else if (level > 1) { printGivenLevel(root->left, level-1); printGivenLevel(root->right, level-1); } }","title":"Level Order"},{"location":"ds/tree/bt-level-order.html#level-order","text":"","title":"Level Order"},{"location":"ds/tree/bt-level-order.html#queue","text":"void printLevelOrderWithQueue(Node *root) { if (root == NULL) return; queue<Node *> q; q.push(root); while (q.empty() == false) { Node *node = q.front(); cout << node->data << \" \"; q.pop(); if (node->left != NULL) q.push(node->left); if (node->right != NULL) q.push(node->right); } }","title":"Queue"},{"location":"ds/tree/bt-level-order.html#reverse","text":"void reverseLevelOrder(node* root) { int h = height(root); int i; for (i=h; i>=1; i--) //THE ONLY LINE DIFFERENT FROM NORMAL LEVEL ORDER printGivenLevel(root, i); } void reverseLevelOrderWithStack(node* root) { stack <node *> S; queue <node *> Q; Q.push(root); while (Q.empty() == false) { root = Q.front(); Q.pop(); S.push(root); if (root->right) Q.push(root->right); if (root->left) Q.push(root->left); } while (S.empty() == false) { root = S.top(); cout << root->data << \" \"; S.pop(); } }","title":"Reverse"},{"location":"ds/tree/bt-level-order.html#given-level","text":"void printLevelOrder(node* root) { int h = height(root); int i; for (i = 1; i <= h; i++) printGivenLevel(root, i); } void printGivenLevel(node* root, int level) { if (root == NULL) return; if (level == 1) cout << root->data << \" \"; else if (level > 1) { printGivenLevel(root->left, level-1); printGivenLevel(root->right, level-1); } }","title":"Given Level"},{"location":"ds/tree/bt-search.html","text":"Search \u00b6 bool iterativeSearch(node *root, int x) { if (root == NULL) return false; queue<node *> q; q.push(root); while (q.empty() == false) { node *node = q.front(); if (node->data == x) return true; q.pop(); if (node->left != NULL) q.push(node->left); if (node->right != NULL) q.push(node->right); } return false; }","title":"Search"},{"location":"ds/tree/bt-search.html#search","text":"bool iterativeSearch(node *root, int x) { if (root == NULL) return false; queue<node *> q; q.push(root); while (q.empty() == false) { node *node = q.front(); if (node->data == x) return true; q.pop(); if (node->left != NULL) q.push(node->left); if (node->right != NULL) q.push(node->right); } return false; }","title":"Search"},{"location":"ds/tree/bt-swap-nodes.html","text":"Binary Tree Swapping \u00b6 Store inorder of BST in auxiliary array. (will be sorted) Sort the auxiliary array. Finally, insert the auxiliary array elements back to the BST, keeping the structure of the BST same. The time complexity of this method is O(nLogn) and the auxiliary space needed is O(n). We can solve this in O(n) time and with a single traversal of the given BST. Since inorder traversal of BST is always a sorted array, the problem can be reduced to a problem where two elements of a sorted array are swapped. There are two cases that we need to handle: The swapped nodes are not adjacent in the inorder traversal of the BST. For example, Nodes 5 and 25 are swapped in {3 5 7 8 10 15 20 25}. The inorder traversal of the given tree is 3 25 7 8 10 15 20 5 If we observe carefully, during inorder traversal, we find node 7 is smaller than the previous visited node 25. Here save the context of node 25 (previous node). Again, we find that node 5 is smaller than the previous node 20. This time, we save the context of node 5 (the current node ). Finally, swap the two node\u2019s values. The swapped nodes are adjacent in the inorder traversal of BST. For example, Nodes 7 and 8 are swapped in {3 5 7 8 10 15 20 25}. The inorder traversal of the given tree is 3 5 8 7 10 15 20 25 Unlike case #1, here only one point exists where a node value is smaller than the previous node value. e.g. node 7 is smaller than node 8. How to Solve? We will maintain three-pointers, first, middle, and last. When we find the first point where the current node value is smaller than the previous node value, we update the first with the previous node & the middle with the current node. When we find the second point where the current node value is smaller than the previous node value, we update the last with the current node. In the case of #2, we will never find the second point. So, the last pointer will not be updated. After processing, if the last node value is null, then two swapped nodes of BST are adjacent.","title":"Binary Tree Swapping"},{"location":"ds/tree/bt-swap-nodes.html#binary-tree-swapping","text":"Store inorder of BST in auxiliary array. (will be sorted) Sort the auxiliary array. Finally, insert the auxiliary array elements back to the BST, keeping the structure of the BST same. The time complexity of this method is O(nLogn) and the auxiliary space needed is O(n). We can solve this in O(n) time and with a single traversal of the given BST. Since inorder traversal of BST is always a sorted array, the problem can be reduced to a problem where two elements of a sorted array are swapped. There are two cases that we need to handle: The swapped nodes are not adjacent in the inorder traversal of the BST. For example, Nodes 5 and 25 are swapped in {3 5 7 8 10 15 20 25}. The inorder traversal of the given tree is 3 25 7 8 10 15 20 5 If we observe carefully, during inorder traversal, we find node 7 is smaller than the previous visited node 25. Here save the context of node 25 (previous node). Again, we find that node 5 is smaller than the previous node 20. This time, we save the context of node 5 (the current node ). Finally, swap the two node\u2019s values. The swapped nodes are adjacent in the inorder traversal of BST. For example, Nodes 7 and 8 are swapped in {3 5 7 8 10 15 20 25}. The inorder traversal of the given tree is 3 5 8 7 10 15 20 25 Unlike case #1, here only one point exists where a node value is smaller than the previous node value. e.g. node 7 is smaller than node 8. How to Solve? We will maintain three-pointers, first, middle, and last. When we find the first point where the current node value is smaller than the previous node value, we update the first with the previous node & the middle with the current node. When we find the second point where the current node value is smaller than the previous node value, we update the last with the current node. In the case of #2, we will never find the second point. So, the last pointer will not be updated. After processing, if the last node value is null, then two swapped nodes of BST are adjacent.","title":"Binary Tree Swapping"},{"location":"ds/tree/handshaking-lemma.html","text":"In every finite undirected graph, an even number of vertices will always have odd degree : The handshaking lemma is a consequence of the degree sum formula (also sometimes called the handshaking lemma): sum(deg(V)) = 2(E) k-ary tree where every node has either 0 or k children \u00b6 L = (k - 1)*I + 1 Where L = Number of leaf nodes I = Number of internal nodes Proof \u00b6 With handshaking lemma \u00b6 Proof can be divided in two cases. Case 1 (Root is Leaf):There is only one node in tree. The above formula is true for single node as L = 1, I = 0. Case 2 (Root is Internal Node): For trees with more than 1 nodes, root is always internal node. The above formula can be proved using Handshaking Lemma for this case. A tree is an undirected acyclic graph. Total number of edges in Tree is number of nodes minus 1, i.e., |E| = L + I \u2013 1. All internal nodes except root in the given type of tree have degree k + 1. Root has degree k. All leaves have degree 1. Applying the Handshaking lemma to such trees, we get following relation. Sum of all degrees = 2 * (Sum of Edges) Sum of degrees of leaves + Sum of degrees for Internal Node except root + Root's degree = 2 * (No. of nodes - 1) Putting values of above terms, L + (I-1)*(k+1) + k = 2 * (L + I - 1) L + k*I - k + I -1 + k = 2*L + 2I - 2 L + K*I + I - 1 = 2*L + 2*I - 2 K*I + 1 - I = L (K-1)*I + 1 = L So the above property is proved using Handshaking Lemma, let us discuss one more interesting property. Without it \u00b6 Since there are I internal nodes, each having K children, therefore total children in the tree = K * I. There are I-1 internal nodes which are children of some other node (root has been excluded hence one less than the total number of internal nodes) That is, out of these K I children, I-1 are internal nodes and therefore the rest (K I \u2013 (I-1)) are leaves. Hence L = (K-1)*I + 1. In Binary tree, \u00b6 L = T + 1 Where L = Number of leaf nodes T = Number of internal nodes with two children Proof \u00b6 Case 1 \u00b6 T = 0 L = 1 Case 2 \u00b6 Root has two children, i.e., degree of root is 2. Sum of degrees of nodes with two children except root + Sum of degrees of nodes with one child + Sum of degrees of leaves + Root's degree = 2 * (No. of Nodes - 1) Putting values of above terms, (T-1)*3 + S*2 + L + 2 = (S + T + L - 1)*2 Cancelling 2S from both sides. (T-1)*3 + L + 2 = (T + L - 1)*2 T - 1 = L - 2 T = L - 1 Case 3: \u00b6 Root has one child, i.e., degree of root is 1. Sum of degrees of nodes with two children + Sum of degrees of nodes with one child except root + Sum of degrees of leaves + Root's degree = 2 * (No. of Nodes - 1) Putting values of above terms, T*3 + (S-1)*2 + L + 1 = (S + T + L - 1)*2 Cancelling 2S from both sides. 3*T + L -1 = 2*T + 2*L - 2 T - 1 = L - 2 T = L - 1","title":"Handshaking lemma"},{"location":"ds/tree/handshaking-lemma.html#k-ary-tree-where-every-node-has-either-0-or-k-children","text":"L = (k - 1)*I + 1 Where L = Number of leaf nodes I = Number of internal nodes","title":"k-ary tree where every node has either 0 or k children"},{"location":"ds/tree/handshaking-lemma.html#proof","text":"","title":"Proof"},{"location":"ds/tree/handshaking-lemma.html#with-handshaking-lemma","text":"Proof can be divided in two cases. Case 1 (Root is Leaf):There is only one node in tree. The above formula is true for single node as L = 1, I = 0. Case 2 (Root is Internal Node): For trees with more than 1 nodes, root is always internal node. The above formula can be proved using Handshaking Lemma for this case. A tree is an undirected acyclic graph. Total number of edges in Tree is number of nodes minus 1, i.e., |E| = L + I \u2013 1. All internal nodes except root in the given type of tree have degree k + 1. Root has degree k. All leaves have degree 1. Applying the Handshaking lemma to such trees, we get following relation. Sum of all degrees = 2 * (Sum of Edges) Sum of degrees of leaves + Sum of degrees for Internal Node except root + Root's degree = 2 * (No. of nodes - 1) Putting values of above terms, L + (I-1)*(k+1) + k = 2 * (L + I - 1) L + k*I - k + I -1 + k = 2*L + 2I - 2 L + K*I + I - 1 = 2*L + 2*I - 2 K*I + 1 - I = L (K-1)*I + 1 = L So the above property is proved using Handshaking Lemma, let us discuss one more interesting property.","title":"With handshaking lemma"},{"location":"ds/tree/handshaking-lemma.html#without-it","text":"Since there are I internal nodes, each having K children, therefore total children in the tree = K * I. There are I-1 internal nodes which are children of some other node (root has been excluded hence one less than the total number of internal nodes) That is, out of these K I children, I-1 are internal nodes and therefore the rest (K I \u2013 (I-1)) are leaves. Hence L = (K-1)*I + 1.","title":"Without it"},{"location":"ds/tree/handshaking-lemma.html#in-binary-tree","text":"L = T + 1 Where L = Number of leaf nodes T = Number of internal nodes with two children","title":"In Binary tree,"},{"location":"ds/tree/handshaking-lemma.html#proof_1","text":"","title":"Proof"},{"location":"ds/tree/handshaking-lemma.html#case-1","text":"T = 0 L = 1","title":"Case 1"},{"location":"ds/tree/handshaking-lemma.html#case-2","text":"Root has two children, i.e., degree of root is 2. Sum of degrees of nodes with two children except root + Sum of degrees of nodes with one child + Sum of degrees of leaves + Root's degree = 2 * (No. of Nodes - 1) Putting values of above terms, (T-1)*3 + S*2 + L + 2 = (S + T + L - 1)*2 Cancelling 2S from both sides. (T-1)*3 + L + 2 = (T + L - 1)*2 T - 1 = L - 2 T = L - 1","title":"Case 2"},{"location":"ds/tree/handshaking-lemma.html#case-3","text":"Root has one child, i.e., degree of root is 1. Sum of degrees of nodes with two children + Sum of degrees of nodes with one child except root + Sum of degrees of leaves + Root's degree = 2 * (No. of Nodes - 1) Putting values of above terms, T*3 + (S-1)*2 + L + 1 = (S + T + L - 1)*2 Cancelling 2S from both sides. 3*T + L -1 = 2*T + 2*L - 2 T - 1 = L - 2 T = L - 1","title":"Case 3:"},{"location":"ds/tree/intro.html","text":"Tree \u00b6 Hierarchal DS ---- j <-- root / \\ f k / \\ \\ a h z <-- leaves f, k -> siblings. k is parent of z, z is child of k. why? \u00b6 need hierarchal structure! eg - file system. moderate access/search (quicker than Linked List and slower than arrays) moderate insertion/deletion (quicker than Arrays and slower than Unordered Linked Lists). no limit like linked list. Applications \u00b6 Manipulate hierarchical data. Make information easy to search aka tree traversal. Manipulate sorted lists of data. As a workflow for compositing digital images for visual effects. Router algorithms Form of a multi-stage decision-making (see business chess).","title":"Tree"},{"location":"ds/tree/intro.html#tree","text":"Hierarchal DS ---- j <-- root / \\ f k / \\ \\ a h z <-- leaves f, k -> siblings. k is parent of z, z is child of k.","title":"Tree"},{"location":"ds/tree/intro.html#why","text":"need hierarchal structure! eg - file system. moderate access/search (quicker than Linked List and slower than arrays) moderate insertion/deletion (quicker than Arrays and slower than Unordered Linked Lists). no limit like linked list.","title":"why?"},{"location":"ds/tree/intro.html#applications","text":"Manipulate hierarchical data. Make information easy to search aka tree traversal. Manipulate sorted lists of data. As a workflow for compositing digital images for visual effects. Router algorithms Form of a multi-stage decision-making (see business chess).","title":"Applications"},{"location":"ds/tree/k-smallest.html","text":"K Smallest \u00b6 Method 1: Using Inorder Traversal (O(n) time and O(h) auxiliary space) \u00b6 Node* kthSmallest(Node* root, int& k) { if (root == NULL) return NULL; Node* left = kthSmallest(root->left, k); if (left != NULL) return left; k--; if (k == 0) return root; return kthSmallest(root->right, k); } Method 2: Augmented Tree Data Structure (O(h) Time Complexity and O(h) auxiliary space) \u00b6 Time complexity: O(h) where h is the height of the tree. struct Node { int data; Node *left, *right; int lCount; Node(int x) { data = x; left = right = NULL; lCount = 0; } }; Node* insert(Node* root, int x) { if (root == NULL) return new Node(x); if (x < root->data) { root->left = insert(root->left, x); root->lCount++; } else if (x > root->data) root->right = insert(root->right, x); return root; } Node* kthSmallest(Node* root, int k) { if (root == NULL) return NULL; int count = root->lCount + 1; if (count == k) return root; if (count > k) return kthSmallest(root->left, k); return kthSmallest(root->right, k - count); }","title":"K Smallest"},{"location":"ds/tree/k-smallest.html#k-smallest","text":"","title":"K Smallest"},{"location":"ds/tree/k-smallest.html#method-1-using-inorder-traversal-on-time-and-oh-auxiliary-space","text":"Node* kthSmallest(Node* root, int& k) { if (root == NULL) return NULL; Node* left = kthSmallest(root->left, k); if (left != NULL) return left; k--; if (k == 0) return root; return kthSmallest(root->right, k); }","title":"Method 1: Using Inorder Traversal (O(n) time and O(h) auxiliary space)"},{"location":"ds/tree/k-smallest.html#method-2-augmented-tree-data-structure-oh-time-complexity-and-oh-auxiliary-space","text":"Time complexity: O(h) where h is the height of the tree. struct Node { int data; Node *left, *right; int lCount; Node(int x) { data = x; left = right = NULL; lCount = 0; } }; Node* insert(Node* root, int x) { if (root == NULL) return new Node(x); if (x < root->data) { root->left = insert(root->left, x); root->lCount++; } else if (x > root->data) root->right = insert(root->right, x); return root; } Node* kthSmallest(Node* root, int k) { if (root == NULL) return NULL; int count = root->lCount + 1; if (count == k) return root; if (count > k) return kthSmallest(root->left, k); return kthSmallest(root->right, k - count); }","title":"Method 2: Augmented Tree Data Structure (O(h) Time Complexity and O(h) auxiliary space)"},{"location":"ds/tree/lowest-common-ancestor.html","text":"Lowest Common Ancestor \u00b6 BST in, if node's value is greater than both n1 and n2 then our LCA lies in the left side of the node, if it's is smaller than both n1 and n2, then LCA lies on the right side. Otherwise, the root is LCA (assuming that both n1 and n2 are present in BST). Time Complexity: O(h). Space Complexity: O(1). node *lca(node* root, int n1, int n2) { if (root == NULL) return NULL; if (root->data > n1 && root->data > n2) return lca(root->left, n1, n2); if (root->data < n1 && root->data < n2) return lca(root->right, n1, n2); return root; } Iterative Implementation: \u00b6 The above solution uses recursion. The recursive solution requires extra space in the form of the function call stack. So an iterative solution can be implemented which does not occupy space in the form of the function call stack. Time Complexity: O(h). Space Complexity: O(1). struct node *lca(struct node* root, int n1, int n2) { while (root != NULL) { if (root->data > n1 && root->data > n2) root = root->left; else if (root->data < n1 && root->data < n2) root = root->right; else break; } return root; } Exercise The above functions assume that n1 and n2 both are in BST. If n1 and n2 are not present, then they may return an incorrect result. Extend the above solutions to return NULL if n1 or n2 or both not present in BST.","title":"Lowest Common Ancestor"},{"location":"ds/tree/lowest-common-ancestor.html#lowest-common-ancestor","text":"BST in, if node's value is greater than both n1 and n2 then our LCA lies in the left side of the node, if it's is smaller than both n1 and n2, then LCA lies on the right side. Otherwise, the root is LCA (assuming that both n1 and n2 are present in BST). Time Complexity: O(h). Space Complexity: O(1). node *lca(node* root, int n1, int n2) { if (root == NULL) return NULL; if (root->data > n1 && root->data > n2) return lca(root->left, n1, n2); if (root->data < n1 && root->data < n2) return lca(root->right, n1, n2); return root; }","title":"Lowest Common Ancestor"},{"location":"ds/tree/lowest-common-ancestor.html#iterative-implementation","text":"The above solution uses recursion. The recursive solution requires extra space in the form of the function call stack. So an iterative solution can be implemented which does not occupy space in the form of the function call stack. Time Complexity: O(h). Space Complexity: O(1). struct node *lca(struct node* root, int n1, int n2) { while (root != NULL) { if (root->data > n1 && root->data > n2) root = root->left; else if (root->data < n1 && root->data < n2) root = root->right; else break; } return root; } Exercise The above functions assume that n1 and n2 both are in BST. If n1 and n2 are not present, then they may return an incorrect result. Extend the above solutions to return NULL if n1 or n2 or both not present in BST.","title":"Iterative Implementation:"},{"location":"ds/tree/segment-tree.html","text":"TODO","title":"Segment tree"},{"location":"ds/tree/traversal.html","text":"Traversal \u00b6 linear data structure -> only one logical way to traverse them, trees can be traversed in different ways 1 / \\ 2 3 / \\ 4 5 Depth First Traversals: Inorder (Left, Root, Right) : 4 2 5 1 3 Preorder (Root, Left, Right) : 1 2 4 5 3 Postorder (Left, Right, Root) : 4 5 2 3 1 Breadth First or Level Order Traversal : 1 2 3 4 5 Depth First Traversals \u00b6 Time Complexity(O(n)): T(n) = T(k) + T(n \u2013 k \u2013 1) + c k -> number of nodes on one side of root n-k-1 on the other side Analysis of boundary conditions: Case 1: Skewed tree (One of the subtrees is empty and other subtree is non-empty) k is 0 in this case. T(n) = T(0) + T(n-1) + c T(n) = 2T(0) + T(n-2) + 2c T(n) = 3T(0) + T(n-3) + 3c T(n) = 4T(0) + T(n-4) + 4c T(n) = (n-1)T(0) + T(1) + (n-1)c T(n) = nT(0) + (n)c Value of T(0) will be some constant say d. (traversing a empty tree will take some constants time) T(n) = n(c+d) T(n) = \u0398(n) Case 2: Both left and right subtrees have equal number of nodes. T(n) = 2T(n/2) + c This recursive function is in the standard form (T(n) = aT(n/b) + \u0398(n)) for master method Auxiliary Space: size of stack -> O(n). Inorder \u00b6 void inorder(Node* temp) { if (temp == NULL) return; inorder(temp->left); cout << temp->data << ' '; inorder(temp->right); } Application: To get nodes of BST in non-increasing order Preorder \u00b6 Application: to create a copy of the tree. to get prefix expression on of an expression tree. void preorder(Node* temp) { if (temp == NULL) return; cout << temp->data << ' '; preorder(temp->left); preorder(temp->right); } Postorder \u00b6 void postorder(Node* temp) { if (temp == NULL) return; postorder(temp->left); postorder(temp->right); cout << temp->data << ' '; } Application: delete the tree. the postfix expression of an expression tree. BFS \u00b6 using call stack \u00b6 void printLevelOrder(node* root) { int h = height(root); int i; for (i = 1; i <= h; i++) printGivenLevel(root, i); } void printGivenLevel(node* root, int level) { if (root == NULL) return; if (level == 1) cout << root->data << \" \"; else if (level > 1) { printGivenLevel(root->left, level-1); printGivenLevel(root->right, level-1); } } Time Complexity: O(n^2) in worst case. For a skewed tree, printGivenLevel() takes O(n) time where n is the number of nodes in the skewed tree. So time complexity of printLevelOrder() is O(n) + O(n-1) + O(n-2) + .. + O(1) which is O(n^2). Space Complexity: O(n) in worst case. For a skewed tree, printGivenLevel() uses O(n) space for call stack. For a Balanced tree, call stack uses O(log n) space, (i.e., height of the balanced tree). using queues \u00b6 void printLevelOrder(Node *root) { if (root == NULL) return; queue<Node *> q; q.push(root); while (q.empty() == false) { Node *node = q.front(); cout << node->data << \" \"; q.pop(); if (node->left != NULL) q.push(node->left); if (node->right != NULL) q.push(node->right); } } Time Complexity: O(n) where n is number of nodes in the binary tree Space Complexity: O(n) where n is number of nodes in the binary tree BFS vs DFS \u00b6 Time complexity \u00b6 same -> O(n) Space Complexity \u00b6 Level Order Traversal is O(w) where w is maximum width of Binary Tree -> coz using queues Depth First Traversals is O(h) where h is maximum height of Binary Tree -> coz using stack (or function call stack) stores all ancestors of a node. Maximum Width of BT -> 2^h where h starts from 0. (height/width) worst case -> perfect binry tree -> Ceil(n/2). Height of BT is O(Log n). Worst case occurs -> skewed tree == O(n). worst case for both -> O(n) But, if(more balanced) -> DFS is better than BFS else BFS is better than DFS How to Pick One? \u00b6 Extra Space DFS -> recursive thus function call overheads. Find closer to root -> BFS ... Find closer to leaves -> DFS","title":"Traversal"},{"location":"ds/tree/traversal.html#traversal","text":"linear data structure -> only one logical way to traverse them, trees can be traversed in different ways 1 / \\ 2 3 / \\ 4 5 Depth First Traversals: Inorder (Left, Root, Right) : 4 2 5 1 3 Preorder (Root, Left, Right) : 1 2 4 5 3 Postorder (Left, Right, Root) : 4 5 2 3 1 Breadth First or Level Order Traversal : 1 2 3 4 5","title":"Traversal"},{"location":"ds/tree/traversal.html#depth-first-traversals","text":"Time Complexity(O(n)): T(n) = T(k) + T(n \u2013 k \u2013 1) + c k -> number of nodes on one side of root n-k-1 on the other side Analysis of boundary conditions: Case 1: Skewed tree (One of the subtrees is empty and other subtree is non-empty) k is 0 in this case. T(n) = T(0) + T(n-1) + c T(n) = 2T(0) + T(n-2) + 2c T(n) = 3T(0) + T(n-3) + 3c T(n) = 4T(0) + T(n-4) + 4c T(n) = (n-1)T(0) + T(1) + (n-1)c T(n) = nT(0) + (n)c Value of T(0) will be some constant say d. (traversing a empty tree will take some constants time) T(n) = n(c+d) T(n) = \u0398(n) Case 2: Both left and right subtrees have equal number of nodes. T(n) = 2T(n/2) + c This recursive function is in the standard form (T(n) = aT(n/b) + \u0398(n)) for master method Auxiliary Space: size of stack -> O(n).","title":"Depth First Traversals"},{"location":"ds/tree/traversal.html#inorder","text":"void inorder(Node* temp) { if (temp == NULL) return; inorder(temp->left); cout << temp->data << ' '; inorder(temp->right); } Application: To get nodes of BST in non-increasing order","title":"Inorder"},{"location":"ds/tree/traversal.html#preorder","text":"Application: to create a copy of the tree. to get prefix expression on of an expression tree. void preorder(Node* temp) { if (temp == NULL) return; cout << temp->data << ' '; preorder(temp->left); preorder(temp->right); }","title":"Preorder"},{"location":"ds/tree/traversal.html#postorder","text":"void postorder(Node* temp) { if (temp == NULL) return; postorder(temp->left); postorder(temp->right); cout << temp->data << ' '; } Application: delete the tree. the postfix expression of an expression tree.","title":"Postorder"},{"location":"ds/tree/traversal.html#bfs","text":"","title":"BFS"},{"location":"ds/tree/traversal.html#using-call-stack","text":"void printLevelOrder(node* root) { int h = height(root); int i; for (i = 1; i <= h; i++) printGivenLevel(root, i); } void printGivenLevel(node* root, int level) { if (root == NULL) return; if (level == 1) cout << root->data << \" \"; else if (level > 1) { printGivenLevel(root->left, level-1); printGivenLevel(root->right, level-1); } } Time Complexity: O(n^2) in worst case. For a skewed tree, printGivenLevel() takes O(n) time where n is the number of nodes in the skewed tree. So time complexity of printLevelOrder() is O(n) + O(n-1) + O(n-2) + .. + O(1) which is O(n^2). Space Complexity: O(n) in worst case. For a skewed tree, printGivenLevel() uses O(n) space for call stack. For a Balanced tree, call stack uses O(log n) space, (i.e., height of the balanced tree).","title":"using call stack"},{"location":"ds/tree/traversal.html#using-queues","text":"void printLevelOrder(Node *root) { if (root == NULL) return; queue<Node *> q; q.push(root); while (q.empty() == false) { Node *node = q.front(); cout << node->data << \" \"; q.pop(); if (node->left != NULL) q.push(node->left); if (node->right != NULL) q.push(node->right); } } Time Complexity: O(n) where n is number of nodes in the binary tree Space Complexity: O(n) where n is number of nodes in the binary tree","title":"using queues"},{"location":"ds/tree/traversal.html#bfs-vs-dfs","text":"","title":"BFS vs DFS"},{"location":"ds/tree/traversal.html#time-complexity","text":"same -> O(n)","title":"Time complexity"},{"location":"ds/tree/traversal.html#space-complexity","text":"Level Order Traversal is O(w) where w is maximum width of Binary Tree -> coz using queues Depth First Traversals is O(h) where h is maximum height of Binary Tree -> coz using stack (or function call stack) stores all ancestors of a node. Maximum Width of BT -> 2^h where h starts from 0. (height/width) worst case -> perfect binry tree -> Ceil(n/2). Height of BT is O(Log n). Worst case occurs -> skewed tree == O(n). worst case for both -> O(n) But, if(more balanced) -> DFS is better than BFS else BFS is better than DFS","title":"Space Complexity"},{"location":"ds/tree/traversal.html#how-to-pick-one","text":"Extra Space DFS -> recursive thus function call overheads. Find closer to root -> BFS ... Find closer to leaves -> DFS","title":"How to Pick One?"},{"location":"game-dev/compression.html","text":"Compression \u00b6 krinkels forums tools algos file types loading into ram access","title":"Compression"},{"location":"game-dev/compression.html#compression","text":"krinkels forums tools algos file types loading into ram access","title":"Compression"},{"location":"game-dev/modding.html","text":"","title":"Modding"},{"location":"game-dev/reverse-engineering.html","text":"","title":"Reverse engineering"},{"location":"graphics/bresenham.html","text":"Bresenham \u00b6 #include<stdio.h> #include<graphics.h> void drawline(int x0, int y0, int x1, int y1) { int dx, dy, p, x, y; dx=x1-x0; dy=y1-y0; x=x0; y=y0; p=2*dy-dx; while(x<x1) { if(p>=0) { putpixel(x,y,7); y=y+1; p=p+2*dy-2*dx; } else { putpixel(x,y,7); p=p+2*dy;} x=x+1; } } int main() { int gdriver=DETECT, gmode, error, x0, y0, x1, y1; initgraph(&gdriver, &gmode, \"c:\\\\turboc3\\\\bgi\"); printf(\"Enter co-ordinates of first point: \"); scanf(\"%d%d\", &x0, &y0); printf(\"Enter co-ordinates of second point: \"); scanf(\"%d%d\", &x1, &y1); drawline(x0, y0, x1, y1); return 0; } DDA Algorithm Bresenham's Line Algorithm 1. DDA Algorithm use floating point, i.e., Real Arithmetic. 1. Bresenham's Line Algorithm use fixed point, i.e., Integer Arithmetic 2. DDA Algorithms uses multiplication & division its operation 2.Bresenham's Line Algorithm uses only subtraction and addition its operation 3. DDA Algorithm is slowly than Bresenham's Line Algorithm in line drawing because it uses real arithmetic (Floating Point operation) 3. Bresenham's Algorithm is faster than DDA Algorithm in line because it involves only addition & subtraction in its calculation and uses only integer arithmetic. 4. DDA Algorithm is not accurate and efficient as Bresenham's Line Algorithm. 4. Bresenham's Line Algorithm is more accurate and efficient at DDA Algorithm. 5.DDA Algorithm can draw circle and curves but are not accurate as Bresenham's Line Algorithm 5. Bresenham's Line Algorithm can draw circle and curves with more accurate than DDA Algorithm.","title":"Bresenham"},{"location":"graphics/bresenham.html#bresenham","text":"#include<stdio.h> #include<graphics.h> void drawline(int x0, int y0, int x1, int y1) { int dx, dy, p, x, y; dx=x1-x0; dy=y1-y0; x=x0; y=y0; p=2*dy-dx; while(x<x1) { if(p>=0) { putpixel(x,y,7); y=y+1; p=p+2*dy-2*dx; } else { putpixel(x,y,7); p=p+2*dy;} x=x+1; } } int main() { int gdriver=DETECT, gmode, error, x0, y0, x1, y1; initgraph(&gdriver, &gmode, \"c:\\\\turboc3\\\\bgi\"); printf(\"Enter co-ordinates of first point: \"); scanf(\"%d%d\", &x0, &y0); printf(\"Enter co-ordinates of second point: \"); scanf(\"%d%d\", &x1, &y1); drawline(x0, y0, x1, y1); return 0; } DDA Algorithm Bresenham's Line Algorithm 1. DDA Algorithm use floating point, i.e., Real Arithmetic. 1. Bresenham's Line Algorithm use fixed point, i.e., Integer Arithmetic 2. DDA Algorithms uses multiplication & division its operation 2.Bresenham's Line Algorithm uses only subtraction and addition its operation 3. DDA Algorithm is slowly than Bresenham's Line Algorithm in line drawing because it uses real arithmetic (Floating Point operation) 3. Bresenham's Algorithm is faster than DDA Algorithm in line because it involves only addition & subtraction in its calculation and uses only integer arithmetic. 4. DDA Algorithm is not accurate and efficient as Bresenham's Line Algorithm. 4. Bresenham's Line Algorithm is more accurate and efficient at DDA Algorithm. 5.DDA Algorithm can draw circle and curves but are not accurate as Bresenham's Line Algorithm 5. Bresenham's Line Algorithm can draw circle and curves with more accurate than DDA Algorithm.","title":"Bresenham"},{"location":"graphics/circle-midpoint.html","text":"Circle with MidPoint \u00b6 void midPointCircleDraw(int x_centre, int y_centre, int r) { int x = r, y = 0; printf(\"(%d, %d) \", x + x_centre, y + y_centre); if (r > 0) { printf(\"(%d, %d) \", x + x_centre, -y + y_centre); printf(\"(%d, %d) \", y + x_centre, x + y_centre); printf(\"(%d, %d)\\n\", -y + x_centre, x + y_centre); } int P = 1 - r; while (x > y) { y++; if (P <= 0) P = P + 2*y + 1; else { x--; P = P + 2*y - 2*x + 1; } if (x < y) break; printf(\"(%d, %d) \", x + x_centre, y + y_centre); printf(\"(%d, %d) \", -x + x_centre, y + y_centre); printf(\"(%d, %d) \", x + x_centre, -y + y_centre); printf(\"(%d, %d)\\n\", -x + x_centre, -y + y_centre); if (x != y) { printf(\"(%d, %d) \", y + x_centre, x + y_centre); printf(\"(%d, %d) \", -y + x_centre, x + y_centre); printf(\"(%d, %d) \", y + x_centre, -x + y_centre); printf(\"(%d, %d)\\n\", -y + x_centre, -x + y_centre); } } }","title":"Circle with MidPoint"},{"location":"graphics/circle-midpoint.html#circle-with-midpoint","text":"void midPointCircleDraw(int x_centre, int y_centre, int r) { int x = r, y = 0; printf(\"(%d, %d) \", x + x_centre, y + y_centre); if (r > 0) { printf(\"(%d, %d) \", x + x_centre, -y + y_centre); printf(\"(%d, %d) \", y + x_centre, x + y_centre); printf(\"(%d, %d)\\n\", -y + x_centre, x + y_centre); } int P = 1 - r; while (x > y) { y++; if (P <= 0) P = P + 2*y + 1; else { x--; P = P + 2*y - 2*x + 1; } if (x < y) break; printf(\"(%d, %d) \", x + x_centre, y + y_centre); printf(\"(%d, %d) \", -x + x_centre, y + y_centre); printf(\"(%d, %d) \", x + x_centre, -y + y_centre); printf(\"(%d, %d)\\n\", -x + x_centre, -y + y_centre); if (x != y) { printf(\"(%d, %d) \", y + x_centre, x + y_centre); printf(\"(%d, %d) \", -y + x_centre, x + y_centre); printf(\"(%d, %d) \", y + x_centre, -x + y_centre); printf(\"(%d, %d)\\n\", -y + x_centre, -x + y_centre); } } }","title":"Circle with MidPoint"},{"location":"graphics/dda.html","text":"Digital differential analyzer \u00b6 dx = X1 - X0; dy = Y1 - Y0; steps = abs(dx) > abs(dy) ? abs(dx) : abs(dy); Xinc = dx / (float) steps; Yinc = dy / (float) steps; X = X0; Y = Y0; for (int i = 0; i <= steps; i++) { putpixel (round(X),round(Y),WHITE); X += Xinc; Y += Yinc; }","title":"Digital differential analyzer"},{"location":"graphics/dda.html#digital-differential-analyzer","text":"dx = X1 - X0; dy = Y1 - Y0; steps = abs(dx) > abs(dy) ? abs(dx) : abs(dy); Xinc = dx / (float) steps; Yinc = dy / (float) steps; X = X0; Y = Y0; for (int i = 0; i <= steps; i++) { putpixel (round(X),round(Y),WHITE); X += Xinc; Y += Yinc; }","title":"Digital differential analyzer"},{"location":"graphics/ellipse-midpoint.html","text":"Ellipse with MidPoint \u00b6 void midptellipse(int rx, int ry, int xc, int yc) { float dx, dy, d1, d2, x, y; x = 0; y = ry; d1 = (ry * ry) - (rx * rx * ry) + (0.25 * rx * rx); dx = 2 * ry * ry * x; dy = 2 * rx * rx * y; while (dx < dy) { printf(\"(%f, %f)\\n\", x + xc, y + yc); printf(\"(%f, %f)\\n\", -x + xc, y + yc); printf(\"(%f, %f)\\n\", x + xc, -y + yc); printf(\"(%f, %f)\\n\", -x + xc, -y + yc); if (d1 < 0) { x++; dx = dx + (2 * ry * ry); d1 = d1 + dx + (ry * ry); } else { x++; y--; dx = dx + (2 * ry * ry); dy = dy - (2 * rx * rx); d1 = d1 + dx - dy + (ry * ry); } } d2 = ((ry * ry) * ((x + 0.5) * (x + 0.5))) + ((rx * rx) * ((y - 1) * (y - 1))) - (rx * rx * ry * ry); while (y >= 0) { printf(\"(%f, %f)\\n\", x + xc, y + yc); printf(\"(%f, %f)\\n\", -x + xc, y + yc); printf(\"(%f, %f)\\n\", x + xc, -y + yc); printf(\"(%f, %f)\\n\", -x + xc, -y + yc); if (d2 > 0) { y--; dy = dy - (2 * rx * rx); d2 = d2 + (rx * rx) - dy; } else { y--; x++; dx = dx + (2 * ry * ry); dy = dy - (2 * rx * rx); d2 = d2 + dx - dy + (rx * rx); } } }","title":"Ellipse with MidPoint"},{"location":"graphics/ellipse-midpoint.html#ellipse-with-midpoint","text":"void midptellipse(int rx, int ry, int xc, int yc) { float dx, dy, d1, d2, x, y; x = 0; y = ry; d1 = (ry * ry) - (rx * rx * ry) + (0.25 * rx * rx); dx = 2 * ry * ry * x; dy = 2 * rx * rx * y; while (dx < dy) { printf(\"(%f, %f)\\n\", x + xc, y + yc); printf(\"(%f, %f)\\n\", -x + xc, y + yc); printf(\"(%f, %f)\\n\", x + xc, -y + yc); printf(\"(%f, %f)\\n\", -x + xc, -y + yc); if (d1 < 0) { x++; dx = dx + (2 * ry * ry); d1 = d1 + dx + (ry * ry); } else { x++; y--; dx = dx + (2 * ry * ry); dy = dy - (2 * rx * rx); d1 = d1 + dx - dy + (ry * ry); } } d2 = ((ry * ry) * ((x + 0.5) * (x + 0.5))) + ((rx * rx) * ((y - 1) * (y - 1))) - (rx * rx * ry * ry); while (y >= 0) { printf(\"(%f, %f)\\n\", x + xc, y + yc); printf(\"(%f, %f)\\n\", -x + xc, y + yc); printf(\"(%f, %f)\\n\", x + xc, -y + yc); printf(\"(%f, %f)\\n\", -x + xc, -y + yc); if (d2 > 0) { y--; dy = dy - (2 * rx * rx); d2 = d2 + (rx * rx) - dy; } else { y--; x++; dx = dx + (2 * ry * ry); dy = dy - (2 * rx * rx); d2 = d2 + dx - dy + (rx * rx); } } }","title":"Ellipse with MidPoint"},{"location":"graphics/fill-boundary.html","text":"Boundary Fill \u00b6 // C Implementation for Boundary Filling Algorithm #include <graphics.h> // Function for 4 connected Pixels void boundaryFill4(int x, int y, int fill_color,int boundary_color) { if(getpixel(x, y) != boundary_color && getpixel(x, y) != fill_color) { putpixel(x, y, fill_color); boundaryFill4(x + 1, y, fill_color, boundary_color); boundaryFill4(x, y + 1, fill_color, boundary_color); boundaryFill4(x - 1, y, fill_color, boundary_color); boundaryFill4(x, y - 1, fill_color, boundary_color); } } //driver code int main() { // gm is Graphics mode which is // a computer display mode that // generates image using pixels. // DETECT is a macro defined in // \"graphics.h\" header file int gd = DETECT, gm; // initgraph initializes the // graphics system by loading a // graphics driver from disk initgraph(&gd, &gm, \"\"); int x = 250, y = 200, radius = 50; // circle function circle(x, y, radius); // Function calling boundaryFill4(x, y, 6, 15); delay(10000); getch(); // closegraph function closes the // graphics mode and deallocates // all memory allocated by // graphics system . closegraph(); return 0; } 4-connected pixels Vs 8-connected pixels \u00b6 Let us take a figure with the boundary color as GREEN and the fill color as RED. The 4-connected method fails to fill this figure completely. This figure will be efficiently filled using the 8-connected technique. Flood fill Vs Boundary fill \u00b6 Though both Flood fill and Boundary fill algorithms color a given figure with a chosen color, they differ in one aspect. In Flood fill, all the connected pixels of a selected color get replaced by a fill color. On the other hand, in Boundary fill, the program stops when a given color boundary is found.","title":"Boundary Fill"},{"location":"graphics/fill-boundary.html#boundary-fill","text":"// C Implementation for Boundary Filling Algorithm #include <graphics.h> // Function for 4 connected Pixels void boundaryFill4(int x, int y, int fill_color,int boundary_color) { if(getpixel(x, y) != boundary_color && getpixel(x, y) != fill_color) { putpixel(x, y, fill_color); boundaryFill4(x + 1, y, fill_color, boundary_color); boundaryFill4(x, y + 1, fill_color, boundary_color); boundaryFill4(x - 1, y, fill_color, boundary_color); boundaryFill4(x, y - 1, fill_color, boundary_color); } } //driver code int main() { // gm is Graphics mode which is // a computer display mode that // generates image using pixels. // DETECT is a macro defined in // \"graphics.h\" header file int gd = DETECT, gm; // initgraph initializes the // graphics system by loading a // graphics driver from disk initgraph(&gd, &gm, \"\"); int x = 250, y = 200, radius = 50; // circle function circle(x, y, radius); // Function calling boundaryFill4(x, y, 6, 15); delay(10000); getch(); // closegraph function closes the // graphics mode and deallocates // all memory allocated by // graphics system . closegraph(); return 0; }","title":"Boundary Fill"},{"location":"graphics/fill-boundary.html#4-connected-pixels-vs-8-connected-pixels","text":"Let us take a figure with the boundary color as GREEN and the fill color as RED. The 4-connected method fails to fill this figure completely. This figure will be efficiently filled using the 8-connected technique.","title":"4-connected pixels Vs 8-connected pixels"},{"location":"graphics/fill-boundary.html#flood-fill-vs-boundary-fill","text":"Though both Flood fill and Boundary fill algorithms color a given figure with a chosen color, they differ in one aspect. In Flood fill, all the connected pixels of a selected color get replaced by a fill color. On the other hand, in Boundary fill, the program stops when a given color boundary is found.","title":"Flood fill Vs Boundary fill"},{"location":"graphics/fill-flood.html","text":"Flood Fill \u00b6 Recursion \u00b6 // A C++ program to implement flood fill algorithm #include<iostream> using namespace std; // Dimensions of paint screen #define M 8 #define N 8 // A recursive function to replace previous color 'prevC' at '(x, y)' // and all surrounding pixels of (x, y) with new color 'newC' and void floodFillUtil(int screen[][N], int x, int y, int prevC, int newC) { // Base cases if (x < 0 || x >= M || y < 0 || y >= N) return; if (screen[x][y] != prevC) return; if (screen[x][y] == newC) return; // Replace the color at (x, y) screen[x][y] = newC; // Recur for north, east, south and west floodFillUtil(screen, x+1, y, prevC, newC); floodFillUtil(screen, x-1, y, prevC, newC); floodFillUtil(screen, x, y+1, prevC, newC); floodFillUtil(screen, x, y-1, prevC, newC); } // It mainly finds the previous color on (x, y) and // calls floodFillUtil() void floodFill(int screen[][N], int x, int y, int newC) { int prevC = screen[x][y]; if(prevC==newC) return; floodFillUtil(screen, x, y, prevC, newC); } // Driver code int main() { int screen[M][N] = {{1, 1, 1, 1, 1, 1, 1, 1}, {1, 1, 1, 1, 1, 1, 0, 0}, {1, 0, 0, 1, 1, 0, 1, 1}, {1, 2, 2, 2, 2, 0, 1, 0}, {1, 1, 1, 2, 2, 0, 1, 0}, {1, 1, 1, 2, 2, 2, 2, 0}, {1, 1, 1, 1, 1, 2, 1, 1}, {1, 1, 1, 1, 1, 2, 2, 1}, }; int x = 4, y = 4, newC = 3; floodFill(screen, x, y, newC); cout << \"Updated screen after call to floodFill: \\n\"; for (int i=0; i<M; i++) { for (int j=0; j<N; j++) cout << screen[i][j] << \" \"; cout << endl; } } //Updated by Arun Pandey BFS \u00b6 // C++ program for above approach #include <bits/stdc++.h> using namespace std; // Function to check valid coordinate int validCoord(int x, int y, int n, int m) { if (x < 0 || y < 0) { return 0; } if (x >= n || y >= m) { return 0; } return 1; } // Function to run bfs void bfs(int n, int m, int data[][8], int x, int y, int color) { // Visiting array int vis[101][101]; // Initialing all as zero memset(vis, 0, sizeof(vis)); // Creating queue for bfs queue<pair<int, int> > obj; // Pushing pair of {x, y} obj.push({ x, y }); // Marking {x, y} as visited vis[x][y] = 1; // Until queue is empty while (obj.empty() != 1) { // Extracting front pair pair<int, int> coord = obj.front(); int x = coord.first; int y = coord.second; int preColor = data[x][y]; data[x][y] = color; // Popping front pair of queue obj.pop(); // For Upside Pixel or Cell if (validCoord(x + 1, y, n, m) && vis[x + 1][y] == 0 && data[x + 1][y] == preColor) { obj.push({ x + 1, y }); vis[x + 1][y] = 1; } // For Downside Pixel or Cell if (validCoord(x - 1, y, n, m) && vis[x - 1][y] == 0 && data[x - 1][y] == preColor) { obj.push({ x - 1, y }); vis[x - 1][y] = 1; } // For Right side Pixel or Cell if (validCoord(x, y + 1, n, m) && vis[x][y + 1] == 0 && data[x][y + 1] == preColor) { obj.push({ x, y + 1 }); vis[x][y + 1] = 1; } // For Left side Pixel or Cell if (validCoord(x, y - 1, n, m) && vis[x][y - 1] == 0 && data[x][y - 1] == preColor) { obj.push({ x, y - 1 }); vis[x][y - 1] = 1; } } // Printing The Changed Matrix Of Pixels for (int i = 0; i < n; i++) { for (int j = 0; j < m; j++) { cout << data[i][j] << \" \"; } cout << endl; } cout << endl; } // Driver Code int main() { int n, m, x, y, color; n = 8; m = 8; int data[8][8] = { { 1, 1, 1, 1, 1, 1, 1, 1 }, { 1, 1, 1, 1, 1, 1, 0, 0 }, { 1, 0, 0, 1, 1, 0, 1, 1 }, { 1, 2, 2, 2, 2, 0, 1, 0 }, { 1, 1, 1, 2, 2, 0, 1, 0 }, { 1, 1, 1, 2, 2, 2, 2, 0 }, { 1, 1, 1, 1, 1, 2, 1, 1 }, { 1, 1, 1, 1, 1, 2, 2, 1 }, }; x = 4, y = 4, color = 3; // Function Call bfs(n, m, data, x, y, color); return 0; }","title":"Flood Fill"},{"location":"graphics/fill-flood.html#flood-fill","text":"","title":"Flood Fill"},{"location":"graphics/fill-flood.html#recursion","text":"// A C++ program to implement flood fill algorithm #include<iostream> using namespace std; // Dimensions of paint screen #define M 8 #define N 8 // A recursive function to replace previous color 'prevC' at '(x, y)' // and all surrounding pixels of (x, y) with new color 'newC' and void floodFillUtil(int screen[][N], int x, int y, int prevC, int newC) { // Base cases if (x < 0 || x >= M || y < 0 || y >= N) return; if (screen[x][y] != prevC) return; if (screen[x][y] == newC) return; // Replace the color at (x, y) screen[x][y] = newC; // Recur for north, east, south and west floodFillUtil(screen, x+1, y, prevC, newC); floodFillUtil(screen, x-1, y, prevC, newC); floodFillUtil(screen, x, y+1, prevC, newC); floodFillUtil(screen, x, y-1, prevC, newC); } // It mainly finds the previous color on (x, y) and // calls floodFillUtil() void floodFill(int screen[][N], int x, int y, int newC) { int prevC = screen[x][y]; if(prevC==newC) return; floodFillUtil(screen, x, y, prevC, newC); } // Driver code int main() { int screen[M][N] = {{1, 1, 1, 1, 1, 1, 1, 1}, {1, 1, 1, 1, 1, 1, 0, 0}, {1, 0, 0, 1, 1, 0, 1, 1}, {1, 2, 2, 2, 2, 0, 1, 0}, {1, 1, 1, 2, 2, 0, 1, 0}, {1, 1, 1, 2, 2, 2, 2, 0}, {1, 1, 1, 1, 1, 2, 1, 1}, {1, 1, 1, 1, 1, 2, 2, 1}, }; int x = 4, y = 4, newC = 3; floodFill(screen, x, y, newC); cout << \"Updated screen after call to floodFill: \\n\"; for (int i=0; i<M; i++) { for (int j=0; j<N; j++) cout << screen[i][j] << \" \"; cout << endl; } } //Updated by Arun Pandey","title":"Recursion"},{"location":"graphics/fill-flood.html#bfs","text":"// C++ program for above approach #include <bits/stdc++.h> using namespace std; // Function to check valid coordinate int validCoord(int x, int y, int n, int m) { if (x < 0 || y < 0) { return 0; } if (x >= n || y >= m) { return 0; } return 1; } // Function to run bfs void bfs(int n, int m, int data[][8], int x, int y, int color) { // Visiting array int vis[101][101]; // Initialing all as zero memset(vis, 0, sizeof(vis)); // Creating queue for bfs queue<pair<int, int> > obj; // Pushing pair of {x, y} obj.push({ x, y }); // Marking {x, y} as visited vis[x][y] = 1; // Until queue is empty while (obj.empty() != 1) { // Extracting front pair pair<int, int> coord = obj.front(); int x = coord.first; int y = coord.second; int preColor = data[x][y]; data[x][y] = color; // Popping front pair of queue obj.pop(); // For Upside Pixel or Cell if (validCoord(x + 1, y, n, m) && vis[x + 1][y] == 0 && data[x + 1][y] == preColor) { obj.push({ x + 1, y }); vis[x + 1][y] = 1; } // For Downside Pixel or Cell if (validCoord(x - 1, y, n, m) && vis[x - 1][y] == 0 && data[x - 1][y] == preColor) { obj.push({ x - 1, y }); vis[x - 1][y] = 1; } // For Right side Pixel or Cell if (validCoord(x, y + 1, n, m) && vis[x][y + 1] == 0 && data[x][y + 1] == preColor) { obj.push({ x, y + 1 }); vis[x][y + 1] = 1; } // For Left side Pixel or Cell if (validCoord(x, y - 1, n, m) && vis[x][y - 1] == 0 && data[x][y - 1] == preColor) { obj.push({ x, y - 1 }); vis[x][y - 1] = 1; } } // Printing The Changed Matrix Of Pixels for (int i = 0; i < n; i++) { for (int j = 0; j < m; j++) { cout << data[i][j] << \" \"; } cout << endl; } cout << endl; } // Driver Code int main() { int n, m, x, y, color; n = 8; m = 8; int data[8][8] = { { 1, 1, 1, 1, 1, 1, 1, 1 }, { 1, 1, 1, 1, 1, 1, 0, 0 }, { 1, 0, 0, 1, 1, 0, 1, 1 }, { 1, 2, 2, 2, 2, 0, 1, 0 }, { 1, 1, 1, 2, 2, 0, 1, 0 }, { 1, 1, 1, 2, 2, 2, 2, 0 }, { 1, 1, 1, 1, 1, 2, 1, 1 }, { 1, 1, 1, 1, 1, 2, 2, 1 }, }; x = 4, y = 4, color = 3; // Function Call bfs(n, m, data, x, y, color); return 0; }","title":"BFS"},{"location":"graphics/scan-line.html","text":"Scan Line \u00b6 // CPP program to illustrate // Scanline Polygon fill Algorithm #include <stdio.h> #include <math.h> #include <GL/glut.h> #define maxHt 800 #define maxWd 600 #define maxVer 10000 FILE *fp; // Start from lower left corner typedef struct edgebucket { int ymax; //max y-coordinate of edge float xofymin; //x-coordinate of lowest edge point updated only in aet float slopeinverse; }EdgeBucket; typedef struct edgetabletup { // the array will give the scanline number // The edge table (ET) with edges entries sorted // in increasing y and x of the lower end int countEdgeBucket; //no. of edgebuckets EdgeBucket buckets[maxVer]; }EdgeTableTuple; EdgeTableTuple EdgeTable[maxHt], ActiveEdgeTuple; // Scanline Function void initEdgeTable() { int i; for (i=0; i<maxHt; i++) { EdgeTable[i].countEdgeBucket = 0; } ActiveEdgeTuple.countEdgeBucket = 0; } void printTuple(EdgeTableTuple *tup) { int j; if (tup->countEdgeBucket) printf(\"\\nCount %d-----\\n\",tup->countEdgeBucket); for (j=0; j<tup->countEdgeBucket; j++) { printf(\" %d+%.2f+%.2f\", tup->buckets[j].ymax, tup->buckets[j].xofymin,tup->buckets[j].slopeinverse); } } void printTable() { int i,j; for (i=0; i<maxHt; i++) { if (EdgeTable[i].countEdgeBucket) printf(\"\\nScanline %d\", i); printTuple(&EdgeTable[i]); } } /* Function to sort an array using insertion sort*/ void insertionSort(EdgeTableTuple *ett) { int i,j; EdgeBucket temp; for (i = 1; i < ett->countEdgeBucket; i++) { temp.ymax = ett->buckets[i].ymax; temp.xofymin = ett->buckets[i].xofymin; temp.slopeinverse = ett->buckets[i].slopeinverse; j = i - 1; while ((temp.xofymin < ett->buckets[j].xofymin) && (j >= 0)) { ett->buckets[j + 1].ymax = ett->buckets[j].ymax; ett->buckets[j + 1].xofymin = ett->buckets[j].xofymin; ett->buckets[j + 1].slopeinverse = ett->buckets[j].slopeinverse; j = j - 1; } ett->buckets[j + 1].ymax = temp.ymax; ett->buckets[j + 1].xofymin = temp.xofymin; ett->buckets[j + 1].slopeinverse = temp.slopeinverse; } } void storeEdgeInTuple (EdgeTableTuple *receiver,int ym,int xm,float slopInv) { // both used for edgetable and active edge table.. // The edge tuple sorted in increasing ymax and x of the lower end. (receiver->buckets[(receiver)->countEdgeBucket]).ymax = ym; (receiver->buckets[(receiver)->countEdgeBucket]).xofymin = (float)xm; (receiver->buckets[(receiver)->countEdgeBucket]).slopeinverse = slopInv; // sort the buckets insertionSort(receiver); (receiver->countEdgeBucket)++; } void storeEdgeInTable (int x1,int y1, int x2, int y2) { float m,minv; int ymaxTS,xwithyminTS, scanline; //ts stands for to store if (x2==x1) { minv=0.000000; } else { m = ((float)(y2-y1))/((float)(x2-x1)); // horizontal lines are not stored in edge table if (y2==y1) return; minv = (float)1.0/m; printf(\"\\nSlope string for %d %d & %d %d: %f\",x1,y1,x2,y2,minv); } if (y1>y2) { scanline=y2; ymaxTS=y1; xwithyminTS=x2; } else { scanline=y1; ymaxTS=y2; xwithyminTS=x1; } // the assignment part is done..now storage.. storeEdgeInTuple(&EdgeTable[scanline],ymaxTS,xwithyminTS,minv); } void removeEdgeByYmax(EdgeTableTuple *Tup,int yy) { int i,j; for (i=0; i< Tup->countEdgeBucket; i++) { if (Tup->buckets[i].ymax == yy) { printf(\"\\nRemoved at %d\",yy); for ( j = i ; j < Tup->countEdgeBucket -1 ; j++ ) { Tup->buckets[j].ymax =Tup->buckets[j+1].ymax; Tup->buckets[j].xofymin =Tup->buckets[j+1].xofymin; Tup->buckets[j].slopeinverse = Tup->buckets[j+1].slopeinverse; } Tup->countEdgeBucket--; i--; } } } void updatexbyslopeinv(EdgeTableTuple *Tup) { int i; for (i=0; i<Tup->countEdgeBucket; i++) { (Tup->buckets[i]).xofymin =(Tup->buckets[i]).xofymin + (Tup->buckets[i]).slopeinverse; } } void ScanlineFill() { /* Follow the following rules: 1. Horizontal edges: Do not include in edge table 2. Horizontal edges: Drawn either on the bottom or on the top. 3. Vertices: If local max or min, then count twice, else count once. 4. Either vertices at local minima or at local maxima are drawn.*/ int i, j, x1, ymax1, x2, ymax2, FillFlag = 0, coordCount; // we will start from scanline 0; // Repeat until last scanline: for (i=0; i<maxHt; i++)//4. Increment y by 1 (next scan line) { // 1. Move from ET bucket y to the // AET those edges whose ymin = y (entering edges) for (j=0; j<EdgeTable[i].countEdgeBucket; j++) { storeEdgeInTuple(&ActiveEdgeTuple,EdgeTable[i].buckets[j]. ymax,EdgeTable[i].buckets[j].xofymin, EdgeTable[i].buckets[j].slopeinverse); } printTuple(&ActiveEdgeTuple); // 2. Remove from AET those edges for // which y=ymax (not involved in next scan line) removeEdgeByYmax(&ActiveEdgeTuple, i); //sort AET (remember: ET is presorted) insertionSort(&ActiveEdgeTuple); printTuple(&ActiveEdgeTuple); //3. Fill lines on scan line y by using pairs of x-coords from AET j = 0; FillFlag = 0; coordCount = 0; x1 = 0; x2 = 0; ymax1 = 0; ymax2 = 0; while (j<ActiveEdgeTuple.countEdgeBucket) { if (coordCount%2==0) { x1 = (int)(ActiveEdgeTuple.buckets[j].xofymin); ymax1 = ActiveEdgeTuple.buckets[j].ymax; if (x1==x2) { /* three cases can arrive- 1. lines are towards top of the intersection 2. lines are towards bottom 3. one line is towards top and other is towards bottom */ if (((x1==ymax1)&&(x2!=ymax2))||((x1!=ymax1)&&(x2==ymax2))) { x2 = x1; ymax2 = ymax1; } else { coordCount++; } } else { coordCount++; } } else { x2 = (int)ActiveEdgeTuple.buckets[j].xofymin; ymax2 = ActiveEdgeTuple.buckets[j].ymax; FillFlag = 0; // checking for intersection... if (x1==x2) { /*three cases can arrive- 1. lines are towards top of the intersection 2. lines are towards bottom 3. one line is towards top and other is towards bottom */ if (((x1==ymax1)&&(x2!=ymax2))||((x1!=ymax1)&&(x2==ymax2))) { x1 = x2; ymax1 = ymax2; } else { coordCount++; FillFlag = 1; } } else { coordCount++; FillFlag = 1; } if(FillFlag) { //drawing actual lines... glColor3f(0.0f,0.7f,0.0f); glBegin(GL_LINES); glVertex2i(x1,i); glVertex2i(x2,i); glEnd(); glFlush(); // printf(\"\\nLine drawn from %d,%d to %d,%d\",x1,i,x2,i); } } j++; } // 5. For each nonvertical edge remaining in AET, update x for new y updatexbyslopeinv(&ActiveEdgeTuple); } printf(\"\\nScanline filling complete\"); } void myInit(void) { glClearColor(1.0,1.0,1.0,0.0); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0,maxHt,0,maxWd); glClear(GL_COLOR_BUFFER_BIT); } void drawPolyDino() { glColor3f(1.0f,0.0f,0.0f); int count = 0,x1,y1,x2,y2; rewind(fp); while(!feof(fp) ) { count++; if (count>2) { x1 = x2; y1 = y2; count=2; } if (count==1) { fscanf(fp, \"%d,%d\", &x1, &y1); } else { fscanf(fp, \"%d,%d\", &x2, &y2); printf(\"\\n%d,%d\", x2, y2); glBegin(GL_LINES); glVertex2i( x1, y1); glVertex2i( x2, y2); glEnd(); storeEdgeInTable(x1, y1, x2, y2);//storage of edges in edge table. glFlush(); } } } void drawDino(void) { initEdgeTable(); drawPolyDino(); printf(\"\\nTable\"); printTable(); ScanlineFill();//actual calling of scanline filling.. } void main(int argc, char** argv) { fp=fopen (\"PolyDino.txt\",\"r\"); if ( fp == NULL ) { printf( \"Could not open file\" ) ; return; } glutInit(&argc, argv); glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); glutInitWindowSize(maxHt,maxWd); glutInitWindowPosition(100, 150); glutCreateWindow(\"Scanline filled dinosaur\"); myInit(); glutDisplayFunc(drawDino); glutMainLoop(); fclose(fp); }","title":"Scan Line"},{"location":"graphics/scan-line.html#scan-line","text":"// CPP program to illustrate // Scanline Polygon fill Algorithm #include <stdio.h> #include <math.h> #include <GL/glut.h> #define maxHt 800 #define maxWd 600 #define maxVer 10000 FILE *fp; // Start from lower left corner typedef struct edgebucket { int ymax; //max y-coordinate of edge float xofymin; //x-coordinate of lowest edge point updated only in aet float slopeinverse; }EdgeBucket; typedef struct edgetabletup { // the array will give the scanline number // The edge table (ET) with edges entries sorted // in increasing y and x of the lower end int countEdgeBucket; //no. of edgebuckets EdgeBucket buckets[maxVer]; }EdgeTableTuple; EdgeTableTuple EdgeTable[maxHt], ActiveEdgeTuple; // Scanline Function void initEdgeTable() { int i; for (i=0; i<maxHt; i++) { EdgeTable[i].countEdgeBucket = 0; } ActiveEdgeTuple.countEdgeBucket = 0; } void printTuple(EdgeTableTuple *tup) { int j; if (tup->countEdgeBucket) printf(\"\\nCount %d-----\\n\",tup->countEdgeBucket); for (j=0; j<tup->countEdgeBucket; j++) { printf(\" %d+%.2f+%.2f\", tup->buckets[j].ymax, tup->buckets[j].xofymin,tup->buckets[j].slopeinverse); } } void printTable() { int i,j; for (i=0; i<maxHt; i++) { if (EdgeTable[i].countEdgeBucket) printf(\"\\nScanline %d\", i); printTuple(&EdgeTable[i]); } } /* Function to sort an array using insertion sort*/ void insertionSort(EdgeTableTuple *ett) { int i,j; EdgeBucket temp; for (i = 1; i < ett->countEdgeBucket; i++) { temp.ymax = ett->buckets[i].ymax; temp.xofymin = ett->buckets[i].xofymin; temp.slopeinverse = ett->buckets[i].slopeinverse; j = i - 1; while ((temp.xofymin < ett->buckets[j].xofymin) && (j >= 0)) { ett->buckets[j + 1].ymax = ett->buckets[j].ymax; ett->buckets[j + 1].xofymin = ett->buckets[j].xofymin; ett->buckets[j + 1].slopeinverse = ett->buckets[j].slopeinverse; j = j - 1; } ett->buckets[j + 1].ymax = temp.ymax; ett->buckets[j + 1].xofymin = temp.xofymin; ett->buckets[j + 1].slopeinverse = temp.slopeinverse; } } void storeEdgeInTuple (EdgeTableTuple *receiver,int ym,int xm,float slopInv) { // both used for edgetable and active edge table.. // The edge tuple sorted in increasing ymax and x of the lower end. (receiver->buckets[(receiver)->countEdgeBucket]).ymax = ym; (receiver->buckets[(receiver)->countEdgeBucket]).xofymin = (float)xm; (receiver->buckets[(receiver)->countEdgeBucket]).slopeinverse = slopInv; // sort the buckets insertionSort(receiver); (receiver->countEdgeBucket)++; } void storeEdgeInTable (int x1,int y1, int x2, int y2) { float m,minv; int ymaxTS,xwithyminTS, scanline; //ts stands for to store if (x2==x1) { minv=0.000000; } else { m = ((float)(y2-y1))/((float)(x2-x1)); // horizontal lines are not stored in edge table if (y2==y1) return; minv = (float)1.0/m; printf(\"\\nSlope string for %d %d & %d %d: %f\",x1,y1,x2,y2,minv); } if (y1>y2) { scanline=y2; ymaxTS=y1; xwithyminTS=x2; } else { scanline=y1; ymaxTS=y2; xwithyminTS=x1; } // the assignment part is done..now storage.. storeEdgeInTuple(&EdgeTable[scanline],ymaxTS,xwithyminTS,minv); } void removeEdgeByYmax(EdgeTableTuple *Tup,int yy) { int i,j; for (i=0; i< Tup->countEdgeBucket; i++) { if (Tup->buckets[i].ymax == yy) { printf(\"\\nRemoved at %d\",yy); for ( j = i ; j < Tup->countEdgeBucket -1 ; j++ ) { Tup->buckets[j].ymax =Tup->buckets[j+1].ymax; Tup->buckets[j].xofymin =Tup->buckets[j+1].xofymin; Tup->buckets[j].slopeinverse = Tup->buckets[j+1].slopeinverse; } Tup->countEdgeBucket--; i--; } } } void updatexbyslopeinv(EdgeTableTuple *Tup) { int i; for (i=0; i<Tup->countEdgeBucket; i++) { (Tup->buckets[i]).xofymin =(Tup->buckets[i]).xofymin + (Tup->buckets[i]).slopeinverse; } } void ScanlineFill() { /* Follow the following rules: 1. Horizontal edges: Do not include in edge table 2. Horizontal edges: Drawn either on the bottom or on the top. 3. Vertices: If local max or min, then count twice, else count once. 4. Either vertices at local minima or at local maxima are drawn.*/ int i, j, x1, ymax1, x2, ymax2, FillFlag = 0, coordCount; // we will start from scanline 0; // Repeat until last scanline: for (i=0; i<maxHt; i++)//4. Increment y by 1 (next scan line) { // 1. Move from ET bucket y to the // AET those edges whose ymin = y (entering edges) for (j=0; j<EdgeTable[i].countEdgeBucket; j++) { storeEdgeInTuple(&ActiveEdgeTuple,EdgeTable[i].buckets[j]. ymax,EdgeTable[i].buckets[j].xofymin, EdgeTable[i].buckets[j].slopeinverse); } printTuple(&ActiveEdgeTuple); // 2. Remove from AET those edges for // which y=ymax (not involved in next scan line) removeEdgeByYmax(&ActiveEdgeTuple, i); //sort AET (remember: ET is presorted) insertionSort(&ActiveEdgeTuple); printTuple(&ActiveEdgeTuple); //3. Fill lines on scan line y by using pairs of x-coords from AET j = 0; FillFlag = 0; coordCount = 0; x1 = 0; x2 = 0; ymax1 = 0; ymax2 = 0; while (j<ActiveEdgeTuple.countEdgeBucket) { if (coordCount%2==0) { x1 = (int)(ActiveEdgeTuple.buckets[j].xofymin); ymax1 = ActiveEdgeTuple.buckets[j].ymax; if (x1==x2) { /* three cases can arrive- 1. lines are towards top of the intersection 2. lines are towards bottom 3. one line is towards top and other is towards bottom */ if (((x1==ymax1)&&(x2!=ymax2))||((x1!=ymax1)&&(x2==ymax2))) { x2 = x1; ymax2 = ymax1; } else { coordCount++; } } else { coordCount++; } } else { x2 = (int)ActiveEdgeTuple.buckets[j].xofymin; ymax2 = ActiveEdgeTuple.buckets[j].ymax; FillFlag = 0; // checking for intersection... if (x1==x2) { /*three cases can arrive- 1. lines are towards top of the intersection 2. lines are towards bottom 3. one line is towards top and other is towards bottom */ if (((x1==ymax1)&&(x2!=ymax2))||((x1!=ymax1)&&(x2==ymax2))) { x1 = x2; ymax1 = ymax2; } else { coordCount++; FillFlag = 1; } } else { coordCount++; FillFlag = 1; } if(FillFlag) { //drawing actual lines... glColor3f(0.0f,0.7f,0.0f); glBegin(GL_LINES); glVertex2i(x1,i); glVertex2i(x2,i); glEnd(); glFlush(); // printf(\"\\nLine drawn from %d,%d to %d,%d\",x1,i,x2,i); } } j++; } // 5. For each nonvertical edge remaining in AET, update x for new y updatexbyslopeinv(&ActiveEdgeTuple); } printf(\"\\nScanline filling complete\"); } void myInit(void) { glClearColor(1.0,1.0,1.0,0.0); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0,maxHt,0,maxWd); glClear(GL_COLOR_BUFFER_BIT); } void drawPolyDino() { glColor3f(1.0f,0.0f,0.0f); int count = 0,x1,y1,x2,y2; rewind(fp); while(!feof(fp) ) { count++; if (count>2) { x1 = x2; y1 = y2; count=2; } if (count==1) { fscanf(fp, \"%d,%d\", &x1, &y1); } else { fscanf(fp, \"%d,%d\", &x2, &y2); printf(\"\\n%d,%d\", x2, y2); glBegin(GL_LINES); glVertex2i( x1, y1); glVertex2i( x2, y2); glEnd(); storeEdgeInTable(x1, y1, x2, y2);//storage of edges in edge table. glFlush(); } } } void drawDino(void) { initEdgeTable(); drawPolyDino(); printf(\"\\nTable\"); printTable(); ScanlineFill();//actual calling of scanline filling.. } void main(int argc, char** argv) { fp=fopen (\"PolyDino.txt\",\"r\"); if ( fp == NULL ) { printf( \"Could not open file\" ) ; return; } glutInit(&argc, argv); glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); glutInitWindowSize(maxHt,maxWd); glutInitWindowPosition(100, 150); glutCreateWindow(\"Scanline filled dinosaur\"); myInit(); glutDisplayFunc(drawDino); glutMainLoop(); fclose(fp); }","title":"Scan Line"},{"location":"java/annotations.html","text":"Annotations \u00b6 A form of metadata, provide data about a program that is not part of the program itself. Information for the compiler \u2014 Annotations can be used by the compiler to detect errors or suppress warnings. Compile-time and deployment-time processing \u2014 Software tools can process annotation information to generate code, XML files, and so forth. Runtime processing \u2014 Some annotations are available to be examined at runtime. stronger type checking extend java.lang.annotation to form your own annotations repeated are allowed Annotations can be applied to declarations: declarations of classes, fields, methods, and other program elements. @Entity @Override void mySuperMethod() { ... } @Author( name = \"Benjamin Franklin\", date = \"3/27/2003\" ) class MyClass { ... } @SuppressWarnings(value = \"unchecked\") void myMethod() { ... } @Author(name = \"Jane Doe\") @EBook class MyClass { ... } @Author(name = \"Jane Doe\") @Author(name = \"John Smith\") class MyClass { ... } Class instance creation expression: new @Interned MyObject(); Type cast: myString = (@NonNull String) str; implements clause: class UnmodifiableList<T> implements @Readonly List<@Readonly T> { ... } Thrown exception declaration: void monitorTemperature() throws @Critical TemperatureException { ... } Annotation Type \u00b6 Many annotations replace comments in code. @Documented @interface ClassPreamble { String author(); String date(); int currentRevision() default 1; String lastModified() default \"N/A\"; String lastModifiedBy() default \"N/A\"; // Note use of array String[] reviewers(); } @ClassPreamble ( author = \"John Doe\", date = \"3/17/2002\", currentRevision = 6, lastModified = \"4/12/2004\", lastModifiedBy = \"Jane Doe\", // Note array notation reviewers = {\"Alice\", \"Bob\", \"Cindy\"} ) public class Generation3List extends Generation2List { // class code goes here } java inbuilt \u00b6 @Deprecated @Override @SuppressWarnings({\"unchecked\", \"deprecation\"}) @SafeVarargs - avoid unsafe operations // TODO @FunctionalInterface Meta annotations \u00b6 @Retention: specifies how the marked annotation is stored: RetentionPolicy.SOURCE \u2013 The marked annotation is retained only in the source level and is ignored by the compiler. RetentionPolicy.CLASS \u2013 The marked annotation is retained by the compiler at compile time, but is ignored by the Java Virtual Machine (JVM). RetentionPolicy.RUNTIME \u2013 The marked annotation is retained by the JVM so it can be used by the runtime environment. @Documented - included in javadoc @Target - on which it is allowed @Inherited - sub class inherets super class's annotion. @Repeatable - more than once Type Annotations and Pluggable Type Systems \u00b6 expressions (new), casts, implements clauses, and throws clauses. @NonNull String str; Checker Framework Repeating Annotations \u00b6 @Schedule(dayOfMonth=\"last\") @Schedule(dayOfWeek=\"Fri\", hour=\"23\") public void doPeriodicCleanup() { ... } @Alert(role=\"Manager\") @Alert(role=\"Administrator\") public class UnauthorizedAccessException extends SecurityException { ... } For compatibility reasons, repeating annotations are stored in a container annotation that is automatically generated by the Java compiler. In order for the compiler to do this, two declarations are required in your code: Declare a Repeatable Annotation Type import java.lang.annotation.Repeatable; @Repeatable(Schedules.class) public @interface Schedule { String dayOfMonth() default \"first\"; String dayOfWeek() default \"Mon\"; int hour() default 12; } Declare the Containing Annotation Type public @interface Schedules { Schedule[] value(); } Retrieving Annotations \u00b6 AnnotatedElement.getAnnotation(Class<T>) AnnotatedElement.getAnnotationsByType(Class<T>) Design Considerations \u00b6 you must consider the cardinality of annotations of that type. to be as flexible and powerful as possible.","title":"Annotations"},{"location":"java/annotations.html#annotations","text":"A form of metadata, provide data about a program that is not part of the program itself. Information for the compiler \u2014 Annotations can be used by the compiler to detect errors or suppress warnings. Compile-time and deployment-time processing \u2014 Software tools can process annotation information to generate code, XML files, and so forth. Runtime processing \u2014 Some annotations are available to be examined at runtime. stronger type checking extend java.lang.annotation to form your own annotations repeated are allowed Annotations can be applied to declarations: declarations of classes, fields, methods, and other program elements. @Entity @Override void mySuperMethod() { ... } @Author( name = \"Benjamin Franklin\", date = \"3/27/2003\" ) class MyClass { ... } @SuppressWarnings(value = \"unchecked\") void myMethod() { ... } @Author(name = \"Jane Doe\") @EBook class MyClass { ... } @Author(name = \"Jane Doe\") @Author(name = \"John Smith\") class MyClass { ... } Class instance creation expression: new @Interned MyObject(); Type cast: myString = (@NonNull String) str; implements clause: class UnmodifiableList<T> implements @Readonly List<@Readonly T> { ... } Thrown exception declaration: void monitorTemperature() throws @Critical TemperatureException { ... }","title":"Annotations"},{"location":"java/annotations.html#annotation-type","text":"Many annotations replace comments in code. @Documented @interface ClassPreamble { String author(); String date(); int currentRevision() default 1; String lastModified() default \"N/A\"; String lastModifiedBy() default \"N/A\"; // Note use of array String[] reviewers(); } @ClassPreamble ( author = \"John Doe\", date = \"3/17/2002\", currentRevision = 6, lastModified = \"4/12/2004\", lastModifiedBy = \"Jane Doe\", // Note array notation reviewers = {\"Alice\", \"Bob\", \"Cindy\"} ) public class Generation3List extends Generation2List { // class code goes here }","title":"Annotation Type"},{"location":"java/annotations.html#java-inbuilt","text":"@Deprecated @Override @SuppressWarnings({\"unchecked\", \"deprecation\"}) @SafeVarargs - avoid unsafe operations // TODO @FunctionalInterface","title":"java inbuilt"},{"location":"java/annotations.html#meta-annotations","text":"@Retention: specifies how the marked annotation is stored: RetentionPolicy.SOURCE \u2013 The marked annotation is retained only in the source level and is ignored by the compiler. RetentionPolicy.CLASS \u2013 The marked annotation is retained by the compiler at compile time, but is ignored by the Java Virtual Machine (JVM). RetentionPolicy.RUNTIME \u2013 The marked annotation is retained by the JVM so it can be used by the runtime environment. @Documented - included in javadoc @Target - on which it is allowed @Inherited - sub class inherets super class's annotion. @Repeatable - more than once","title":"Meta annotations"},{"location":"java/annotations.html#type-annotations-and-pluggable-type-systems","text":"expressions (new), casts, implements clauses, and throws clauses. @NonNull String str; Checker Framework","title":"Type Annotations and Pluggable Type Systems"},{"location":"java/annotations.html#repeating-annotations","text":"@Schedule(dayOfMonth=\"last\") @Schedule(dayOfWeek=\"Fri\", hour=\"23\") public void doPeriodicCleanup() { ... } @Alert(role=\"Manager\") @Alert(role=\"Administrator\") public class UnauthorizedAccessException extends SecurityException { ... } For compatibility reasons, repeating annotations are stored in a container annotation that is automatically generated by the Java compiler. In order for the compiler to do this, two declarations are required in your code: Declare a Repeatable Annotation Type import java.lang.annotation.Repeatable; @Repeatable(Schedules.class) public @interface Schedule { String dayOfMonth() default \"first\"; String dayOfWeek() default \"Mon\"; int hour() default 12; } Declare the Containing Annotation Type public @interface Schedules { Schedule[] value(); }","title":"Repeating Annotations"},{"location":"java/annotations.html#retrieving-annotations","text":"AnnotatedElement.getAnnotation(Class<T>) AnnotatedElement.getAnnotationsByType(Class<T>)","title":"Retrieving Annotations"},{"location":"java/annotations.html#design-considerations","text":"you must consider the cardinality of annotations of that type. to be as flexible and powerful as possible.","title":"Design Considerations"},{"location":"java/basics.html","text":"Basics \u00b6 Variables \u00b6 Instance Variables (Non-Static Fields) Class Variables (Static Fields) Local Variables: bw {}. Parameters: always classified as \"variables\" not \"fields\" long creditCardNumber = 1234_5678_9012_3456L; primitive data types are: byte, short, int, long, float, double, boolean, and char. java.lang.String class Naming \u00b6 case sensitive unlimited-length sequence of Unicode letters and digits (avoid _ and $ in start) not keyword or reserved word gearRatio NUM_GEARS = 6 Arrays \u00b6 its length is fixed. int[] anArray = new int[10]; String[][] names = { {\"Mr. \", \"Mrs. \", \"Ms. \"}, {\"Smith\", \"Jones\"} }; System.arraycopy(copyFrom, 2, copyTo, 0, 7); java.util.Arrays.copyOfRange(copyFrom, 2, 9); java.util.Arrays \u00b6 binarySearch equals fill parallelSort java.util.Arrays.stream(copyTo).map(coffee -> coffee + \" \").forEach(System.out::print) System.out.println(java.util.Arrays.toString(copyTo)) Operators \u00b6 specific operations on one, two, or three operands, and then return a result. Simple Assignment Operator = Arithmetic Operators + Additive operator (also used for String concatenation) - Subtraction operator * Multiplication operator / Division operator % Remainder operator Unary Operators + Unary plus operator; indicates positive value (numbers are positive without this, however) - Unary minus operator; negates an expression ++ Increment operator; increments a value by 1 -- Decrement operator; decrements a value by 1 ! Logical complement operator; inverts the value of a boolean Equality and Relational Operators == Equal to != Not equal to > Greater than >= Greater than or equal to < Less than <= Less than or equal to Conditional Operators && Conditional-AND || Conditional-OR ?: Ternary (shorthand for if-then-else statement) Type Comparison Operator instanceof Compares an object to a specified type Bitwise and Bit Shift Operators ~ Unary bitwise complement << Signed left shift >> Signed right shift >>> Unsigned right shift & Bitwise AND ^ Bitwise exclusive OR | Bitwise inclusive OR Expressions -> result=9 blocks -> {} statements ;->; Control flow \u00b6 program runs in top-down decision-making statements if-then-else if (testscore >= 90) { grade = 'A'; } else if (testscore >= 60) { grade = 'D'; } else { grade = 'F'; } switch switch (month) { case 1: monthString = \"January\"; break; default: monthString = \"Invalid month\"; break; } the looping statements (for, while, do-while) while (expression) { statement(s) } do { statement(s) } while (expression); for (initialization; termination; increment) { statement(s) } branching statements (break, continue, return) search: for (i = 0; i < arrayOfInts.length; i++) { for (j = 0; j < arrayOfInts[i].length; j++) { if (arrayOfInts[i][j] == searchfor) { foundIt = true; break search; } } } if (foundIt) { System.out.println(\"Found \" + searchfor + \" at \" + i + \", \" + j); } else { System.out.println(searchfor + \" not in the array\"); } access modifiers \u00b6 public field is accessible from all classes. private field is accessible only within its own class. protected - same class and derived classes","title":"Basics"},{"location":"java/basics.html#basics","text":"","title":"Basics"},{"location":"java/basics.html#variables","text":"Instance Variables (Non-Static Fields) Class Variables (Static Fields) Local Variables: bw {}. Parameters: always classified as \"variables\" not \"fields\" long creditCardNumber = 1234_5678_9012_3456L; primitive data types are: byte, short, int, long, float, double, boolean, and char. java.lang.String class","title":"Variables"},{"location":"java/basics.html#naming","text":"case sensitive unlimited-length sequence of Unicode letters and digits (avoid _ and $ in start) not keyword or reserved word gearRatio NUM_GEARS = 6","title":"Naming"},{"location":"java/basics.html#arrays","text":"its length is fixed. int[] anArray = new int[10]; String[][] names = { {\"Mr. \", \"Mrs. \", \"Ms. \"}, {\"Smith\", \"Jones\"} }; System.arraycopy(copyFrom, 2, copyTo, 0, 7); java.util.Arrays.copyOfRange(copyFrom, 2, 9);","title":"Arrays"},{"location":"java/basics.html#javautilarrays","text":"binarySearch equals fill parallelSort java.util.Arrays.stream(copyTo).map(coffee -> coffee + \" \").forEach(System.out::print) System.out.println(java.util.Arrays.toString(copyTo))","title":"java.util.Arrays"},{"location":"java/basics.html#operators","text":"specific operations on one, two, or three operands, and then return a result. Simple Assignment Operator = Arithmetic Operators + Additive operator (also used for String concatenation) - Subtraction operator * Multiplication operator / Division operator % Remainder operator Unary Operators + Unary plus operator; indicates positive value (numbers are positive without this, however) - Unary minus operator; negates an expression ++ Increment operator; increments a value by 1 -- Decrement operator; decrements a value by 1 ! Logical complement operator; inverts the value of a boolean Equality and Relational Operators == Equal to != Not equal to > Greater than >= Greater than or equal to < Less than <= Less than or equal to Conditional Operators && Conditional-AND || Conditional-OR ?: Ternary (shorthand for if-then-else statement) Type Comparison Operator instanceof Compares an object to a specified type Bitwise and Bit Shift Operators ~ Unary bitwise complement << Signed left shift >> Signed right shift >>> Unsigned right shift & Bitwise AND ^ Bitwise exclusive OR | Bitwise inclusive OR Expressions -> result=9 blocks -> {} statements ;->;","title":"Operators"},{"location":"java/basics.html#control-flow","text":"program runs in top-down decision-making statements if-then-else if (testscore >= 90) { grade = 'A'; } else if (testscore >= 60) { grade = 'D'; } else { grade = 'F'; } switch switch (month) { case 1: monthString = \"January\"; break; default: monthString = \"Invalid month\"; break; } the looping statements (for, while, do-while) while (expression) { statement(s) } do { statement(s) } while (expression); for (initialization; termination; increment) { statement(s) } branching statements (break, continue, return) search: for (i = 0; i < arrayOfInts.length; i++) { for (j = 0; j < arrayOfInts[i].length; j++) { if (arrayOfInts[i][j] == searchfor) { foundIt = true; break search; } } } if (foundIt) { System.out.println(\"Found \" + searchfor + \" at \" + i + \", \" + j); } else { System.out.println(searchfor + \" not in the array\"); }","title":"Control flow"},{"location":"java/basics.html#access-modifiers","text":"public field is accessible from all classes. private field is accessible only within its own class. protected - same class and derived classes","title":"access modifiers"},{"location":"java/classes.html","text":"Classes \u00b6 private class MyClass extends MySuperClass implements YourInterface { // field, constructor, and // method declarations } Member variables -> fields. Variables in a method or block of code -> local variables. Variables in method declarations -> parameters. Overloading \u00b6 public class DataArtist { ... public void draw(String s) { ... } public void draw(int i) { ... } public void draw(double f) { ... } public void draw(int i, double f) { ... } } Arbitrary no. of Parameters \u00b6 public PrintStream printf(String format, Object... args) Creating objects \u00b6 Declaration: type name; Instantiation: new Class() Initialization: new Class() This \u00b6 Access Specifier \u00b6 Modifier Class Package Subclass World public Y Y Y Y protected Y Y Y N no modifier Y Y N N private Y N N N NestedClass \u00b6 class OuterClass { ... <access> class NestedClass { ... } } Why \u00b6 It is a way of logically grouping classes that are only used in one place It increases encapsulation It can lead to more readable and maintainable code Shadowing \u00b6 If a declaration of a type (such as a member variable or a parameter name) in a particular scope (such as an inner class or a method definition) has the same name as another declaration in the enclosing scope, then the declaration shadows the declaration of the enclosing scope. public class ShadowTest { public int x = 0; class FirstLevel { public int x = 1; void methodInFirstLevel(int x) { System.out.println(\"x = \" + x); System.out.println(\"this.x = \" + this.x); System.out.println(\"ShadowTest.this.x = \" + ShadowTest.this.x); } } public static void main(String... args) { ShadowTest st = new ShadowTest(); ShadowTest.FirstLevel fl = st.new FirstLevel(); fl.methodInFirstLevel(23); } } Output: x = 23 this.x = 1 ShadowTest.this.x = 0 Serialization \u00b6 Serialization of inner classes, including local and anonymous classes, is strongly discouraged. When the Java compiler compiles certain constructs, such as inner classes, it creates synthetic constructs; these are classes, methods, fields, and other constructs that do not have a corresponding construct in the source code. Synthetic constructs enable Java compilers to implement new Java language features without changes to the JVM. However, synthetic constructs can vary among different Java compiler implementations, which means that .class files can vary among different implementations as well. Consequently, you may have compatibility issues if you serialize an inner class and then deserialize it with a different JRE implementation. See the section Implicit and Synthetic Parameters in the section Obtaining Names of Method Parameters for more information about the synthetic constructs generated when an inner class is compiled. Local and Anonymous Classes \u00b6 There are two additional types of inner classes. You can declare an inner class within the body of a method. These classes are known as local classes. You can also declare an inner class within the body of a method without naming the class. These classes are known as anonymous classes. import javafx.event.ActionEvent; import javafx.event.EventHandler; import javafx.scene.Scene; import javafx.scene.control.Button; import javafx.scene.layout.StackPane; import javafx.stage.Stage; public class HelloWorld extends Application { public static void main(String[] args) { launch(args); } @Override public void start(Stage primaryStage) { primaryStage.setTitle(\"Hello World!\"); Button btn = new Button(); btn.setText(\"Say 'Hello World'\"); btn.setOnAction(new EventHandler<ActionEvent>() { @Override public void handle(ActionEvent event) { System.out.println(\"Hello World!\"); } }); StackPane root = new StackPane(); root.getChildren().add(btn); primaryStage.setScene(new Scene(root, 300, 250)); primaryStage.show(); } } Lambda Expressions \u00b6 p -> p.getGender() == Person.Sex.MALE && p.getAge() >= 18 && p.getAge() <= 25 Ideall use cases: Field Description Name Perform action on selected members Primary Actor Administrator Preconditions Administrator is logged in to the system. Postconditions Action is performed only on members that fit the specified criteria. Frequency of Occurrence Many times during the day. Extensions Administrator has an option to preview those members who match the specified criteria before he or she specifies the action to be performed or before selecting the Submit button. Main Success Scenario Administrator specifies criteria of members on which to perform a certain action. Administrator specifies an action to perform on those selected members. Administrator selects the Submit button. The system finds all members that match the specified criteria. The system performs the specified action on all matching members. Serialization \u00b6 When to Use which \u00b6 As mentioned in the section Nested Classes, nested classes enable you to logically group classes that are only used in one place, increase the use of encapsulation, and create more readable and maintainable code. Local classes, anonymous classes, and lambda expressions also impart these advantages; however, they are intended to be used for more specific situations: Local class: Use it if you need to create more than one instance of a class, access its constructor, or introduce a new, named type (because, for example, you need to invoke additional methods later). Anonymous class: Use it if you need to declare fields or additional methods. Lambda expression: Use it if you are encapsulating a single unit of behavior that you want to pass to other code. For example, you would use a lambda expression if you want a certain action performed on each element of a collection, when a process is completed, or when a process encounters an error. Use it if you need a simple instance of a functional interface and none of the preceding criteria apply (for example, you do not need a constructor, a named type, fields, or additional methods). Nested class: Use it if your requirements are similar to those of a local class, you want to make the type more widely available, and you don't require access to local variables or method parameters. Use a non-static nested class (or inner class) if you require access to an enclosing instance's non-public fields and methods. Use a static nested class if you don't require this access. Method \u00b6 Kind Syntax Examples Reference to a static method ContainingClass::staticMethodName Person::compareByAge MethodReferencesExamples::appendStrings Reference to an instance method of a particular object containingObject::instanceMethodName myComparisonProvider::compareByName myApp::appendStrings2 Reference to an instance method of an arbitrary object of a particular type ContainingType::methodName String::compareToIgnoreCase String::concat Reference to a constructor ClassName::new HashSet::new TODO Link","title":"Classes"},{"location":"java/classes.html#classes","text":"private class MyClass extends MySuperClass implements YourInterface { // field, constructor, and // method declarations } Member variables -> fields. Variables in a method or block of code -> local variables. Variables in method declarations -> parameters.","title":"Classes"},{"location":"java/classes.html#overloading","text":"public class DataArtist { ... public void draw(String s) { ... } public void draw(int i) { ... } public void draw(double f) { ... } public void draw(int i, double f) { ... } }","title":"Overloading"},{"location":"java/classes.html#arbitrary-no-of-parameters","text":"public PrintStream printf(String format, Object... args)","title":"Arbitrary no. of Parameters"},{"location":"java/classes.html#creating-objects","text":"Declaration: type name; Instantiation: new Class() Initialization: new Class()","title":"Creating objects"},{"location":"java/classes.html#this","text":"","title":"This"},{"location":"java/classes.html#access-specifier","text":"Modifier Class Package Subclass World public Y Y Y Y protected Y Y Y N no modifier Y Y N N private Y N N N","title":"Access Specifier"},{"location":"java/classes.html#nestedclass","text":"class OuterClass { ... <access> class NestedClass { ... } }","title":"NestedClass"},{"location":"java/classes.html#why","text":"It is a way of logically grouping classes that are only used in one place It increases encapsulation It can lead to more readable and maintainable code","title":"Why"},{"location":"java/classes.html#shadowing","text":"If a declaration of a type (such as a member variable or a parameter name) in a particular scope (such as an inner class or a method definition) has the same name as another declaration in the enclosing scope, then the declaration shadows the declaration of the enclosing scope. public class ShadowTest { public int x = 0; class FirstLevel { public int x = 1; void methodInFirstLevel(int x) { System.out.println(\"x = \" + x); System.out.println(\"this.x = \" + this.x); System.out.println(\"ShadowTest.this.x = \" + ShadowTest.this.x); } } public static void main(String... args) { ShadowTest st = new ShadowTest(); ShadowTest.FirstLevel fl = st.new FirstLevel(); fl.methodInFirstLevel(23); } } Output: x = 23 this.x = 1 ShadowTest.this.x = 0","title":"Shadowing"},{"location":"java/classes.html#serialization","text":"Serialization of inner classes, including local and anonymous classes, is strongly discouraged. When the Java compiler compiles certain constructs, such as inner classes, it creates synthetic constructs; these are classes, methods, fields, and other constructs that do not have a corresponding construct in the source code. Synthetic constructs enable Java compilers to implement new Java language features without changes to the JVM. However, synthetic constructs can vary among different Java compiler implementations, which means that .class files can vary among different implementations as well. Consequently, you may have compatibility issues if you serialize an inner class and then deserialize it with a different JRE implementation. See the section Implicit and Synthetic Parameters in the section Obtaining Names of Method Parameters for more information about the synthetic constructs generated when an inner class is compiled.","title":"Serialization"},{"location":"java/classes.html#local-and-anonymous-classes","text":"There are two additional types of inner classes. You can declare an inner class within the body of a method. These classes are known as local classes. You can also declare an inner class within the body of a method without naming the class. These classes are known as anonymous classes. import javafx.event.ActionEvent; import javafx.event.EventHandler; import javafx.scene.Scene; import javafx.scene.control.Button; import javafx.scene.layout.StackPane; import javafx.stage.Stage; public class HelloWorld extends Application { public static void main(String[] args) { launch(args); } @Override public void start(Stage primaryStage) { primaryStage.setTitle(\"Hello World!\"); Button btn = new Button(); btn.setText(\"Say 'Hello World'\"); btn.setOnAction(new EventHandler<ActionEvent>() { @Override public void handle(ActionEvent event) { System.out.println(\"Hello World!\"); } }); StackPane root = new StackPane(); root.getChildren().add(btn); primaryStage.setScene(new Scene(root, 300, 250)); primaryStage.show(); } }","title":"Local and Anonymous Classes"},{"location":"java/classes.html#lambda-expressions","text":"p -> p.getGender() == Person.Sex.MALE && p.getAge() >= 18 && p.getAge() <= 25 Ideall use cases: Field Description Name Perform action on selected members Primary Actor Administrator Preconditions Administrator is logged in to the system. Postconditions Action is performed only on members that fit the specified criteria. Frequency of Occurrence Many times during the day. Extensions Administrator has an option to preview those members who match the specified criteria before he or she specifies the action to be performed or before selecting the Submit button. Main Success Scenario Administrator specifies criteria of members on which to perform a certain action. Administrator specifies an action to perform on those selected members. Administrator selects the Submit button. The system finds all members that match the specified criteria. The system performs the specified action on all matching members.","title":"Lambda Expressions"},{"location":"java/classes.html#serialization_1","text":"","title":"Serialization"},{"location":"java/classes.html#when-to-use-which","text":"As mentioned in the section Nested Classes, nested classes enable you to logically group classes that are only used in one place, increase the use of encapsulation, and create more readable and maintainable code. Local classes, anonymous classes, and lambda expressions also impart these advantages; however, they are intended to be used for more specific situations: Local class: Use it if you need to create more than one instance of a class, access its constructor, or introduce a new, named type (because, for example, you need to invoke additional methods later). Anonymous class: Use it if you need to declare fields or additional methods. Lambda expression: Use it if you are encapsulating a single unit of behavior that you want to pass to other code. For example, you would use a lambda expression if you want a certain action performed on each element of a collection, when a process is completed, or when a process encounters an error. Use it if you need a simple instance of a functional interface and none of the preceding criteria apply (for example, you do not need a constructor, a named type, fields, or additional methods). Nested class: Use it if your requirements are similar to those of a local class, you want to make the type more widely available, and you don't require access to local variables or method parameters. Use a non-static nested class (or inner class) if you require access to an enclosing instance's non-public fields and methods. Use a static nested class if you don't require this access.","title":"When to Use which"},{"location":"java/classes.html#method","text":"Kind Syntax Examples Reference to a static method ContainingClass::staticMethodName Person::compareByAge MethodReferencesExamples::appendStrings Reference to an instance method of a particular object containingObject::instanceMethodName myComparisonProvider::compareByName myApp::appendStrings2 Reference to an instance method of an arbitrary object of a particular type ContainingType::methodName String::compareToIgnoreCase String::concat Reference to a constructor ClassName::new HashSet::new TODO Link","title":"Method"},{"location":"java/collections.html","text":"Collections \u00b6 A collection \u2014 sometimes called a container \u2014 is simply an object that groups multiple elements into a single unit. Collections are used to store, retrieve, manipulate, and communicate aggregate data. collections framework is a unified architecture for representing and manipulating collections. eg c++ STL Interfaces: These are abstract data types that represent collections. form a hierarchy. Implementations: These are the concrete implementations of the collection interfaces. reusable DS. Algorithms: These are the methods that perform useful computations, polymorphic, reusable functionality. Why? \u00b6 Reduces programming effort Increases program speed and quality Allows interoperability among unrelated APIs Reduces effort to learn and to use new APIs Reduces effort to design new APIs Fosters software reuse interoperability among unrelated APIs Collections: * Set * SortedSet * List * Queue * Dequeue Map * SortedMap Interfaces \u00b6 These interfaces allow collections to be manipulated independently of the details of their representation. public interface Collection<E> Collection \u2014 the root of the collection hierarchy. A collection represents a group of objects known as its elements. Set \u2014 a collection that cannot contain duplicate elements. List \u2014 an ordered collection (sometimes called a sequence). Lists can contain duplicate elements. eg Vector Queue \u2014 a collection used to hold multiple elements prior to processing. Queues typically, but do not necessarily, order elements in a FIFO (first-in, first-out) manner. priority queues. ordering properties. Deque \u2014 a collection used to hold multiple elements prior to processing. both ends-> insertion, deletion, removed. Map \u2014 an object that maps keys to values. SortedSet \u2014 a Set that maintains its elements in ascending order. SortedMap \u2014 a Map that maintains its mappings in ascending key order. Traversing Collections \u00b6 using aggregate operations with the for-each construct by using Iterators. myShapesCollection.stream() .filter(e -> e.getColor() == Color.RED) .forEach(e -> System.out.println(e.getName())); String joined = elements.stream() .map(Object::toString) .collect(Collectors.joining(\", \")); int total = employees.stream() .collect(Collectors.summingInt(Employee::getSalary))); The key difference between the new aggregate operations and the existing bulk operations (containsAll, addAll, etc.) is that the old versions are all mutative, meaning that they all modify the underlying collection. for (Object o : collection) System.out.println(o); public interface Iterator<E> { boolean hasNext(); E next(); void remove(); //optional } polymorphic: static void filter(Collection<?> c) { for (Iterator<?> it = c.iterator(); it.hasNext(); ) if (!cond(it.next())) it.remove(); } c.removeAll(Collections.singleton(e)); import java.util.*; import java.io.*; public class Anagrams { public static void main(String[] args) { int minGroupSize = Integer.parseInt(args[1]); // Read words from file and put into a simulated multimap Map<String, List<String>> m = new HashMap<String, List<String>>(); try { Scanner s = new Scanner(new File(args[0])); while (s.hasNext()) { String word = s.next(); String alpha = alphabetize(word); List<String> l = m.get(alpha); if (l == null) m.put(alpha, l=new ArrayList<String>()); l.add(word); } } catch (IOException e) { System.err.println(e); System.exit(1); } // Print all permutation groups above size threshold for (List<String> l : m.values()) if (l.size() >= minGroupSize) System.out.println(l.size() + \": \" + l); } private static String alphabetize(String s) { char[] a = s.toCharArray(); Arrays.sort(a); return new String(a); } } Aggregate Operations \u00b6 double average = roster .stream() .filter(p -> p.getGender() == Person.Sex.MALE) .mapToInt(Person::getAge) .average() .getAsDouble(); Aggregate Operations vs Iterators \u00b6 They use internal iteration: It can more easily take advantage of parallel computing, which involves dividing a problem into subproblems, solving those problems simultaneously, and then combining the results of the solutions to the subproblems. Parallelism They process elements from a stream. They support behavior as parameters: lambda expressions Reduction \u00b6 Integer totalAgeReduce = roster .stream() .map(Person::getAge) .reduce( 0, // identity (a, b) -> a + b // accumulator ); Stream.collect Method \u00b6 Unlike the reduce method, which always creates a new value when it processes an element, the collect method modifies, or mutates, an existing value. Averager averageCollect = roster.stream() .filter(p -> p.getGender() == Person.Sex.MALE) .map(Person::getAge) .collect(Averager::new, // supplier Averager::accept, // accumulator Averager::combine // combiner ); System.out.println(\"Average age of male members: \" + averageCollect.average()); Map<Person.Sex, Integer> totalAgeByGender = roster .stream() .collect( Collectors.groupingBy( Person::getGender, Collectors.reducing( 0, // identity Person::getAge, // mapper Integer::sum))); // operation Algorithms \u00b6 Custom implementation \u00b6 Interoperability \u00b6 TODO Link","title":"Collections"},{"location":"java/collections.html#collections","text":"A collection \u2014 sometimes called a container \u2014 is simply an object that groups multiple elements into a single unit. Collections are used to store, retrieve, manipulate, and communicate aggregate data. collections framework is a unified architecture for representing and manipulating collections. eg c++ STL Interfaces: These are abstract data types that represent collections. form a hierarchy. Implementations: These are the concrete implementations of the collection interfaces. reusable DS. Algorithms: These are the methods that perform useful computations, polymorphic, reusable functionality.","title":"Collections"},{"location":"java/collections.html#why","text":"Reduces programming effort Increases program speed and quality Allows interoperability among unrelated APIs Reduces effort to learn and to use new APIs Reduces effort to design new APIs Fosters software reuse interoperability among unrelated APIs Collections: * Set * SortedSet * List * Queue * Dequeue Map * SortedMap","title":"Why?"},{"location":"java/collections.html#interfaces","text":"These interfaces allow collections to be manipulated independently of the details of their representation. public interface Collection<E> Collection \u2014 the root of the collection hierarchy. A collection represents a group of objects known as its elements. Set \u2014 a collection that cannot contain duplicate elements. List \u2014 an ordered collection (sometimes called a sequence). Lists can contain duplicate elements. eg Vector Queue \u2014 a collection used to hold multiple elements prior to processing. Queues typically, but do not necessarily, order elements in a FIFO (first-in, first-out) manner. priority queues. ordering properties. Deque \u2014 a collection used to hold multiple elements prior to processing. both ends-> insertion, deletion, removed. Map \u2014 an object that maps keys to values. SortedSet \u2014 a Set that maintains its elements in ascending order. SortedMap \u2014 a Map that maintains its mappings in ascending key order.","title":"Interfaces"},{"location":"java/collections.html#traversing-collections","text":"using aggregate operations with the for-each construct by using Iterators. myShapesCollection.stream() .filter(e -> e.getColor() == Color.RED) .forEach(e -> System.out.println(e.getName())); String joined = elements.stream() .map(Object::toString) .collect(Collectors.joining(\", \")); int total = employees.stream() .collect(Collectors.summingInt(Employee::getSalary))); The key difference between the new aggregate operations and the existing bulk operations (containsAll, addAll, etc.) is that the old versions are all mutative, meaning that they all modify the underlying collection. for (Object o : collection) System.out.println(o); public interface Iterator<E> { boolean hasNext(); E next(); void remove(); //optional } polymorphic: static void filter(Collection<?> c) { for (Iterator<?> it = c.iterator(); it.hasNext(); ) if (!cond(it.next())) it.remove(); } c.removeAll(Collections.singleton(e)); import java.util.*; import java.io.*; public class Anagrams { public static void main(String[] args) { int minGroupSize = Integer.parseInt(args[1]); // Read words from file and put into a simulated multimap Map<String, List<String>> m = new HashMap<String, List<String>>(); try { Scanner s = new Scanner(new File(args[0])); while (s.hasNext()) { String word = s.next(); String alpha = alphabetize(word); List<String> l = m.get(alpha); if (l == null) m.put(alpha, l=new ArrayList<String>()); l.add(word); } } catch (IOException e) { System.err.println(e); System.exit(1); } // Print all permutation groups above size threshold for (List<String> l : m.values()) if (l.size() >= minGroupSize) System.out.println(l.size() + \": \" + l); } private static String alphabetize(String s) { char[] a = s.toCharArray(); Arrays.sort(a); return new String(a); } }","title":"Traversing Collections"},{"location":"java/collections.html#aggregate-operations","text":"double average = roster .stream() .filter(p -> p.getGender() == Person.Sex.MALE) .mapToInt(Person::getAge) .average() .getAsDouble();","title":"Aggregate Operations"},{"location":"java/collections.html#aggregate-operations-vs-iterators","text":"They use internal iteration: It can more easily take advantage of parallel computing, which involves dividing a problem into subproblems, solving those problems simultaneously, and then combining the results of the solutions to the subproblems. Parallelism They process elements from a stream. They support behavior as parameters: lambda expressions","title":"Aggregate Operations vs Iterators"},{"location":"java/collections.html#reduction","text":"Integer totalAgeReduce = roster .stream() .map(Person::getAge) .reduce( 0, // identity (a, b) -> a + b // accumulator );","title":"Reduction"},{"location":"java/collections.html#streamcollect-method","text":"Unlike the reduce method, which always creates a new value when it processes an element, the collect method modifies, or mutates, an existing value. Averager averageCollect = roster.stream() .filter(p -> p.getGender() == Person.Sex.MALE) .map(Person::getAge) .collect(Averager::new, // supplier Averager::accept, // accumulator Averager::combine // combiner ); System.out.println(\"Average age of male members: \" + averageCollect.average()); Map<Person.Sex, Integer> totalAgeByGender = roster .stream() .collect( Collectors.groupingBy( Person::getGender, Collectors.reducing( 0, // identity Person::getAge, // mapper Integer::sum))); // operation","title":"Stream.collect Method"},{"location":"java/collections.html#algorithms","text":"","title":"Algorithms"},{"location":"java/collections.html#custom-implementation","text":"","title":"Custom implementation"},{"location":"java/collections.html#interoperability","text":"TODO Link","title":"Interoperability"},{"location":"java/concurrency.html","text":"Concurrency \u00b6 Computer users take it for granted that their systems can do more than one thing at a time. Two basic units of execution: Processes Threads In java, we mainly deal with threads. Processing time for a single core is shared among processes and threads through an OS feature called time slicing . Process \u00b6 A process has a self-contained execution environment. (A complete, private set of basic run-time resources: memory space atleast) An application is a set of processes comunicating with each other. OS support Inter Process Communication (IPC) resources, such as pipes and sockets. Most programs in JVM run as a single process, if you want multiple processes extend ProcessBuilder . Threads \u00b6 lightweight processes (lower resource requirement) Threads share the process's resources, including memory and open files. This makes for efficient, but potentially problematic, communication. Thread Objects \u00b6 2 basic strategies for using Thread objects to create a concurrent application: To directly control thread creation and management, simply instantiate Thread each time the application needs to initiate an asynchronous task. To abstract thread management from the rest of your application, pass the application's tasks to an executor . Defination \u00b6 Thread must provide the code that will run in that thread. Provide a Runnable object. This approach more flexible and it is applicable to the high-level thread management APIs. public class HelloRunnable implements Runnable { public void run() { System.out.println(\"Hello from a thread!\"); } public static void main(String args[]) { (new Thread(new HelloRunnable())).start(); } } Subclass Thread. The Thread class itself implements Runnable, though its run method does nothing The Thread class defines a number of methods useful for thread management. public class HelloThread extends Thread { public void run() { System.out.println(\"Hello from a thread!\"); } public static void main(String args[]) { (new HelloThread()).start(); } } Operations \u00b6 Sleep \u00b6 public class SleepMessages { public static void main(String args[]) throws InterruptedException { String importantInfo[] = { \"Mares eat oats\", \"Does eat oats\", \"Little lambs eat ivy\", \"A kid will eat ivy too\" }; for (int i = 0; i < importantInfo.length; i++) { Thread.sleep(4000); System.out.println(importantInfo[i]); } } } Interrupts \u00b6 An interrupt is an indication to a thread that it should stop what it is doing and do something else. for (int i = 0; i < importantInfo.length; i++) { // Pause for 4 seconds try { Thread.sleep(4000); } catch (InterruptedException e) { // We've been interrupted: no more messages. return; } // Print a message System.out.println(importantInfo[i]); } for (int i = 0; i < inputs.length; i++) { heavyCrunch(inputs[i]); if (Thread.interrupted()) { // We've been interrupted: no more crunching. return; } } if (Thread.interrupted()) { throw new InterruptedException(); } The Interrupt Status Flag Invoking Thread.interrupt sets this flag. When a thread checks for an interrupt by invoking the static method Thread.interrupted, interrupt status is cleared. The non-static isInterrupted method, which is used by one thread to query the interrupt status of another, does not change the interrupt status flag. Joins \u00b6 Join method allows one thread to wait for the completion of another t.join(); Causes the current thread to pause execution until t's thread terminates Overloads of join allow the programmer to specify a waiting period. However, as with sleep, join is dependent on the OS for timing The Simple Thread \u00b6 public class SimpleThreads { static void threadMessage(String message) { String threadName = Thread.currentThread().getName(); System.out.format(\"%s: %s%n\", threadName, message); } private static class MessageLoop implements Runnable { public void run() { String importantInfo[] = { \"Mares eat oats\", \"Does eat oats\", \"Little lambs eat ivy\", \"A kid will eat ivy too\" }; try { for (int i = 0; i < importantInfo.length; i++) { Thread.sleep(4000); threadMessage(importantInfo[i]); } } catch (InterruptedException e) { threadMessage(\"I wasn't done!\"); } } } public static void main(String args[]) throws InterruptedException { long patience = 1000 * 60 * 60; if (args.length > 0) { try { patience = Long.parseLong(args[0]) * 1000; } catch (NumberFormatException e) { System.err.println(\"Argument must be an integer.\"); System.exit(1); } } threadMessage(\"Starting MessageLoop thread\"); long startTime = System.currentTimeMillis(); Thread t = new Thread(new MessageLoop()); t.start(); threadMessage(\"Waiting for MessageLoop thread to finish\"); while (t.isAlive()) { threadMessage(\"Still waiting...\"); t.join(1000); if (((System.currentTimeMillis() - startTime) > patience) && t.isAlive()) { threadMessage(\"Tired of waiting!\"); t.interrupt(); t.join(); } } threadMessage(\"Finally!\"); } } The main thread creates a new thread from the Runnable object, MessageLoop, and waits for it to finish. If the MessageLoop thread takes too long to finish, the main thread interrupts it. Synchronisation \u00b6 Threads communicate primarily by sharing access to fields and the objects reference fields refer to. Problems: Thread interference Memory consistency errors Synchronisation solves it but a problem of thread contention (Starvation, Livelock etc) arises, which occurs when two or more threads try to access the same resource simultaneously and cause the Java runtime to execute one or more threads more slowly, or even suspend their execution. two basic synchronization idioms: synchronized methods synchronized statements. Thread Interference \u00b6 Interference happens when two operations, running in different threads, but acting on the same data, interleave. The single expression c++ can be decomposed into: Retrieve the current value of c. Increment it by 1. Store the value back in c. eg. Thread A: Retrieve c. Thread B: Retrieve c. Thread A: Increment retrieved value; result is 1. Thread B: Decrement retrieved value; result is -1. Thread A: Store result in c; c is now 1. Thread B: Store result in c; c is now -1. Memory Consistency Errors \u00b6 Memory consistency errors occur when different threads have inconsistent views of what should be the same data. Understanding the cause is difficult, just focus on avoiding them. The key to avoiding memory consistency errors is understanding the happens-before relationship : When a statement invokes Thread.start, every statement that has a happens-before relationship with that statement also has a happens-before relationship with every statement executed by the new thread. The effects of the code that led up to the creation of the new thread are visible to the new thread. When a thread terminates and causes a Thread.join in another thread to return, then all the statements executed by the terminated thread have a happens-before relationship with all the statements following the successful join. The effects of the code in the thread are now visible to the thread that performed the join. Liveness \u00b6 A concurrent application's ability to execute in a timely manner is known as its liveness. Deadlock \u00b6 Deadlock describes a situation where two or more threads are blocked forever, waiting for each other. public class Deadlock { static class Friend { private final String name; public Friend(String name) { this.name = name; } public String getName() { return this.name; } public synchronized void bow(Friend bower) { System.out.format(\"%s: %s\" + \" has bowed to me!%n\", this.name, bower.getName()); bower.bowBack(this); } public synchronized void bowBack(Friend bower) { System.out.format(\"%s: %s\" + \" has bowed back to me!%n\", this.name, bower.getName()); } } public static void main(String[] args) { final Friend alphonse = new Friend(\"Alphonse\"); final Friend gaston = new Friend(\"Gaston\"); new Thread(new Runnable() { public void run() { alphonse.bow(gaston); } }).start(); new Thread(new Runnable() { public void run() { gaston.bow(alphonse); } }).start(); } } Starvation \u00b6 Starvation describes a situation where a thread is unable to gain regular access to shared resources and is unable to make progress. This happens when shared resources are made unavailable for long periods by \"greedy\" threads. Livelock \u00b6 A thread often acts in response to the action of another thread. If the other thread's action is also a response to the action of another thread, then livelock may result. This is comparable to two people attempting to pass each other in a corridor: Alphonse moves to his left to let Gaston pass, while Gaston moves to his right to let Alphonse pass. Seeing that they are still blocking each other, Alphone moves to his right, while Gaston moves to his left. They're still blocking each other, so.. Guarded Blocks \u00b6 Threads often have to coordinate their actions. The most common coordination idiom is the guarded block. Such a block begins by polling a condition that must be true before the block can proceed. public synchronized void guardedJoy() { while(!joy) { try { wait(); } catch (InterruptedException e) {} } System.out.println(\"Joy and efficiency have been achieved!\"); } Immutable objects \u00b6 An object is considered immutable if its state cannot change after it is constructed. Great for concurrency! The cost of creating a new object vs updating. But this can be optimised by: Decreased overhead due to garbage collection the elimination of code needed to protect mutable objects from corruption. public class SynchronizedRGB { // Values must be between 0 and 255. private int red; private int green; private int blue; private String name; private void check(int red, int green, int blue) { if (red < 0 || red > 255 || green < 0 || green > 255 || blue < 0 || blue > 255) { throw new IllegalArgumentException(); } } public SynchronizedRGB(int red, int green, int blue, String name) { check(red, green, blue); this.red = red; this.green = green; this.blue = blue; this.name = name; } public void set(int red, int green, int blue, String name) { check(red, green, blue); synchronized (this) { this.red = red; this.green = green; this.blue = blue; this.name = name; } } public synchronized int getRGB() { return ((red << 16) | (green << 8) | blue); } public synchronized String getName() { return name; } public synchronized void invert() { red = 255 - red; green = 255 - green; blue = 255 - blue; name = \"Inverse of \" + name; } } SynchronizedRGB color = new SynchronizedRGB(0, 0, 0, \"Pitch Black\"); ... int myColorInt = color.getRGB(); //Statement 1 String myColorName = color.getName(); //Statement 2 If another thread invokes color.set after Statement 1 but before Statement 2, the value of myColorInt won't match the value of myColorName. To avoid this outcome, the two statements must be bound together: synchronized (color) { int myColorInt = color.getRGB(); String myColorName = color.getName(); } This kind of inconsistency is only possible for mutable objects \u2014 it will not be an issue for the immutable version of SynchronizedRGB. Strategy \u00b6 Don't provide \"setter\" methods \u2014 methods that modify fields or objects referred to by fields. Make all fields final and private. Don't allow subclasses to override methods. The simplest way to do this is to declare the class as final. A more sophisticated approach is to make the constructor private and construct instances in factory methods. If the instance fields include references to mutable objects, don't allow those objects to be changed: Don't provide methods that modify the mutable objects. Don't share references to the mutable objects. Never store references to external, mutable objects passed to the constructor; if necessary, create copies, and store references to the copies. Similarly, create copies of your internal mutable objects when necessary to avoid returning the originals in your methods. High Level Concurrency Objects \u00b6 Lock Objects \u00b6 Synchronized code relies on a simple kind of reentrant lock. wait/notify mechanism, through their associated Condition objects. The tryLock method backs out if the lock is not available immediately or before a timeout expires (if specified). import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; import java.util.Random; public class Safelock { static class Friend { private final String name; private final Lock lock = new ReentrantLock(); public Friend(String name) { this.name = name; } public String getName() { return this.name; } public boolean impendingBow(Friend bower) { Boolean myLock = false; Boolean yourLock = false; try { myLock = lock.tryLock(); yourLock = bower.lock.tryLock(); } finally { if (! (myLock && yourLock)) { if (myLock) { lock.unlock(); } if (yourLock) { bower.lock.unlock(); } } } return myLock && yourLock; } public void bow(Friend bower) { if (impendingBow(bower)) { try { System.out.format(\"%s: %s has\" + \" bowed to me!%n\", this.name, bower.getName()); bower.bowBack(this); } finally { lock.unlock(); bower.lock.unlock(); } } else { System.out.format(\"%s: %s started\" + \" to bow to me, but saw that\" + \" I was already bowing to\" + \" him.%n\", this.name, bower.getName()); } } public void bowBack(Friend bower) { System.out.format(\"%s: %s has\" + \" bowed back to me!%n\", this.name, bower.getName()); } } static class BowLoop implements Runnable { private Friend bower; private Friend bowee; public BowLoop(Friend bower, Friend bowee) { this.bower = bower; this.bowee = bowee; } public void run() { Random random = new Random(); for (;;) { try { Thread.sleep(random.nextInt(10)); } catch (InterruptedException e) {} bowee.bow(bower); } } } public static void main(String[] args) { final Friend alphonse = new Friend(\"Alphonse\"); final Friend gaston = new Friend(\"Gaston\"); new Thread(new BowLoop(alphonse, gaston)).start(); new Thread(new BowLoop(gaston, alphonse)).start(); } } Executors \u00b6 In large-scale applications, it makes sense to separate thread management and creation from the rest of the application. Executor Interfaces \u00b6 Executor a simple interface that supports launching new tasks. The Executor interface provides a single method, execute, designed to be a drop-in replacement for a common thread-creation idiom. replace (new Thread(r)).start(); with e.execute(r); ( r -> Runnable, e -> Executor object) ExecutorService a subinterface of Executor, which adds features that help manage the life cycle, both of the individual tasks and of the executor itself. more versatile submit method. accepts Callable, Runnable objects. which allow the task to return a value. The submit method returns a Future object, which is used to retrieve the Callable return value and to manage the status of both Callable and Runnable tasks. provides methods for submitting large collections of Callable objects. provides a number of methods for managing the shutdown of the executor. ScheduledExecutorService a subinterface of ExecutorService, supports future and/or periodic execution of tasks. schedule, which executes a Runnable or Callable task after a specified delay. scheduleAtFixedRate and scheduleWithFixedDelay, which executes specified tasks repeatedly, at defined intervals. Thread Pools \u00b6 consist of worker threads. Using worker threads minimizes the overhead due to thread creation. fixed thread pool. This type of pool always has a specified number of threads running; applications using it degrade gracefully newFixedThreadPool factory method in java.util.concurrent.Executors: newCachedThreadPool-> executor with an expandable thread pool -> suitable for applications that launch many short-lived tasks. newSingleThreadExecutor -> creates an executor that executes a single task at a time. Several factory methods are ScheduledExecutorService versions of the above executors. java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ScheduledThreadPoolExecutor Fork/Join \u00b6 Concurrent Collections \u00b6 BlockingQueue defines a first-in-first-out data structure that blocks or times out when you attempt to add to a full queue, or retrieve from an empty queue. ConcurrentMap is a subinterface of java.util.Map that defines useful atomic operations. These operations remove or replace a key-value pair only if the key is present, or add a key-value pair only if the key is absent. Making these operations atomic helps avoid synchronization. The standard general-purpose implementation of ConcurrentMap is ConcurrentHashMap, which is a concurrent analog of HashMap. ConcurrentNavigableMap is a subinterface of ConcurrentMap that supports approximate matches. The standard general-purpose implementation of ConcurrentNavigableMap is ConcurrentSkipListMap, which is a concurrent analog of TreeMap. Atomic Variables \u00b6 The java.util.concurrent.atomic package defines classes that support atomic operations on single variables. All classes have get and set methods that work like reads and writes on volatile variables. That is, a set has a happens-before relationship with any subsequent get on the same variable. The atomic compareAndSet method also has these memory consistency features, as do the simple atomic arithmetic methods that apply to integer atomic variables. import java.util.concurrent.atomic.AtomicInteger; class AtomicCounter { private AtomicInteger c = new AtomicInteger(0); public void increment() { c.incrementAndGet(); } public void decrement() { c.decrementAndGet(); } public int value() { return c.get(); } } Concurrent Random Numbers \u00b6 ThreadLocalRandom -> random numbers from multiple threads or ForkJoinTasks. For concurrent access, using ThreadLocalRandom instead of Math.random() results in less contention and, ultimately, better performance. All you need to do is call ThreadLocalRandom.current(), then call one of its methods to retrieve a random number. Here is one example: int r = ThreadLocalRandom.current().nextInt(4, 77); Further Explorations \u00b6 Java Concurrent Animated : Animations that show usage of concurrency features. TODO LINK","title":"Concurrency"},{"location":"java/concurrency.html#concurrency","text":"Computer users take it for granted that their systems can do more than one thing at a time. Two basic units of execution: Processes Threads In java, we mainly deal with threads. Processing time for a single core is shared among processes and threads through an OS feature called time slicing .","title":"Concurrency"},{"location":"java/concurrency.html#process","text":"A process has a self-contained execution environment. (A complete, private set of basic run-time resources: memory space atleast) An application is a set of processes comunicating with each other. OS support Inter Process Communication (IPC) resources, such as pipes and sockets. Most programs in JVM run as a single process, if you want multiple processes extend ProcessBuilder .","title":"Process"},{"location":"java/concurrency.html#threads","text":"lightweight processes (lower resource requirement) Threads share the process's resources, including memory and open files. This makes for efficient, but potentially problematic, communication.","title":"Threads"},{"location":"java/concurrency.html#thread-objects","text":"2 basic strategies for using Thread objects to create a concurrent application: To directly control thread creation and management, simply instantiate Thread each time the application needs to initiate an asynchronous task. To abstract thread management from the rest of your application, pass the application's tasks to an executor .","title":"Thread Objects"},{"location":"java/concurrency.html#defination","text":"Thread must provide the code that will run in that thread. Provide a Runnable object. This approach more flexible and it is applicable to the high-level thread management APIs. public class HelloRunnable implements Runnable { public void run() { System.out.println(\"Hello from a thread!\"); } public static void main(String args[]) { (new Thread(new HelloRunnable())).start(); } } Subclass Thread. The Thread class itself implements Runnable, though its run method does nothing The Thread class defines a number of methods useful for thread management. public class HelloThread extends Thread { public void run() { System.out.println(\"Hello from a thread!\"); } public static void main(String args[]) { (new HelloThread()).start(); } }","title":"Defination"},{"location":"java/concurrency.html#operations","text":"","title":"Operations"},{"location":"java/concurrency.html#sleep","text":"public class SleepMessages { public static void main(String args[]) throws InterruptedException { String importantInfo[] = { \"Mares eat oats\", \"Does eat oats\", \"Little lambs eat ivy\", \"A kid will eat ivy too\" }; for (int i = 0; i < importantInfo.length; i++) { Thread.sleep(4000); System.out.println(importantInfo[i]); } } }","title":"Sleep"},{"location":"java/concurrency.html#interrupts","text":"An interrupt is an indication to a thread that it should stop what it is doing and do something else. for (int i = 0; i < importantInfo.length; i++) { // Pause for 4 seconds try { Thread.sleep(4000); } catch (InterruptedException e) { // We've been interrupted: no more messages. return; } // Print a message System.out.println(importantInfo[i]); } for (int i = 0; i < inputs.length; i++) { heavyCrunch(inputs[i]); if (Thread.interrupted()) { // We've been interrupted: no more crunching. return; } } if (Thread.interrupted()) { throw new InterruptedException(); } The Interrupt Status Flag Invoking Thread.interrupt sets this flag. When a thread checks for an interrupt by invoking the static method Thread.interrupted, interrupt status is cleared. The non-static isInterrupted method, which is used by one thread to query the interrupt status of another, does not change the interrupt status flag.","title":"Interrupts"},{"location":"java/concurrency.html#joins","text":"Join method allows one thread to wait for the completion of another t.join(); Causes the current thread to pause execution until t's thread terminates Overloads of join allow the programmer to specify a waiting period. However, as with sleep, join is dependent on the OS for timing","title":"Joins"},{"location":"java/concurrency.html#the-simple-thread","text":"public class SimpleThreads { static void threadMessage(String message) { String threadName = Thread.currentThread().getName(); System.out.format(\"%s: %s%n\", threadName, message); } private static class MessageLoop implements Runnable { public void run() { String importantInfo[] = { \"Mares eat oats\", \"Does eat oats\", \"Little lambs eat ivy\", \"A kid will eat ivy too\" }; try { for (int i = 0; i < importantInfo.length; i++) { Thread.sleep(4000); threadMessage(importantInfo[i]); } } catch (InterruptedException e) { threadMessage(\"I wasn't done!\"); } } } public static void main(String args[]) throws InterruptedException { long patience = 1000 * 60 * 60; if (args.length > 0) { try { patience = Long.parseLong(args[0]) * 1000; } catch (NumberFormatException e) { System.err.println(\"Argument must be an integer.\"); System.exit(1); } } threadMessage(\"Starting MessageLoop thread\"); long startTime = System.currentTimeMillis(); Thread t = new Thread(new MessageLoop()); t.start(); threadMessage(\"Waiting for MessageLoop thread to finish\"); while (t.isAlive()) { threadMessage(\"Still waiting...\"); t.join(1000); if (((System.currentTimeMillis() - startTime) > patience) && t.isAlive()) { threadMessage(\"Tired of waiting!\"); t.interrupt(); t.join(); } } threadMessage(\"Finally!\"); } } The main thread creates a new thread from the Runnable object, MessageLoop, and waits for it to finish. If the MessageLoop thread takes too long to finish, the main thread interrupts it.","title":"The Simple Thread"},{"location":"java/concurrency.html#synchronisation","text":"Threads communicate primarily by sharing access to fields and the objects reference fields refer to. Problems: Thread interference Memory consistency errors Synchronisation solves it but a problem of thread contention (Starvation, Livelock etc) arises, which occurs when two or more threads try to access the same resource simultaneously and cause the Java runtime to execute one or more threads more slowly, or even suspend their execution. two basic synchronization idioms: synchronized methods synchronized statements.","title":"Synchronisation"},{"location":"java/concurrency.html#thread-interference","text":"Interference happens when two operations, running in different threads, but acting on the same data, interleave. The single expression c++ can be decomposed into: Retrieve the current value of c. Increment it by 1. Store the value back in c. eg. Thread A: Retrieve c. Thread B: Retrieve c. Thread A: Increment retrieved value; result is 1. Thread B: Decrement retrieved value; result is -1. Thread A: Store result in c; c is now 1. Thread B: Store result in c; c is now -1.","title":"Thread Interference"},{"location":"java/concurrency.html#memory-consistency-errors","text":"Memory consistency errors occur when different threads have inconsistent views of what should be the same data. Understanding the cause is difficult, just focus on avoiding them. The key to avoiding memory consistency errors is understanding the happens-before relationship : When a statement invokes Thread.start, every statement that has a happens-before relationship with that statement also has a happens-before relationship with every statement executed by the new thread. The effects of the code that led up to the creation of the new thread are visible to the new thread. When a thread terminates and causes a Thread.join in another thread to return, then all the statements executed by the terminated thread have a happens-before relationship with all the statements following the successful join. The effects of the code in the thread are now visible to the thread that performed the join.","title":"Memory Consistency Errors"},{"location":"java/concurrency.html#liveness","text":"A concurrent application's ability to execute in a timely manner is known as its liveness.","title":"Liveness"},{"location":"java/concurrency.html#deadlock","text":"Deadlock describes a situation where two or more threads are blocked forever, waiting for each other. public class Deadlock { static class Friend { private final String name; public Friend(String name) { this.name = name; } public String getName() { return this.name; } public synchronized void bow(Friend bower) { System.out.format(\"%s: %s\" + \" has bowed to me!%n\", this.name, bower.getName()); bower.bowBack(this); } public synchronized void bowBack(Friend bower) { System.out.format(\"%s: %s\" + \" has bowed back to me!%n\", this.name, bower.getName()); } } public static void main(String[] args) { final Friend alphonse = new Friend(\"Alphonse\"); final Friend gaston = new Friend(\"Gaston\"); new Thread(new Runnable() { public void run() { alphonse.bow(gaston); } }).start(); new Thread(new Runnable() { public void run() { gaston.bow(alphonse); } }).start(); } }","title":"Deadlock"},{"location":"java/concurrency.html#starvation","text":"Starvation describes a situation where a thread is unable to gain regular access to shared resources and is unable to make progress. This happens when shared resources are made unavailable for long periods by \"greedy\" threads.","title":"Starvation"},{"location":"java/concurrency.html#livelock","text":"A thread often acts in response to the action of another thread. If the other thread's action is also a response to the action of another thread, then livelock may result. This is comparable to two people attempting to pass each other in a corridor: Alphonse moves to his left to let Gaston pass, while Gaston moves to his right to let Alphonse pass. Seeing that they are still blocking each other, Alphone moves to his right, while Gaston moves to his left. They're still blocking each other, so..","title":"Livelock"},{"location":"java/concurrency.html#guarded-blocks","text":"Threads often have to coordinate their actions. The most common coordination idiom is the guarded block. Such a block begins by polling a condition that must be true before the block can proceed. public synchronized void guardedJoy() { while(!joy) { try { wait(); } catch (InterruptedException e) {} } System.out.println(\"Joy and efficiency have been achieved!\"); }","title":"Guarded Blocks"},{"location":"java/concurrency.html#immutable-objects","text":"An object is considered immutable if its state cannot change after it is constructed. Great for concurrency! The cost of creating a new object vs updating. But this can be optimised by: Decreased overhead due to garbage collection the elimination of code needed to protect mutable objects from corruption. public class SynchronizedRGB { // Values must be between 0 and 255. private int red; private int green; private int blue; private String name; private void check(int red, int green, int blue) { if (red < 0 || red > 255 || green < 0 || green > 255 || blue < 0 || blue > 255) { throw new IllegalArgumentException(); } } public SynchronizedRGB(int red, int green, int blue, String name) { check(red, green, blue); this.red = red; this.green = green; this.blue = blue; this.name = name; } public void set(int red, int green, int blue, String name) { check(red, green, blue); synchronized (this) { this.red = red; this.green = green; this.blue = blue; this.name = name; } } public synchronized int getRGB() { return ((red << 16) | (green << 8) | blue); } public synchronized String getName() { return name; } public synchronized void invert() { red = 255 - red; green = 255 - green; blue = 255 - blue; name = \"Inverse of \" + name; } } SynchronizedRGB color = new SynchronizedRGB(0, 0, 0, \"Pitch Black\"); ... int myColorInt = color.getRGB(); //Statement 1 String myColorName = color.getName(); //Statement 2 If another thread invokes color.set after Statement 1 but before Statement 2, the value of myColorInt won't match the value of myColorName. To avoid this outcome, the two statements must be bound together: synchronized (color) { int myColorInt = color.getRGB(); String myColorName = color.getName(); } This kind of inconsistency is only possible for mutable objects \u2014 it will not be an issue for the immutable version of SynchronizedRGB.","title":"Immutable objects"},{"location":"java/concurrency.html#strategy","text":"Don't provide \"setter\" methods \u2014 methods that modify fields or objects referred to by fields. Make all fields final and private. Don't allow subclasses to override methods. The simplest way to do this is to declare the class as final. A more sophisticated approach is to make the constructor private and construct instances in factory methods. If the instance fields include references to mutable objects, don't allow those objects to be changed: Don't provide methods that modify the mutable objects. Don't share references to the mutable objects. Never store references to external, mutable objects passed to the constructor; if necessary, create copies, and store references to the copies. Similarly, create copies of your internal mutable objects when necessary to avoid returning the originals in your methods.","title":"Strategy"},{"location":"java/concurrency.html#high-level-concurrency-objects","text":"","title":"High Level Concurrency Objects"},{"location":"java/concurrency.html#lock-objects","text":"Synchronized code relies on a simple kind of reentrant lock. wait/notify mechanism, through their associated Condition objects. The tryLock method backs out if the lock is not available immediately or before a timeout expires (if specified). import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; import java.util.Random; public class Safelock { static class Friend { private final String name; private final Lock lock = new ReentrantLock(); public Friend(String name) { this.name = name; } public String getName() { return this.name; } public boolean impendingBow(Friend bower) { Boolean myLock = false; Boolean yourLock = false; try { myLock = lock.tryLock(); yourLock = bower.lock.tryLock(); } finally { if (! (myLock && yourLock)) { if (myLock) { lock.unlock(); } if (yourLock) { bower.lock.unlock(); } } } return myLock && yourLock; } public void bow(Friend bower) { if (impendingBow(bower)) { try { System.out.format(\"%s: %s has\" + \" bowed to me!%n\", this.name, bower.getName()); bower.bowBack(this); } finally { lock.unlock(); bower.lock.unlock(); } } else { System.out.format(\"%s: %s started\" + \" to bow to me, but saw that\" + \" I was already bowing to\" + \" him.%n\", this.name, bower.getName()); } } public void bowBack(Friend bower) { System.out.format(\"%s: %s has\" + \" bowed back to me!%n\", this.name, bower.getName()); } } static class BowLoop implements Runnable { private Friend bower; private Friend bowee; public BowLoop(Friend bower, Friend bowee) { this.bower = bower; this.bowee = bowee; } public void run() { Random random = new Random(); for (;;) { try { Thread.sleep(random.nextInt(10)); } catch (InterruptedException e) {} bowee.bow(bower); } } } public static void main(String[] args) { final Friend alphonse = new Friend(\"Alphonse\"); final Friend gaston = new Friend(\"Gaston\"); new Thread(new BowLoop(alphonse, gaston)).start(); new Thread(new BowLoop(gaston, alphonse)).start(); } }","title":"Lock Objects"},{"location":"java/concurrency.html#executors","text":"In large-scale applications, it makes sense to separate thread management and creation from the rest of the application.","title":"Executors"},{"location":"java/concurrency.html#executor-interfaces","text":"Executor a simple interface that supports launching new tasks. The Executor interface provides a single method, execute, designed to be a drop-in replacement for a common thread-creation idiom. replace (new Thread(r)).start(); with e.execute(r); ( r -> Runnable, e -> Executor object) ExecutorService a subinterface of Executor, which adds features that help manage the life cycle, both of the individual tasks and of the executor itself. more versatile submit method. accepts Callable, Runnable objects. which allow the task to return a value. The submit method returns a Future object, which is used to retrieve the Callable return value and to manage the status of both Callable and Runnable tasks. provides methods for submitting large collections of Callable objects. provides a number of methods for managing the shutdown of the executor. ScheduledExecutorService a subinterface of ExecutorService, supports future and/or periodic execution of tasks. schedule, which executes a Runnable or Callable task after a specified delay. scheduleAtFixedRate and scheduleWithFixedDelay, which executes specified tasks repeatedly, at defined intervals.","title":"Executor Interfaces"},{"location":"java/concurrency.html#thread-pools","text":"consist of worker threads. Using worker threads minimizes the overhead due to thread creation. fixed thread pool. This type of pool always has a specified number of threads running; applications using it degrade gracefully newFixedThreadPool factory method in java.util.concurrent.Executors: newCachedThreadPool-> executor with an expandable thread pool -> suitable for applications that launch many short-lived tasks. newSingleThreadExecutor -> creates an executor that executes a single task at a time. Several factory methods are ScheduledExecutorService versions of the above executors. java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ScheduledThreadPoolExecutor","title":"Thread Pools"},{"location":"java/concurrency.html#forkjoin","text":"","title":"Fork/Join"},{"location":"java/concurrency.html#concurrent-collections","text":"BlockingQueue defines a first-in-first-out data structure that blocks or times out when you attempt to add to a full queue, or retrieve from an empty queue. ConcurrentMap is a subinterface of java.util.Map that defines useful atomic operations. These operations remove or replace a key-value pair only if the key is present, or add a key-value pair only if the key is absent. Making these operations atomic helps avoid synchronization. The standard general-purpose implementation of ConcurrentMap is ConcurrentHashMap, which is a concurrent analog of HashMap. ConcurrentNavigableMap is a subinterface of ConcurrentMap that supports approximate matches. The standard general-purpose implementation of ConcurrentNavigableMap is ConcurrentSkipListMap, which is a concurrent analog of TreeMap.","title":"Concurrent Collections"},{"location":"java/concurrency.html#atomic-variables","text":"The java.util.concurrent.atomic package defines classes that support atomic operations on single variables. All classes have get and set methods that work like reads and writes on volatile variables. That is, a set has a happens-before relationship with any subsequent get on the same variable. The atomic compareAndSet method also has these memory consistency features, as do the simple atomic arithmetic methods that apply to integer atomic variables. import java.util.concurrent.atomic.AtomicInteger; class AtomicCounter { private AtomicInteger c = new AtomicInteger(0); public void increment() { c.incrementAndGet(); } public void decrement() { c.decrementAndGet(); } public int value() { return c.get(); } }","title":"Atomic Variables"},{"location":"java/concurrency.html#concurrent-random-numbers","text":"ThreadLocalRandom -> random numbers from multiple threads or ForkJoinTasks. For concurrent access, using ThreadLocalRandom instead of Math.random() results in less contention and, ultimately, better performance. All you need to do is call ThreadLocalRandom.current(), then call one of its methods to retrieve a random number. Here is one example: int r = ThreadLocalRandom.current().nextInt(4, 77);","title":"Concurrent Random Numbers"},{"location":"java/concurrency.html#further-explorations","text":"Java Concurrent Animated : Animations that show usage of concurrency features. TODO LINK","title":"Further Explorations"},{"location":"java/debugging.html","text":"Debugging \u00b6 TODO LINK","title":"Debugging"},{"location":"java/debugging.html#debugging","text":"TODO LINK","title":"Debugging"},{"location":"java/deployment.html","text":"Deployment \u00b6 Java Applet \u00b6 Special kind of Java program that is designed to be transmitted over the Internet and automatically executed by a Java-compatible web browser. \u2022 Reacts to user input and changes dynamically. Java rich internet applications (RIA) are applications that have traits similar to desktop applications, but are deployed via the Internet. Java RIAs may be developed and deployed as Java applets or Java Web Start applications. Applets - Java applets run in the context of a browser. The Java Plug-in software controls the execution and lifecycle of Java applets. Java Web Start applications - Java Web Start applications are launched via a browser the first time. They may subsequently be launched from a desktop shortcut. Once a Java Web Start application is downloaded and its security certificate has been accepted by the user, it behaves almost like a standalone application. Component-Based Architecture for RIAs In the past, the decision of whether to deploy a Java rich internet application inside the browser as an applet, or outside the browser as a Java Web Start application, could significantly impact the design of the application. With the latest Java Plug-in, this decision has been greatly simplified. Traditionally, applications construct their user interfaces, including the top-level Frame, in the main method. This programming style prevents easy re-deployment of the application in the browser, because it assumes that the application creates its own Frame. When running in the browser as an applet, the applet is the top level container that should hold the user interface for the application. A top-level Frame is not needed. Use component-based architecture when designing your Java rich internet application. Try to organize its functionality into one or more components that can be composed together. In this context, the term \"component\" refers to a GUI element that is a subclass of the AWT Component class, the Swing JComponent class, or another subclass. For example, you could have a top level JPanel which contains other UI components in it (like a combination of more nested JPanels and text fields, combo boxes etc.). With such a design, it becomes relatively easy to deploy the core functionality as an applet or a Java Web Start application. To deploy as a Java applet, you just need to wrap the core functionality in an Applet or JApplet and add the browser specific functionality, if necessary. To deploy as a Java Web Start application, wrap the functionality in a JFrame. Choosing Between Java Applets and Java Web Start Applications The Rich Internet Applications Decision Guide contains detailed information to help you decide whether to deploy your code as a Java applet or Java Web Start application. The Self-Contained Application Alternative Self-contained applications provide a deployment option that does not require a browser. Users install your application locally and run it similar to native applications. Self-contained applications include the JRE needed to run the application, so users always have the correct JRE. This trail discusses the development and deployment of RIAs and self-contained applications. See What's New for capabilities introduced in various versions of the client Java Runtime Environment (JRE) software. TODO LINK","title":"Deployment"},{"location":"java/deployment.html#deployment","text":"","title":"Deployment"},{"location":"java/deployment.html#java-applet","text":"Special kind of Java program that is designed to be transmitted over the Internet and automatically executed by a Java-compatible web browser. \u2022 Reacts to user input and changes dynamically. Java rich internet applications (RIA) are applications that have traits similar to desktop applications, but are deployed via the Internet. Java RIAs may be developed and deployed as Java applets or Java Web Start applications. Applets - Java applets run in the context of a browser. The Java Plug-in software controls the execution and lifecycle of Java applets. Java Web Start applications - Java Web Start applications are launched via a browser the first time. They may subsequently be launched from a desktop shortcut. Once a Java Web Start application is downloaded and its security certificate has been accepted by the user, it behaves almost like a standalone application. Component-Based Architecture for RIAs In the past, the decision of whether to deploy a Java rich internet application inside the browser as an applet, or outside the browser as a Java Web Start application, could significantly impact the design of the application. With the latest Java Plug-in, this decision has been greatly simplified. Traditionally, applications construct their user interfaces, including the top-level Frame, in the main method. This programming style prevents easy re-deployment of the application in the browser, because it assumes that the application creates its own Frame. When running in the browser as an applet, the applet is the top level container that should hold the user interface for the application. A top-level Frame is not needed. Use component-based architecture when designing your Java rich internet application. Try to organize its functionality into one or more components that can be composed together. In this context, the term \"component\" refers to a GUI element that is a subclass of the AWT Component class, the Swing JComponent class, or another subclass. For example, you could have a top level JPanel which contains other UI components in it (like a combination of more nested JPanels and text fields, combo boxes etc.). With such a design, it becomes relatively easy to deploy the core functionality as an applet or a Java Web Start application. To deploy as a Java applet, you just need to wrap the core functionality in an Applet or JApplet and add the browser specific functionality, if necessary. To deploy as a Java Web Start application, wrap the functionality in a JFrame. Choosing Between Java Applets and Java Web Start Applications The Rich Internet Applications Decision Guide contains detailed information to help you decide whether to deploy your code as a Java applet or Java Web Start application. The Self-Contained Application Alternative Self-contained applications provide a deployment option that does not require a browser. Users install your application locally and run it similar to native applications. Self-contained applications include the JRE needed to run the application, so users always have the correct JRE. This trail discusses the development and deployment of RIAs and self-contained applications. See What's New for capabilities introduced in various versions of the client Java Runtime Environment (JRE) software. TODO LINK","title":"Java Applet"},{"location":"java/ejb.html","text":"","title":"Ejb"},{"location":"java/exceptions.html","text":"Exceptions \u00b6 exceptional event disrupts the normal flow of the program's instructions. exception object throwing an exception call stack exception handler catch the exception Catch or Specify Requirement \u00b6 A try statement that catches the exception. A method that specifies that it can throw the exception. Checked Exception These are exceptional conditions that a well-written application should anticipate and recover from. eg. java.io.FileNotFoundException. Error These are exceptional conditions that are external to the application. the application usually cannot anticipate or recover.from. Errors are not subject to the Catch or Specify Requirement. eg. java.io.IOError Runtime Exception These are exceptional conditions that are internal to the application, and that the application usually cannot anticipate or recover from. programming bugs, such as logic errors or improper use of an API. are not subject to the Catch or Specify Requirement. those indicated by RuntimeException and its subclasses. eg. NullPointerException Errors and runtime exceptions are collectively known as unchecked exceptions. Bypassing Catch or Specify \u00b6 using unchecked exceptions in place of checked exceptions. Syntax \u00b6 // Note: This class will not compile yet. import java.io.*; import java.util.List; import java.util.ArrayList; public class ListOfNumbers { private List<Integer> list; private static final int SIZE = 10; public ListOfNumbers () { list = new ArrayList<Integer>(SIZE); for (int i = 0; i < SIZE; i++) { list.add(new Integer(i)); } } public void writeList() { // The FileWriter constructor throws IOException, which must be caught. PrintWriter out = new PrintWriter(new FileWriter(\"OutFile.txt\")); for (int i = 0; i < SIZE; i++) { // The get(int) method throws IndexOutOfBoundsException, which must be caught. out.println(\"Value at: \" + i + \" = \" + list.get(i)); } out.close(); } } with exception handling public void writeList() { PrintWriter out = null; try { System.out.println(\"Entering\" + \" try statement\"); out = new PrintWriter(new FileWriter(\"OutFile.txt\")); for (int i = 0; i < SIZE; i++) { out.println(\"Value at: \" + i + \" = \" + list.get(i)); } } catch (IndexOutOfBoundsException e) { System.err.println(\"Caught IndexOutOfBoundsException: \" + e.getMessage()); } catch (IOException e) { System.err.println(\"Caught IOException: \" + e.getMessage()); } finally { if (out != null) { System.out.println(\"Closing PrintWriter\"); out.close(); } else { System.out.println(\"PrintWriter not open\"); } } } Suppressed Exceptions \u00b6 Classes That Implement the AutoCloseable or Closeable Interface \u00b6 Try with Resources \u00b6 public static void viewTable(Connection con) throws SQLException { String query = \"select COF_NAME, SUP_ID, PRICE, SALES, TOTAL from COFFEES\"; try (Statement stmt = con.createStatement()) { ResultSet rs = stmt.executeQuery(query); while (rs.next()) { String coffeeName = rs.getString(\"COF_NAME\"); int supplierID = rs.getInt(\"SUP_ID\"); float price = rs.getFloat(\"PRICE\"); int sales = rs.getInt(\"SALES\"); int total = rs.getInt(\"TOTAL\"); System.out.println(coffeeName + \", \" + supplierID + \", \" + price + \", \" + sales + \", \" + total); } } catch (SQLException e) { JDBCTutorialUtilities.printSQLException(e); } } Specification \u00b6 public void writeList() throws IOException, IndexOutOfBoundsException {} Classes \u00b6 Throwable Error Exception Chained Exceptions \u00b6 Throwable getCause() Throwable initCause(Throwable) Throwable(String, Throwable) Throwable(Throwable) Stack Trace Information \u00b6 catch (Exception cause) { StackTraceElement elements[] = cause.getStackTrace(); for (int i = 0, n = elements.length; i < n; i++) { System.err.println(elements[i].getFileName() + \":\" + elements[i].getLineNumber() + \">> \" + elements[i].getMethodName() + \"()\"); } } Logging \u00b6 try { Handler handler = new FileHandler(\"OutFile.log\"); Logger.getLogger(\"\").addHandler(handler); } catch (IOException e) { Logger logger = Logger.getLogger(\"package.name\"); StackTraceElement elements[] = e.getStackTrace(); for (int i = 0, n = elements.length; i < n; i++) { logger.log(Level.WARNING, elements[i].getMethodName()); } } Unchecked Exceptions \u2014 The Controversy \u00b6 Why did the designers decide to force a method to specify all uncaught checked exceptions that can be thrown within its scope? Advantages of Exceptions \u00b6 Separating Error-Handling Code from \"Regular\" Code Propagating Errors Up the Call Stack Grouping and Differentiating Error Types TODO Link","title":"Exceptions"},{"location":"java/exceptions.html#exceptions","text":"exceptional event disrupts the normal flow of the program's instructions. exception object throwing an exception call stack exception handler catch the exception","title":"Exceptions"},{"location":"java/exceptions.html#catch-or-specify-requirement","text":"A try statement that catches the exception. A method that specifies that it can throw the exception. Checked Exception These are exceptional conditions that a well-written application should anticipate and recover from. eg. java.io.FileNotFoundException. Error These are exceptional conditions that are external to the application. the application usually cannot anticipate or recover.from. Errors are not subject to the Catch or Specify Requirement. eg. java.io.IOError Runtime Exception These are exceptional conditions that are internal to the application, and that the application usually cannot anticipate or recover from. programming bugs, such as logic errors or improper use of an API. are not subject to the Catch or Specify Requirement. those indicated by RuntimeException and its subclasses. eg. NullPointerException Errors and runtime exceptions are collectively known as unchecked exceptions.","title":"Catch or Specify Requirement"},{"location":"java/exceptions.html#bypassing-catch-or-specify","text":"using unchecked exceptions in place of checked exceptions.","title":"Bypassing Catch or Specify"},{"location":"java/exceptions.html#syntax","text":"// Note: This class will not compile yet. import java.io.*; import java.util.List; import java.util.ArrayList; public class ListOfNumbers { private List<Integer> list; private static final int SIZE = 10; public ListOfNumbers () { list = new ArrayList<Integer>(SIZE); for (int i = 0; i < SIZE; i++) { list.add(new Integer(i)); } } public void writeList() { // The FileWriter constructor throws IOException, which must be caught. PrintWriter out = new PrintWriter(new FileWriter(\"OutFile.txt\")); for (int i = 0; i < SIZE; i++) { // The get(int) method throws IndexOutOfBoundsException, which must be caught. out.println(\"Value at: \" + i + \" = \" + list.get(i)); } out.close(); } } with exception handling public void writeList() { PrintWriter out = null; try { System.out.println(\"Entering\" + \" try statement\"); out = new PrintWriter(new FileWriter(\"OutFile.txt\")); for (int i = 0; i < SIZE; i++) { out.println(\"Value at: \" + i + \" = \" + list.get(i)); } } catch (IndexOutOfBoundsException e) { System.err.println(\"Caught IndexOutOfBoundsException: \" + e.getMessage()); } catch (IOException e) { System.err.println(\"Caught IOException: \" + e.getMessage()); } finally { if (out != null) { System.out.println(\"Closing PrintWriter\"); out.close(); } else { System.out.println(\"PrintWriter not open\"); } } }","title":"Syntax"},{"location":"java/exceptions.html#suppressed-exceptions","text":"","title":"Suppressed Exceptions"},{"location":"java/exceptions.html#classes-that-implement-the-autocloseable-or-closeable-interface","text":"","title":"Classes That Implement the AutoCloseable or Closeable Interface"},{"location":"java/exceptions.html#try-with-resources","text":"public static void viewTable(Connection con) throws SQLException { String query = \"select COF_NAME, SUP_ID, PRICE, SALES, TOTAL from COFFEES\"; try (Statement stmt = con.createStatement()) { ResultSet rs = stmt.executeQuery(query); while (rs.next()) { String coffeeName = rs.getString(\"COF_NAME\"); int supplierID = rs.getInt(\"SUP_ID\"); float price = rs.getFloat(\"PRICE\"); int sales = rs.getInt(\"SALES\"); int total = rs.getInt(\"TOTAL\"); System.out.println(coffeeName + \", \" + supplierID + \", \" + price + \", \" + sales + \", \" + total); } } catch (SQLException e) { JDBCTutorialUtilities.printSQLException(e); } }","title":"Try with Resources"},{"location":"java/exceptions.html#specification","text":"public void writeList() throws IOException, IndexOutOfBoundsException {}","title":"Specification"},{"location":"java/exceptions.html#classes","text":"Throwable Error Exception","title":"Classes"},{"location":"java/exceptions.html#chained-exceptions","text":"Throwable getCause() Throwable initCause(Throwable) Throwable(String, Throwable) Throwable(Throwable)","title":"Chained Exceptions"},{"location":"java/exceptions.html#stack-trace-information","text":"catch (Exception cause) { StackTraceElement elements[] = cause.getStackTrace(); for (int i = 0, n = elements.length; i < n; i++) { System.err.println(elements[i].getFileName() + \":\" + elements[i].getLineNumber() + \">> \" + elements[i].getMethodName() + \"()\"); } }","title":"Stack Trace Information"},{"location":"java/exceptions.html#logging","text":"try { Handler handler = new FileHandler(\"OutFile.log\"); Logger.getLogger(\"\").addHandler(handler); } catch (IOException e) { Logger logger = Logger.getLogger(\"package.name\"); StackTraceElement elements[] = e.getStackTrace(); for (int i = 0, n = elements.length; i < n; i++) { logger.log(Level.WARNING, elements[i].getMethodName()); } }","title":"Logging"},{"location":"java/exceptions.html#unchecked-exceptions-the-controversy","text":"Why did the designers decide to force a method to specify all uncaught checked exceptions that can be thrown within its scope?","title":"Unchecked Exceptions \u2014 The Controversy"},{"location":"java/exceptions.html#advantages-of-exceptions","text":"Separating Error-Handling Code from \"Regular\" Code Propagating Errors Up the Call Stack Grouping and Differentiating Error Types TODO Link","title":"Advantages of Exceptions"},{"location":"java/garbage-collection.html","text":"Garbage Collection \u00b6 Automatic GC \u00b6 Marking Normal Deletion Deletion with compacting Generational GC \u00b6 Object Allocation Filling with Eden Space Copying referenced objects Object Aging Promotion Refferences \u00b6 Oracle - Java Garbage Collection Basics","title":"Garbage Collection"},{"location":"java/garbage-collection.html#garbage-collection","text":"","title":"Garbage Collection"},{"location":"java/garbage-collection.html#automatic-gc","text":"Marking Normal Deletion Deletion with compacting","title":"Automatic GC"},{"location":"java/garbage-collection.html#generational-gc","text":"Object Allocation Filling with Eden Space Copying referenced objects Object Aging Promotion","title":"Generational GC"},{"location":"java/garbage-collection.html#refferences","text":"Oracle - Java Garbage Collection Basics","title":"Refferences"},{"location":"java/generics.html","text":"Generics \u00b6 bugs are difficult to find. with generics you can detect some runtime bugs at compile time. generics enable types to be parameters when defining classes, interfaces and methods. Why? \u00b6 Stronger type checks at compile time. Elimination of casts Enabling programmers to implement generic algorithms. TODO LINK","title":"Generics"},{"location":"java/generics.html#generics","text":"bugs are difficult to find. with generics you can detect some runtime bugs at compile time. generics enable types to be parameters when defining classes, interfaces and methods.","title":"Generics"},{"location":"java/generics.html#why","text":"Stronger type checks at compile time. Elimination of casts Enabling programmers to implement generic algorithms. TODO LINK","title":"Why?"},{"location":"java/hello.html","text":"Hello \u00b6 class HelloWorldApp { public static void main(String[] args) { System.out.println(\"Hello World!\"); // Display the string. } } multiline - / / javadoc - /* / single line - //","title":"Hello"},{"location":"java/hello.html#hello","text":"class HelloWorldApp { public static void main(String[] args) { System.out.println(\"Hello World!\"); // Display the string. } } multiline - / / javadoc - /* / single line - //","title":"Hello"},{"location":"java/i18n.html","text":"Internalization \u00b6 TODO LINK","title":"Internalization"},{"location":"java/i18n.html#internalization","text":"TODO LINK","title":"Internalization"},{"location":"java/inheritance.html","text":"Inheretance \u00b6 subclass (also a derived class, extended class, or child class). superclass (also a base class or a parent class). the topmost class, Object. Such a class is said to be descended from all the classes in the inheritance chain stretching back to Object. Constructors are not members, so they are not inherited by subclasses. A subclass inherits all of the public and protected members of its parent, no matter what package the subclass is in. If the subclass is in the same package as its parent, it also inherits the package-private members of the parent. You can use the inherited members as is, replace them, hide them, or supplement them with new members: You can declare a field in the subclass with the same name as the one in the superclass, thus hiding it (not recommended). You can write a new instance method in the subclass that has the same signature as the one in the superclass, thus overriding it. You can write a new static method in the subclass that has the same signature as the one in the superclass, thus hiding it. You can write a subclass constructor that invokes the constructor of the superclass, either implicitly or by using the keyword super. A nested class has access to all the private members of its enclosing class\u2014both fields and methods. Therefore, a public or protected nested class inherited by a subclass has indirect access to all of the private members of the superclass. Casting Objects \u00b6 // implicit casting Object obj = new MountainBike(); // explicit casting MountainBike myBike = (MountainBike)obj; instanceof Multiple Inheritance \u00b6 of State, Implementation, and Type multiple inheritance of state problem: name conflicts and ambiguity. Because interfaces do not contain fields, you do not have to worry about problems that result from multiple inheritance of state. As with multiple inheritance of implementation, a class can inherit different implementations of a method defined (as default or static) in the interfaces that it extends. In this case, the compiler or the user must decide which one to use. Polymorphism \u00b6 principle in biology in which an organism or species can have many different forms or stages. Subclasses of a class can define their own unique behaviors and yet share some of the same functionality of the parent class. virtual method invocation \u00b6 It does not call the method that is defined by the variable's type super keyword \u00b6 Note: If a constructor does not explicitly invoke a superclass constructor, the Java compiler automatically inserts a call to the no-argument constructor of the superclass. If the super class does not have a no-argument constructor, you will get a compile-time error. Object does have such a constructor, so if Object is the only superclass, there is no problem. super(parameter list) -> constructor of super class. constructor chaining \u00b6 a whole chain of constructors called, all the way back to the constructor of Object. Object as superclass \u00b6 protected Object clone() throws CloneNotSupportedException Creates and returns a copy of this object. public boolean equals(Object obj) Indicates whether some other object is \"equal to\" this one. check only refernece / symbol not values protected void finalize() throws Throwable Called by the garbage collector on an object when garbage collection determines that there are no more references to the object public final Class getClass() Returns the runtime class of an object. public int hashCode() Returns a hash code value for the object. public String toString() Returns a string representation of the object. public final void notify() public final void notifyAll() public final void wait() public final void wait(long timeout) public final void wait(long timeout, int nanos) If you override equals(), you must override hashCode() as well. The Class class, in the java.lang package, has a large number of methods (more than 50). For example, you can test to see if the class is an annotation (isAnnotation()), an interface (isInterface()), or an enumeration (isEnum()). You can see what the object's fields are (getFields()) or what its methods are (getMethods()), and so on. You can declare some or all of a class's methods final Methods called from constructors should generally be declared finaal. A class that is declared final cannot be subclassed. Abstract Methods and Classes \u00b6 Abstract classes cannot be instantiated, but they can be subclassed. Abstract classes vs interface \u00b6 Abstract classes are similar to interfaces. You cannot instantiate them, and they may contain a mix of methods declared with or without an implementation. However, with abstract classes, you can declare fields that are not static and final, and define public, protected, and private concrete methods. With interfaces, all fields are automatically public, static, and final, and all methods that you declare or define (as default methods) are public. In addition, you can extend only one class, whether or not it is abstract, whereas you can implement any number of interfaces. Which should you use, abstract classes or interfaces? Consider using abstract classes if any of these statements apply to your situation: You want to share code among several closely related classes. You expect that classes that extend your abstract class have many common methods or fields, or require access modifiers other than public (such as protected and private). You want to declare non-static or non-final fields. This enables you to define methods that can access and modify the state of the object to which they belong. Consider using interfaces if any of these statements apply to your situation: You expect that unrelated classes would implement your interface. For example, the interfaces Comparable and Cloneable are implemented by many unrelated classes. You want to specify the behavior of a particular data type, but not concerned about who implements its behavior. You want to take advantage of multiple inheritance of type. overriding and hiding methods \u00b6 same signature (name, plus the number and the type of its parameters) and return type @Override If a subclass defines a static method with the same signature as a static method in the superclass, then the method in the subclass hides the one in the superclass. The version of the overridden instance method that gets invoked is the one in the subclass. The version of the hidden static method that gets invoked depends on whether it is invoked from the superclass or the subclass. public class Animal { public static void testClassMethod() { System.out.println(\"The static method in Animal\"); } public void testInstanceMethod() { System.out.println(\"The instance method in Animal\"); } } public class Cat extends Animal { public static void testClassMethod() { System.out.println(\"The static method in Cat\"); } public void testInstanceMethod() { System.out.println(\"The instance method in Cat\"); } public static void main(String[] args) { Cat myCat = new Cat(); Animal myAnimal = myCat; Animal.testClassMethod(); myAnimal.testInstanceMethod(); } } The static method in Animal The instance method in Cat","title":"Inheretance"},{"location":"java/inheritance.html#inheretance","text":"subclass (also a derived class, extended class, or child class). superclass (also a base class or a parent class). the topmost class, Object. Such a class is said to be descended from all the classes in the inheritance chain stretching back to Object. Constructors are not members, so they are not inherited by subclasses. A subclass inherits all of the public and protected members of its parent, no matter what package the subclass is in. If the subclass is in the same package as its parent, it also inherits the package-private members of the parent. You can use the inherited members as is, replace them, hide them, or supplement them with new members: You can declare a field in the subclass with the same name as the one in the superclass, thus hiding it (not recommended). You can write a new instance method in the subclass that has the same signature as the one in the superclass, thus overriding it. You can write a new static method in the subclass that has the same signature as the one in the superclass, thus hiding it. You can write a subclass constructor that invokes the constructor of the superclass, either implicitly or by using the keyword super. A nested class has access to all the private members of its enclosing class\u2014both fields and methods. Therefore, a public or protected nested class inherited by a subclass has indirect access to all of the private members of the superclass.","title":"Inheretance"},{"location":"java/inheritance.html#casting-objects","text":"// implicit casting Object obj = new MountainBike(); // explicit casting MountainBike myBike = (MountainBike)obj; instanceof","title":"Casting Objects"},{"location":"java/inheritance.html#multiple-inheritance","text":"of State, Implementation, and Type multiple inheritance of state problem: name conflicts and ambiguity. Because interfaces do not contain fields, you do not have to worry about problems that result from multiple inheritance of state. As with multiple inheritance of implementation, a class can inherit different implementations of a method defined (as default or static) in the interfaces that it extends. In this case, the compiler or the user must decide which one to use.","title":"Multiple Inheritance"},{"location":"java/inheritance.html#polymorphism","text":"principle in biology in which an organism or species can have many different forms or stages. Subclasses of a class can define their own unique behaviors and yet share some of the same functionality of the parent class.","title":"Polymorphism"},{"location":"java/inheritance.html#virtual-method-invocation","text":"It does not call the method that is defined by the variable's type","title":"virtual method invocation"},{"location":"java/inheritance.html#super-keyword","text":"Note: If a constructor does not explicitly invoke a superclass constructor, the Java compiler automatically inserts a call to the no-argument constructor of the superclass. If the super class does not have a no-argument constructor, you will get a compile-time error. Object does have such a constructor, so if Object is the only superclass, there is no problem. super(parameter list) -> constructor of super class.","title":"super keyword"},{"location":"java/inheritance.html#constructor-chaining","text":"a whole chain of constructors called, all the way back to the constructor of Object.","title":"constructor chaining"},{"location":"java/inheritance.html#object-as-superclass","text":"protected Object clone() throws CloneNotSupportedException Creates and returns a copy of this object. public boolean equals(Object obj) Indicates whether some other object is \"equal to\" this one. check only refernece / symbol not values protected void finalize() throws Throwable Called by the garbage collector on an object when garbage collection determines that there are no more references to the object public final Class getClass() Returns the runtime class of an object. public int hashCode() Returns a hash code value for the object. public String toString() Returns a string representation of the object. public final void notify() public final void notifyAll() public final void wait() public final void wait(long timeout) public final void wait(long timeout, int nanos) If you override equals(), you must override hashCode() as well. The Class class, in the java.lang package, has a large number of methods (more than 50). For example, you can test to see if the class is an annotation (isAnnotation()), an interface (isInterface()), or an enumeration (isEnum()). You can see what the object's fields are (getFields()) or what its methods are (getMethods()), and so on. You can declare some or all of a class's methods final Methods called from constructors should generally be declared finaal. A class that is declared final cannot be subclassed.","title":"Object as superclass"},{"location":"java/inheritance.html#abstract-methods-and-classes","text":"Abstract classes cannot be instantiated, but they can be subclassed.","title":"Abstract Methods and Classes"},{"location":"java/inheritance.html#abstract-classes-vs-interface","text":"Abstract classes are similar to interfaces. You cannot instantiate them, and they may contain a mix of methods declared with or without an implementation. However, with abstract classes, you can declare fields that are not static and final, and define public, protected, and private concrete methods. With interfaces, all fields are automatically public, static, and final, and all methods that you declare or define (as default methods) are public. In addition, you can extend only one class, whether or not it is abstract, whereas you can implement any number of interfaces. Which should you use, abstract classes or interfaces? Consider using abstract classes if any of these statements apply to your situation: You want to share code among several closely related classes. You expect that classes that extend your abstract class have many common methods or fields, or require access modifiers other than public (such as protected and private). You want to declare non-static or non-final fields. This enables you to define methods that can access and modify the state of the object to which they belong. Consider using interfaces if any of these statements apply to your situation: You expect that unrelated classes would implement your interface. For example, the interfaces Comparable and Cloneable are implemented by many unrelated classes. You want to specify the behavior of a particular data type, but not concerned about who implements its behavior. You want to take advantage of multiple inheritance of type.","title":"Abstract classes vs interface"},{"location":"java/inheritance.html#overriding-and-hiding-methods","text":"same signature (name, plus the number and the type of its parameters) and return type @Override If a subclass defines a static method with the same signature as a static method in the superclass, then the method in the subclass hides the one in the superclass. The version of the overridden instance method that gets invoked is the one in the subclass. The version of the hidden static method that gets invoked depends on whether it is invoked from the superclass or the subclass. public class Animal { public static void testClassMethod() { System.out.println(\"The static method in Animal\"); } public void testInstanceMethod() { System.out.println(\"The instance method in Animal\"); } } public class Cat extends Animal { public static void testClassMethod() { System.out.println(\"The static method in Cat\"); } public void testInstanceMethod() { System.out.println(\"The instance method in Cat\"); } public static void main(String[] args) { Cat myCat = new Cat(); Animal myAnimal = myCat; Animal.testClassMethod(); myAnimal.testInstanceMethod(); } } The static method in Animal The instance method in Cat","title":"overriding and hiding methods"},{"location":"java/interfaces.html","text":"Interfaces \u00b6 A contract that other people adhere to and people know the API but not the implementation public interface OperateCar { // constant declarations, if any // method signatures // An enum with values RIGHT, LEFT int turn(Direction direction, double radius, double startSpeed, double endSpeed); int changeLanes(Direction direction, double startSpeed, double endSpeed); int signalTurn(Direction direction, boolean signalOn); int getRadarFront(double distanceToCar, double speedOfCar); int getRadarRear(double distanceToCar, double speedOfCar); ...... // more method signatures } public interface GroupedInterface extends Interface1, Interface2, Interface3 { // constant declarations // base of natural logarithms double E = 2.718282; // method signatures void doSomething (int i, double x); int doSomethingElse(String s); } abstract methods, default methods, and static methods -> default public , can omit. All constant values defined in an interface are implicitly public, static, and final. public interface Relatable { // this (object calling isLargerThan) // and other must be instances of // the same class returns 1, 0, -1 // if this is greater than, // equal to, or less than other public int isLargerThan(Relatable other); } public class RectanglePlus implements Relatable { public int isLargerThan(Relatable other) { RectanglePlus otherRect = (RectanglePlus)other; if (this.getArea() < otherRect.getArea()) return -1; else if (this.getArea() > otherRect.getArea()) return 1; else return 0; } } Evolving Interfaces \u00b6 making changes in historic interfaces make devs angry. either extend it, or define static and default methods. default boolean didItWork(int i, double x, String s) { // Method body } Extending Interfaces That Contain Default Methods \u00b6 Not mention the default method at all, which lets your extended interface inherit the default method. Redeclare the default method, which makes it abstract. Redefine the default method, which overrides it. Static Methods \u00b6 A static method is a method that is associated with the class in which it is defined rather than with any object. Every instance of the class shares its static methods myDeck.sort( Comparator.comparing(Card::getRank) .reversed() .thenComparing(Comparator.comparing(Card::getSuit)));","title":"Interfaces"},{"location":"java/interfaces.html#interfaces","text":"A contract that other people adhere to and people know the API but not the implementation public interface OperateCar { // constant declarations, if any // method signatures // An enum with values RIGHT, LEFT int turn(Direction direction, double radius, double startSpeed, double endSpeed); int changeLanes(Direction direction, double startSpeed, double endSpeed); int signalTurn(Direction direction, boolean signalOn); int getRadarFront(double distanceToCar, double speedOfCar); int getRadarRear(double distanceToCar, double speedOfCar); ...... // more method signatures } public interface GroupedInterface extends Interface1, Interface2, Interface3 { // constant declarations // base of natural logarithms double E = 2.718282; // method signatures void doSomething (int i, double x); int doSomethingElse(String s); } abstract methods, default methods, and static methods -> default public , can omit. All constant values defined in an interface are implicitly public, static, and final. public interface Relatable { // this (object calling isLargerThan) // and other must be instances of // the same class returns 1, 0, -1 // if this is greater than, // equal to, or less than other public int isLargerThan(Relatable other); } public class RectanglePlus implements Relatable { public int isLargerThan(Relatable other) { RectanglePlus otherRect = (RectanglePlus)other; if (this.getArea() < otherRect.getArea()) return -1; else if (this.getArea() > otherRect.getArea()) return 1; else return 0; } }","title":"Interfaces"},{"location":"java/interfaces.html#evolving-interfaces","text":"making changes in historic interfaces make devs angry. either extend it, or define static and default methods. default boolean didItWork(int i, double x, String s) { // Method body }","title":"Evolving Interfaces"},{"location":"java/interfaces.html#extending-interfaces-that-contain-default-methods","text":"Not mention the default method at all, which lets your extended interface inherit the default method. Redeclare the default method, which makes it abstract. Redefine the default method, which overrides it.","title":"Extending Interfaces That Contain Default Methods"},{"location":"java/interfaces.html#static-methods","text":"A static method is a method that is associated with the class in which it is defined rather than with any object. Every instance of the class shares its static methods myDeck.sort( Comparator.comparing(Card::getRank) .reversed() .thenComparing(Comparator.comparing(Card::getSuit)));","title":"Static Methods"},{"location":"java/intro.html","text":"What is Java? \u00b6 javase/tutorial Java is Object oriented by design. Procedural Object-Oriented Programs are divided into procedures Into objects Focus: procedure Data Data moves throughout the program. Object communicates none Data hiding, polymorphism, inheritance programming language \u00b6 general-purpose high-level .java -> compiler -> .class (byte code) -> JVM (interpreter) -> machine code perform additional steps at runtime to give your application a performance boost. finding performance bottlenecks and recompiling (to native code) frequently used sections of code. Simple Object oriented Distributed Multithreaded Dynamic Architecture neutral Portable High performance Robust Secure learn more at whitepaper Platform \u00b6 A platform is the hardware or software environment in which a program runs. Java platform a software-only platform that runs on top of other hardware-based platforms. JVM Java API Development Tools - javac, java, javadoc etc API - basic objects, networking, security, XML generation, database access etc Deployment Technologies - Java Web Start and Java Plug-In for deploying your applications to end users. User Interface Toolkits - JavaFX, Swing, and Java 2D (GUI) Integration Libraries - database access and manipulation of remote objects. (IDL, JDBC, JNDI, RMI, RMI-IIOP) docs platform-independent environment Why? \u00b6 Get started quickly : powerful object-oriented language, it's easy to learn. less code : 4 times smaller than the same program written in C++. (based on methods, classes count etc) better code : good coding practices, automatic garbage collection, object orientation, JavaBeans\u2122 component architecture, API Develop more quickly : 2x c++ portable Write once, run anywhere Distribute software easily : With Java Web Start software. JEE vs JSE \u00b6 Java\u2122 Standard Edition (Java\u2122 SE ) and the Java\u2122 Enterprise Edition (Java\u2122 EE).","title":"What is Java?"},{"location":"java/intro.html#what-is-java","text":"javase/tutorial Java is Object oriented by design. Procedural Object-Oriented Programs are divided into procedures Into objects Focus: procedure Data Data moves throughout the program. Object communicates none Data hiding, polymorphism, inheritance","title":"What is Java?"},{"location":"java/intro.html#programming-language","text":"general-purpose high-level .java -> compiler -> .class (byte code) -> JVM (interpreter) -> machine code perform additional steps at runtime to give your application a performance boost. finding performance bottlenecks and recompiling (to native code) frequently used sections of code. Simple Object oriented Distributed Multithreaded Dynamic Architecture neutral Portable High performance Robust Secure learn more at whitepaper","title":"programming language"},{"location":"java/intro.html#platform","text":"A platform is the hardware or software environment in which a program runs. Java platform a software-only platform that runs on top of other hardware-based platforms. JVM Java API Development Tools - javac, java, javadoc etc API - basic objects, networking, security, XML generation, database access etc Deployment Technologies - Java Web Start and Java Plug-In for deploying your applications to end users. User Interface Toolkits - JavaFX, Swing, and Java 2D (GUI) Integration Libraries - database access and manipulation of remote objects. (IDL, JDBC, JNDI, RMI, RMI-IIOP) docs platform-independent environment","title":"Platform"},{"location":"java/intro.html#why","text":"Get started quickly : powerful object-oriented language, it's easy to learn. less code : 4 times smaller than the same program written in C++. (based on methods, classes count etc) better code : good coding practices, automatic garbage collection, object orientation, JavaBeans\u2122 component architecture, API Develop more quickly : 2x c++ portable Write once, run anywhere Distribute software easily : With Java Web Start software.","title":"Why?"},{"location":"java/intro.html#jee-vs-jse","text":"Java\u2122 Standard Edition (Java\u2122 SE ) and the Java\u2122 Enterprise Edition (Java\u2122 EE).","title":"JEE vs JSE"},{"location":"java/io.html","text":"IO \u00b6 TODO LINK","title":"IO"},{"location":"java/io.html#io","text":"TODO LINK","title":"IO"},{"location":"java/java9-beyond.html","text":"Java 9 & Beyond \u00b6 Java 9 \u00b6 jshell \u00b6 Read-Evaluate-Print Loop (REPL) modules and linking \u00b6 level of organisation above packages reduce size of apps coupled with new java linker private packages fast failure incase method not present java --list-modules javadoc \u00b6 searchable HTML5 compliant compatible with the new module hierarchy Collection immutable factory methods \u00b6 Set<String> seasons = Set.of(\"winter\", \"spring\", \"summer\", \"fall\") Stream improvements \u00b6 functional programming Stream.iterate(\"hey\", x -> x.length() < 7, x -> x + \"y\").forEach(System.out::println) IntStream.range(0, 5).takeWhile(i -> i < 3).forEach(System.out::println) IntStream.range(0, 5).dropWhile(i -> i < 3).forEach(System.out::println) Optional.ofNullable(null).stream().forEach(System.out::println) Maybe we will get list comprehensions in the future? multi-release jars \u00b6 specify class implementations for each java version jar root / - Foo.class - Bar.class - META-INF - MANIFEST.MF - versions - 9 - Foo.class - 10 - Foo.class private interface methods \u00b6 interface MyInterface { private void printHelper (String verb, int n, int pow) { System.out.printf(\"%d %s is %d%n\", n, verb, (int) Math.pow(n, pow)); } default void printSquared (int n) { printHelper(\"squared\", n, 2); } default void printCubed (int n) { printHelper(\"cubed\", n, 3); } } public class MyImplementation implements MyInterface { } Java 10 \u00b6 GraalVM \u00b6 new VM based on HotSpot and OpenJDK. supports Ahead of Time (AOT) compilation polyglot programming Android Runtime has used AOT compilation since about 2013 GraalVM combines these two steps to produce machine-native images -- binary code which is created for the particular architecture on which the VM is running. Graal provides zero-overhead interoperability between Java, JavaScript, R, Python, Ruby, and C, thanks to the Truffle Language Implementation Framework. const express = require('express'); const app = express(); app.listen(3000); app.get('/', function(req, res) { var text = 'Hello World!'; const BigInteger = Java.type('java.math.BigInteger'); text += BigInteger.valueOf(2).pow(100).toString(16); text += Polyglot.eval('R', 'runif(100)')[0]; res.send(text); }) local variable type inference \u00b6 var type allows for local type inference Local type inference means that var can only be used inside of method bodies or other similar blocks of code. It can't be used to declare instance variables or as the return type of a method, etc. Java is still a statically-typed language This is different from, for instance, JavaScript, where the type of a variable is dynamic and can change from line to line. Unmodifiable Collection enhancements \u00b6 List<Integer> view = Collections.unmodifiableList(lint); Container awarenesss \u00b6 Docker and Java are finally friends. Java 11 \u00b6 Single source file launch \u00b6 java Example.java Java 12 \u00b6 switch expressions \u00b6 a step toward pattern matching String name = switch(x) { case 1 -> \"one\"; case 2 -> \"two\"; case 3 -> \"three\"; default -> throw new IllegalArgumentException(\"I can only count to 3.\"); }; System.out.println(name); teeing Collectors \u00b6 import static java.util.stream.Collectors.* var ints = DoubleStream.of(1, 2, 3, 4, 5) ints.boxed().collect(teeing( summingDouble(e -> e), counting(), (a,b) -> a/b )) Java 14 \u00b6 Multiline text blocks \u00b6 String html3 = \"\"\" <html> <body> <h1>\"I love Java and Java loves me!\"</h1> </body> </html> \"\"\" Java 14+ \u00b6 Java-on-Java compiler with Project Metropolis \u00b6 decouple Java from dependencies on other languages allow the VM to optimise itself maintainability / simplification Project Amber \u00b6 flow typing void alertNChars (Object o) { if (o instanceof String s) System.out.println(\"String contains \" + s.length() + \" characters\"); else System.out.println(\"not a String\"); } anonymous variables BiFunction<Integer, Double, String> bids = (i, _) -> String.valueOf(i); data classes public instance variables hashCode() equals() toString() record Boilerplate (int myInt, double myDouble, String myString) { } sealed types sealed interface Car (Make make, Model model) { } record FossilFuelCar (Make make, Model model) implements Car { } record ElectricCar (Make make, Model model) implements Car { } record HybridCar (Make make, Model model) implements Car { } record FuelCellCar (Make make, Model model) implements Car { } Project Loom \u00b6 lightweight multithreading application-level Thread-like abstraction called a Fiber. VM-level multithreading (rather than OS-level) coroutines tail-call optimisation lightweight user-mode Fiber Project Valhalla \u00b6 value types generic specialisation reified generics Refferences \u00b6 20 Reasons to Move On from Java 8 Keeping up with Java 9, 10, 11, and Beyond From Java 8 to Java 11 \u2013 Quick Guide Java 8 v/s Java 11 JDK 17: The new features in Java 17 Java Versions and Features","title":"Java 9 & Beyond"},{"location":"java/java9-beyond.html#java-9-beyond","text":"","title":"Java 9 &amp; Beyond"},{"location":"java/java9-beyond.html#java-9","text":"","title":"Java 9"},{"location":"java/java9-beyond.html#jshell","text":"Read-Evaluate-Print Loop (REPL)","title":"jshell"},{"location":"java/java9-beyond.html#modules-and-linking","text":"level of organisation above packages reduce size of apps coupled with new java linker private packages fast failure incase method not present java --list-modules","title":"modules and linking"},{"location":"java/java9-beyond.html#javadoc","text":"searchable HTML5 compliant compatible with the new module hierarchy","title":"javadoc"},{"location":"java/java9-beyond.html#collection-immutable-factory-methods","text":"Set<String> seasons = Set.of(\"winter\", \"spring\", \"summer\", \"fall\")","title":"Collection immutable factory methods"},{"location":"java/java9-beyond.html#stream-improvements","text":"functional programming Stream.iterate(\"hey\", x -> x.length() < 7, x -> x + \"y\").forEach(System.out::println) IntStream.range(0, 5).takeWhile(i -> i < 3).forEach(System.out::println) IntStream.range(0, 5).dropWhile(i -> i < 3).forEach(System.out::println) Optional.ofNullable(null).stream().forEach(System.out::println) Maybe we will get list comprehensions in the future?","title":"Stream improvements"},{"location":"java/java9-beyond.html#multi-release-jars","text":"specify class implementations for each java version jar root / - Foo.class - Bar.class - META-INF - MANIFEST.MF - versions - 9 - Foo.class - 10 - Foo.class","title":"multi-release jars"},{"location":"java/java9-beyond.html#private-interface-methods","text":"interface MyInterface { private void printHelper (String verb, int n, int pow) { System.out.printf(\"%d %s is %d%n\", n, verb, (int) Math.pow(n, pow)); } default void printSquared (int n) { printHelper(\"squared\", n, 2); } default void printCubed (int n) { printHelper(\"cubed\", n, 3); } } public class MyImplementation implements MyInterface { }","title":"private interface methods"},{"location":"java/java9-beyond.html#java-10","text":"","title":"Java 10"},{"location":"java/java9-beyond.html#graalvm","text":"new VM based on HotSpot and OpenJDK. supports Ahead of Time (AOT) compilation polyglot programming Android Runtime has used AOT compilation since about 2013 GraalVM combines these two steps to produce machine-native images -- binary code which is created for the particular architecture on which the VM is running. Graal provides zero-overhead interoperability between Java, JavaScript, R, Python, Ruby, and C, thanks to the Truffle Language Implementation Framework. const express = require('express'); const app = express(); app.listen(3000); app.get('/', function(req, res) { var text = 'Hello World!'; const BigInteger = Java.type('java.math.BigInteger'); text += BigInteger.valueOf(2).pow(100).toString(16); text += Polyglot.eval('R', 'runif(100)')[0]; res.send(text); })","title":"GraalVM"},{"location":"java/java9-beyond.html#local-variable-type-inference","text":"var type allows for local type inference Local type inference means that var can only be used inside of method bodies or other similar blocks of code. It can't be used to declare instance variables or as the return type of a method, etc. Java is still a statically-typed language This is different from, for instance, JavaScript, where the type of a variable is dynamic and can change from line to line.","title":"local variable type inference"},{"location":"java/java9-beyond.html#unmodifiable-collection-enhancements","text":"List<Integer> view = Collections.unmodifiableList(lint);","title":"Unmodifiable Collection enhancements"},{"location":"java/java9-beyond.html#container-awarenesss","text":"Docker and Java are finally friends.","title":"Container awarenesss"},{"location":"java/java9-beyond.html#java-11","text":"","title":"Java 11"},{"location":"java/java9-beyond.html#single-source-file-launch","text":"java Example.java","title":"Single source file launch"},{"location":"java/java9-beyond.html#java-12","text":"","title":"Java 12"},{"location":"java/java9-beyond.html#switch-expressions","text":"a step toward pattern matching String name = switch(x) { case 1 -> \"one\"; case 2 -> \"two\"; case 3 -> \"three\"; default -> throw new IllegalArgumentException(\"I can only count to 3.\"); }; System.out.println(name);","title":"switch expressions"},{"location":"java/java9-beyond.html#teeing-collectors","text":"import static java.util.stream.Collectors.* var ints = DoubleStream.of(1, 2, 3, 4, 5) ints.boxed().collect(teeing( summingDouble(e -> e), counting(), (a,b) -> a/b ))","title":"teeing Collectors"},{"location":"java/java9-beyond.html#java-14","text":"","title":"Java 14"},{"location":"java/java9-beyond.html#multiline-text-blocks","text":"String html3 = \"\"\" <html> <body> <h1>\"I love Java and Java loves me!\"</h1> </body> </html> \"\"\"","title":"Multiline text blocks"},{"location":"java/java9-beyond.html#java-14_1","text":"","title":"Java 14+"},{"location":"java/java9-beyond.html#java-on-java-compiler-with-project-metropolis","text":"decouple Java from dependencies on other languages allow the VM to optimise itself maintainability / simplification","title":"Java-on-Java compiler with Project Metropolis"},{"location":"java/java9-beyond.html#project-amber","text":"flow typing void alertNChars (Object o) { if (o instanceof String s) System.out.println(\"String contains \" + s.length() + \" characters\"); else System.out.println(\"not a String\"); } anonymous variables BiFunction<Integer, Double, String> bids = (i, _) -> String.valueOf(i); data classes public instance variables hashCode() equals() toString() record Boilerplate (int myInt, double myDouble, String myString) { } sealed types sealed interface Car (Make make, Model model) { } record FossilFuelCar (Make make, Model model) implements Car { } record ElectricCar (Make make, Model model) implements Car { } record HybridCar (Make make, Model model) implements Car { } record FuelCellCar (Make make, Model model) implements Car { }","title":"Project Amber"},{"location":"java/java9-beyond.html#project-loom","text":"lightweight multithreading application-level Thread-like abstraction called a Fiber. VM-level multithreading (rather than OS-level) coroutines tail-call optimisation lightweight user-mode Fiber","title":"Project Loom"},{"location":"java/java9-beyond.html#project-valhalla","text":"value types generic specialisation reified generics","title":"Project Valhalla"},{"location":"java/java9-beyond.html#refferences","text":"20 Reasons to Move On from Java 8 Keeping up with Java 9, 10, 11, and Beyond From Java 8 to Java 11 \u2013 Quick Guide Java 8 v/s Java 11 JDK 17: The new features in Java 17 Java Versions and Features","title":"Refferences"},{"location":"java/jaxb.html","text":"Java Architecture for XML Binding \u00b6 TODO LINK","title":"Java Architecture for XML Binding"},{"location":"java/jaxb.html#java-architecture-for-xml-binding","text":"TODO LINK","title":"Java Architecture for XML Binding"},{"location":"java/jaxp.html","text":"","title":"Jaxp"},{"location":"java/jdbc.html","text":"JDBC \u00b6 The JDBC API is a Java API that can access any kind of tabular data, especially data stored in a Relational Database. Connect to a data source, like a database Send queries and update statements to the database Retrieve and process the results received from the database in answer to your query public void connectToAndQueryDatabase(String username, String password) { Connection con = DriverManager.getConnection( \"jdbc:myDriver:myDatabase\", username, password); Statement stmt = con.createStatement(); ResultSet rs = stmt.executeQuery(\"SELECT a, b, c FROM Table1\"); while (rs.next()) { int x = rs.getInt(\"a\"); String s = rs.getString(\"b\"); float f = rs.getFloat(\"c\"); } } Components \u00b6 The JDBC API (java.sql and javax.sql) JDBC Driver Manager JDBC Test Suite JDBC-ODBC Bridge JDBC Architecture \u00b6 2 Tier In the two-tier model, a Java applet or application talks directly to the data source. This requires a JDBC driver that can communicate with the particular data source being accessed. A user's commands are delivered to the database or other data source, and the results of those statements are sent back to the user. The data source may be located on another machine to which the user is connected via a network. This is referred to as a client/server configuration, with the user's machine as the client, and the machine housing the data source as the server. The network can be an intranet, which, for example, connects employees within a corporation, or it can be the Internet. 3 Tier In the three-tier model, commands are sent to a \"middle tier\" of services, which then sends the commands to the data source. The data source processes the commands and sends the results back to the middle tier, which then sends them to the user. MIS directors find the three-tier model very attractive because the middle tier makes it possible to maintain control over access and the kinds of updates that can be made to corporate data. Another advantage is that it simplifies the deployment of applications. Finally, in many cases, the three-tier architecture can provide performance advantages. Until recently, the middle tier has often been written in languages such as C or C++, which offer fast performance. However, with the introduction of optimizing compilers that translate Java bytecode into efficient machine-specific code and technologies such as Enterprise JavaBeans\u2122, the Java platform is fast becoming the standard platform for middle-tier development. This is a big plus, making it possible to take advantage of Java's robustness, multithreading, and security features. With enterprises increasingly using the Java programming language for writing server code, the JDBC API is being used more and more in the middle tier of a three-tier architecture. Some of the features that make JDBC a server technology are its support for connection pooling, distributed transactions, and disconnected rowsets. The JDBC API is also what allows access to a data source from a Java middle tier. Result Sets and Cursors \u00b6 The rows that satisfy the conditions of a query are called the result set. The number of rows returned in a result set can be zero, one, or many. A user can access the data in a result set one row at a time, and a cursor provides the means to do that. A cursor can be thought of as a pointer into a file that contains the rows of the result set, and that pointer has the ability to keep track of which row is currently being accessed. A cursor allows a user to process each row of a result set from top to bottom and consequently may be used for iterative processing. Most DBMSs create a cursor automatically when a result set is generated. Earlier JDBC API versions added new capabilities for a result set's cursor, allowing it to move both forward and backward and also allowing it to move to a specified row or to a row whose position is relative to another row. Transactions \u00b6 When one user is accessing data in a database, another user may be accessing the same data at the same time. If, for instance, the first user is updating some columns in a table at the same time the second user is selecting columns from that same table, it is possible for the second user to get partly old data and partly updated data. For this reason, DBMSs use transactions to maintain data in a consistent state (data consistency) while allowing more than one user to access a database at the same time (data concurrency). A transaction is a set of one or more SQL statements that make up a logical unit of work. A transaction ends with either a commit or a rollback, depending on whether there are any problems with data consistency or data concurrency. The commit statement makes permanent the changes resulting from the SQL statements in the transaction, and the rollback statement undoes all changes resulting from the SQL statements in the transaction. A lock is a mechanism that prohibits two transactions from manipulating the same data at the same time. For example, a table lock prevents a table from being dropped if there is an uncommitted transaction on that table. In some DBMSs, a table lock also locks all of the rows in a table. A row lock prevents two transactions from modifying the same row, or it prevents one transaction from selecting a row while another transaction is still modifying it. Stored Procedures \u00b6 A stored procedure is a group of SQL statements that can be called by name. In other words, it is executable code, a mini-program, that performs a particular task that can be invoked the same way one can call a function or method. Traditionally, stored procedures have been written in a DBMS-specific programming language. The latest generation of database products allows stored procedures to be written using the Java programming language and the JDBC API. Stored procedures written in the Java programming language are bytecode portable between DBMSs. Once a stored procedure is written, it can be used and reused because a DBMS that supports stored procedures will, as its name implies, store it in the database. See Using Stored Procedures for information about writing stored procedures. Metadata \u00b6 Databases store user data, and they also store information about the database itself. Most DBMSs have a set of system tables, which list tables in the database, column names in each table, primary keys, foreign keys, stored procedures, and so forth. Each DBMS has its own functions for getting information about table layouts and database features. JDBC provides the interface DatabaseMetaData, which a driver writer must implement so that its methods return information about the driver and/or DBMS for which the driver is written. For example, a large number of methods return whether or not the driver supports a particular functionality. This interface gives users and tools a standardized way to get metadata. In general, developers writing tools and drivers are the ones most likely to be concerned with metadata. TODO LINK","title":"JDBC"},{"location":"java/jdbc.html#jdbc","text":"The JDBC API is a Java API that can access any kind of tabular data, especially data stored in a Relational Database. Connect to a data source, like a database Send queries and update statements to the database Retrieve and process the results received from the database in answer to your query public void connectToAndQueryDatabase(String username, String password) { Connection con = DriverManager.getConnection( \"jdbc:myDriver:myDatabase\", username, password); Statement stmt = con.createStatement(); ResultSet rs = stmt.executeQuery(\"SELECT a, b, c FROM Table1\"); while (rs.next()) { int x = rs.getInt(\"a\"); String s = rs.getString(\"b\"); float f = rs.getFloat(\"c\"); } }","title":"JDBC"},{"location":"java/jdbc.html#components","text":"The JDBC API (java.sql and javax.sql) JDBC Driver Manager JDBC Test Suite JDBC-ODBC Bridge","title":"Components"},{"location":"java/jdbc.html#jdbc-architecture","text":"2 Tier In the two-tier model, a Java applet or application talks directly to the data source. This requires a JDBC driver that can communicate with the particular data source being accessed. A user's commands are delivered to the database or other data source, and the results of those statements are sent back to the user. The data source may be located on another machine to which the user is connected via a network. This is referred to as a client/server configuration, with the user's machine as the client, and the machine housing the data source as the server. The network can be an intranet, which, for example, connects employees within a corporation, or it can be the Internet. 3 Tier In the three-tier model, commands are sent to a \"middle tier\" of services, which then sends the commands to the data source. The data source processes the commands and sends the results back to the middle tier, which then sends them to the user. MIS directors find the three-tier model very attractive because the middle tier makes it possible to maintain control over access and the kinds of updates that can be made to corporate data. Another advantage is that it simplifies the deployment of applications. Finally, in many cases, the three-tier architecture can provide performance advantages. Until recently, the middle tier has often been written in languages such as C or C++, which offer fast performance. However, with the introduction of optimizing compilers that translate Java bytecode into efficient machine-specific code and technologies such as Enterprise JavaBeans\u2122, the Java platform is fast becoming the standard platform for middle-tier development. This is a big plus, making it possible to take advantage of Java's robustness, multithreading, and security features. With enterprises increasingly using the Java programming language for writing server code, the JDBC API is being used more and more in the middle tier of a three-tier architecture. Some of the features that make JDBC a server technology are its support for connection pooling, distributed transactions, and disconnected rowsets. The JDBC API is also what allows access to a data source from a Java middle tier.","title":"JDBC Architecture"},{"location":"java/jdbc.html#result-sets-and-cursors","text":"The rows that satisfy the conditions of a query are called the result set. The number of rows returned in a result set can be zero, one, or many. A user can access the data in a result set one row at a time, and a cursor provides the means to do that. A cursor can be thought of as a pointer into a file that contains the rows of the result set, and that pointer has the ability to keep track of which row is currently being accessed. A cursor allows a user to process each row of a result set from top to bottom and consequently may be used for iterative processing. Most DBMSs create a cursor automatically when a result set is generated. Earlier JDBC API versions added new capabilities for a result set's cursor, allowing it to move both forward and backward and also allowing it to move to a specified row or to a row whose position is relative to another row.","title":"Result Sets and Cursors"},{"location":"java/jdbc.html#transactions","text":"When one user is accessing data in a database, another user may be accessing the same data at the same time. If, for instance, the first user is updating some columns in a table at the same time the second user is selecting columns from that same table, it is possible for the second user to get partly old data and partly updated data. For this reason, DBMSs use transactions to maintain data in a consistent state (data consistency) while allowing more than one user to access a database at the same time (data concurrency). A transaction is a set of one or more SQL statements that make up a logical unit of work. A transaction ends with either a commit or a rollback, depending on whether there are any problems with data consistency or data concurrency. The commit statement makes permanent the changes resulting from the SQL statements in the transaction, and the rollback statement undoes all changes resulting from the SQL statements in the transaction. A lock is a mechanism that prohibits two transactions from manipulating the same data at the same time. For example, a table lock prevents a table from being dropped if there is an uncommitted transaction on that table. In some DBMSs, a table lock also locks all of the rows in a table. A row lock prevents two transactions from modifying the same row, or it prevents one transaction from selecting a row while another transaction is still modifying it.","title":"Transactions"},{"location":"java/jdbc.html#stored-procedures","text":"A stored procedure is a group of SQL statements that can be called by name. In other words, it is executable code, a mini-program, that performs a particular task that can be invoked the same way one can call a function or method. Traditionally, stored procedures have been written in a DBMS-specific programming language. The latest generation of database products allows stored procedures to be written using the Java programming language and the JDBC API. Stored procedures written in the Java programming language are bytecode portable between DBMSs. Once a stored procedure is written, it can be used and reused because a DBMS that supports stored procedures will, as its name implies, store it in the database. See Using Stored Procedures for information about writing stored procedures.","title":"Stored Procedures"},{"location":"java/jdbc.html#metadata","text":"Databases store user data, and they also store information about the database itself. Most DBMSs have a set of system tables, which list tables in the database, column names in each table, primary keys, foreign keys, stored procedures, and so forth. Each DBMS has its own functions for getting information about table layouts and database features. JDBC provides the interface DatabaseMetaData, which a driver writer must implement so that its methods return information about the driver and/or DBMS for which the driver is written. For example, a large number of methods return whether or not the driver supports a particular functionality. This interface gives users and tools a standardized way to get metadata. In general, developers writing tools and drivers are the ones most likely to be concerned with metadata. TODO LINK","title":"Metadata"},{"location":"java/jndi.html","text":"","title":"Jndi"},{"location":"java/json.html","text":"JSON \u00b6 TODO LINK1 LINK2","title":"JSON"},{"location":"java/json.html#json","text":"TODO LINK1 LINK2","title":"JSON"},{"location":"java/jsp.html","text":"JSP \u00b6 TODO LINK","title":"JSP"},{"location":"java/jsp.html#jsp","text":"TODO LINK","title":"JSP"},{"location":"java/networking.html","text":"Networking \u00b6 TODO LINK","title":"Networking"},{"location":"java/networking.html#networking","text":"TODO LINK","title":"Networking"},{"location":"java/numbers.html","text":"Numbers \u00b6 reasons to use objects in place of primitives, and the Java platform provides wrapper classes for each of the primitive data types. These classes \"wrap\" the primitive in an object. Often, the wrapping is done by the compiler As an argument of a method that expects an object (often used when manipulating collections of numbers). To use constants defined by the class, such as MIN_VALUE and MAX_VALUE, that provide the upper and lower bounds of the data type. To use class methods for converting values to and from other primitive types, for converting to and from strings, and for converting between number systems (decimal, octal, hexadecimal, binary). BigDecimal and BigInteger are used for high-precision calculations. AtomicInteger and AtomicLong are used for multi-threaded applications. Byte Short Integer Long Float Double System.out.printf(...) System.out.format(\"The value of \" + \"the float variable is \" + \"%f%n, while the value of the \" + \"integer variable is %d, \" + \"and the string is %s\", floatVar, intVar, stringVar); public PrintStream format(Locale l, String format, Object... args) the java.text.DecimalFormat(...) DecimalFormat myFormatter = new DecimalFormat(pattern); String output = myFormatter.format(value); System.out.println(value + \" \" + pattern + \" \" + output); number pattern output 123456.789 ###,###.### 123,456.789 123456.789 ###.## 123456.79 123.78 000000.000 000123.780 12345.67 $###,###.### $12,345.67 Advanced math \u00b6 import static java.lang.Math.*; E PI abs ceil floor rint round min max exp log pow sqrt sin random() java.util.Random","title":"Numbers"},{"location":"java/numbers.html#numbers","text":"reasons to use objects in place of primitives, and the Java platform provides wrapper classes for each of the primitive data types. These classes \"wrap\" the primitive in an object. Often, the wrapping is done by the compiler As an argument of a method that expects an object (often used when manipulating collections of numbers). To use constants defined by the class, such as MIN_VALUE and MAX_VALUE, that provide the upper and lower bounds of the data type. To use class methods for converting values to and from other primitive types, for converting to and from strings, and for converting between number systems (decimal, octal, hexadecimal, binary). BigDecimal and BigInteger are used for high-precision calculations. AtomicInteger and AtomicLong are used for multi-threaded applications. Byte Short Integer Long Float Double System.out.printf(...) System.out.format(\"The value of \" + \"the float variable is \" + \"%f%n, while the value of the \" + \"integer variable is %d, \" + \"and the string is %s\", floatVar, intVar, stringVar); public PrintStream format(Locale l, String format, Object... args) the java.text.DecimalFormat(...) DecimalFormat myFormatter = new DecimalFormat(pattern); String output = myFormatter.format(value); System.out.println(value + \" \" + pattern + \" \" + output); number pattern output 123456.789 ###,###.### 123,456.789 123456.789 ###.## 123456.79 123.78 000000.000 000123.780 12345.67 $###,###.### $12,345.67","title":"Numbers"},{"location":"java/numbers.html#advanced-math","text":"import static java.lang.Math.*; E PI abs ceil floor rint round min max exp log pow sqrt sin random() java.util.Random","title":"Advanced math"},{"location":"java/oops.html","text":"Java OOPS \u00b6 Objects \u00b6 real world mapping of objects state / fields / variables behaviour / methods / functions - primary mechanism for object-to-object communication Why? \u00b6 Modularity : an object can be easily passed around inside the system. Information- hiding Code re-use Pluggability and debugging ease : If a bolt breaks, you replace it, not the entire machine. data encapsulation \u00b6 Hiding internal state and requiring all interaction to be performed through an object's methods. Class \u00b6 bicycle is an instance of the class of objects known as bicycles A class is the blueprint from which individual objects are created. (instances) Inheritance \u00b6 organizing and structuring your software subclass has superclass's fields and methods. class MountainBike extends Bicycle { // new fields and methods defining // a mountain bike would go here } Interface \u00b6 form a contract between the class and the outside world. interface Bicycle { // wheel revolutions per minute void changeCadence(int newValue); void changeGear(int newValue); void speedUp(int increment); void applyBrakes(int decrement); } class ACMEBicycle implements Bicycle { int cadence = 0; int speed = 0; int gear = 1; void changeCadence(int newValue) { cadence = newValue; } void changeGear(int newValue) { gear = newValue; } void speedUp(int increment) { speed = speed + increment; } void applyBrakes(int decrement) { speed = speed - decrement; } void printStates() { System.out.println(\"cadence:\" + cadence + \" speed:\" + speed + \" gear:\" + gear); } } Package \u00b6 namespace that organizes a set of related classes and interfaces. Library \u00b6 a set of packages","title":"Java OOPS"},{"location":"java/oops.html#java-oops","text":"","title":"Java OOPS"},{"location":"java/oops.html#objects","text":"real world mapping of objects state / fields / variables behaviour / methods / functions - primary mechanism for object-to-object communication","title":"Objects"},{"location":"java/oops.html#why","text":"Modularity : an object can be easily passed around inside the system. Information- hiding Code re-use Pluggability and debugging ease : If a bolt breaks, you replace it, not the entire machine.","title":"Why?"},{"location":"java/oops.html#data-encapsulation","text":"Hiding internal state and requiring all interaction to be performed through an object's methods.","title":"data encapsulation"},{"location":"java/oops.html#class","text":"bicycle is an instance of the class of objects known as bicycles A class is the blueprint from which individual objects are created. (instances)","title":"Class"},{"location":"java/oops.html#inheritance","text":"organizing and structuring your software subclass has superclass's fields and methods. class MountainBike extends Bicycle { // new fields and methods defining // a mountain bike would go here }","title":"Inheritance"},{"location":"java/oops.html#interface","text":"form a contract between the class and the outside world. interface Bicycle { // wheel revolutions per minute void changeCadence(int newValue); void changeGear(int newValue); void speedUp(int increment); void applyBrakes(int decrement); } class ACMEBicycle implements Bicycle { int cadence = 0; int speed = 0; int gear = 1; void changeCadence(int newValue) { cadence = newValue; } void changeGear(int newValue) { gear = newValue; } void speedUp(int increment) { speed = speed + increment; } void applyBrakes(int decrement) { speed = speed - decrement; } void printStates() { System.out.println(\"cadence:\" + cadence + \" speed:\" + speed + \" gear:\" + gear); } }","title":"Interface"},{"location":"java/oops.html#package","text":"namespace that organizes a set of related classes and interfaces.","title":"Package"},{"location":"java/oops.html#library","text":"a set of packages","title":"Library"},{"location":"java/packages.html","text":"Packages \u00b6 A package is a grouping of related types (classes, interfaces, enumerations (classes), and annotation types (interfaces)) providing access protection and name space management. Why? \u00b6 types are related. easy to find types that can provide particular functions. The names won't conflict with the type names in other packages because the package creates a new namespace. unrestricted access to one another yet still restrict access for types outside the package. At top: package <name>; only one public type in one source file. You can include non-public types in the same file as a public type (this is strongly discouraged, unless the non-public types are small and closely related to the public type). All the top-level, non-public types will be package private. Naming \u00b6 The fully qualified name -> library.package.class lower case reversed Internet domain name region specific -> com.example.region.mypackage Packages in the Java language itself begin with java. or javax. add an underscore for reserved keywords and hyphens adn starting with numbers Usage \u00b6 by Its Qualified Name \u00b6 all right for infrequent use You can use a package member's simple name if the code you are writing is in the same package as that member or if that member has been imported. Importing a Package Member \u00b6 import graphics.Rectangle; Importing an Entire Package \u00b6 if you use many types from a package, you should import the entire package. It cannot be used to match a subset of the classes in a package. import graphics.*; import the public nested classes of an enclosing class. \u00b6 import graphics.Rectangle; import graphics.Rectangle.*; Be aware that the second import statement will not import Rectangle. static import \u00b6 when you need frequent access to static final fields (constants) and static methods from one or two classes. import static java.lang.Math.*; Java compiler automatically imports: the java.lang package the package for the current file Apparent Hierarchies of Packages \u00b6 java.awt.xxxx packages are not included in the java.awt package. java.awt is used for a number of related packages to make the relationship evident, but not to show inclusion. Name Ambiguities \u00b6 if a type in 2 packages have same name and both packagesare included -> use member's fully qualified name. Managing Source and Class Files \u00b6 hierarchical file systems to manage source and class files, although The Java Language Specification does not require this. <path_one>\\sources\\com\\example\\graphics\\Rectangle.java <path_two>\\classes\\com\\example\\graphics\\Rectangle.class The full path to the classes directory, \\classes, is called the class path, and is set with the CLASSPATH system variable. Both the compiler and the JVM construct the path to your .class files by adding the package name to the class path. For example, if <path_two>\\classes is your class path, and the package name is com.example.graphics, then the compiler and JVM look for .class files in <path_two>\\classes\\com\\example\\graphics. By default, the compiler and the JVM search the current directory and the JAR file containing the Java platform classes so that these directories are automatically in your class path.","title":"Packages"},{"location":"java/packages.html#packages","text":"A package is a grouping of related types (classes, interfaces, enumerations (classes), and annotation types (interfaces)) providing access protection and name space management.","title":"Packages"},{"location":"java/packages.html#why","text":"types are related. easy to find types that can provide particular functions. The names won't conflict with the type names in other packages because the package creates a new namespace. unrestricted access to one another yet still restrict access for types outside the package. At top: package <name>; only one public type in one source file. You can include non-public types in the same file as a public type (this is strongly discouraged, unless the non-public types are small and closely related to the public type). All the top-level, non-public types will be package private.","title":"Why?"},{"location":"java/packages.html#naming","text":"The fully qualified name -> library.package.class lower case reversed Internet domain name region specific -> com.example.region.mypackage Packages in the Java language itself begin with java. or javax. add an underscore for reserved keywords and hyphens adn starting with numbers","title":"Naming"},{"location":"java/packages.html#usage","text":"","title":"Usage"},{"location":"java/packages.html#by-its-qualified-name","text":"all right for infrequent use You can use a package member's simple name if the code you are writing is in the same package as that member or if that member has been imported.","title":"by Its Qualified Name"},{"location":"java/packages.html#importing-a-package-member","text":"import graphics.Rectangle;","title":"Importing a Package Member"},{"location":"java/packages.html#importing-an-entire-package","text":"if you use many types from a package, you should import the entire package. It cannot be used to match a subset of the classes in a package. import graphics.*;","title":"Importing an Entire Package"},{"location":"java/packages.html#import-the-public-nested-classes-of-an-enclosing-class","text":"import graphics.Rectangle; import graphics.Rectangle.*; Be aware that the second import statement will not import Rectangle.","title":"import the public nested classes of an enclosing class."},{"location":"java/packages.html#static-import","text":"when you need frequent access to static final fields (constants) and static methods from one or two classes. import static java.lang.Math.*; Java compiler automatically imports: the java.lang package the package for the current file","title":"static import"},{"location":"java/packages.html#apparent-hierarchies-of-packages","text":"java.awt.xxxx packages are not included in the java.awt package. java.awt is used for a number of related packages to make the relationship evident, but not to show inclusion.","title":"Apparent Hierarchies of Packages"},{"location":"java/packages.html#name-ambiguities","text":"if a type in 2 packages have same name and both packagesare included -> use member's fully qualified name.","title":"Name Ambiguities"},{"location":"java/packages.html#managing-source-and-class-files","text":"hierarchical file systems to manage source and class files, although The Java Language Specification does not require this. <path_one>\\sources\\com\\example\\graphics\\Rectangle.java <path_two>\\classes\\com\\example\\graphics\\Rectangle.class The full path to the classes directory, \\classes, is called the class path, and is set with the CLASSPATH system variable. Both the compiler and the JVM construct the path to your .class files by adding the package name to the class path. For example, if <path_two>\\classes is your class path, and the package name is com.example.graphics, then the compiler and JVM look for .class files in <path_two>\\classes\\com\\example\\graphics. By default, the compiler and the JVM search the current directory and the JAR file containing the Java platform classes so that these directories are automatically in your class path.","title":"Managing Source and Class Files"},{"location":"java/platform.html","text":"The Platform Environment \u00b6 TODO LINK","title":"The Platform Environment"},{"location":"java/platform.html#the-platform-environment","text":"TODO LINK","title":"The Platform Environment"},{"location":"java/re.html","text":"RegEx \u00b6 TODO LINK","title":"RegEx"},{"location":"java/re.html#regex","text":"TODO LINK","title":"RegEx"},{"location":"java/security.html","text":"Security \u00b6 Java provides firewall between a networked application and your computer. TODO LINK","title":"Security"},{"location":"java/security.html#security","text":"Java provides firewall between a networked application and your computer. TODO LINK","title":"Security"},{"location":"java/strings.html","text":"Strings \u00b6 Character class is immutable String class is immutable The Java programming language does not permit literal strings to span lines in source files Breaking strings between lines using the + concatenation operator is, once again, very common in print statements. int i; String s1 = \"\" + i; public class Filename { private String fullPath; private char pathSeparator, extensionSeparator; public Filename(String str, char sep, char ext) { fullPath = str; pathSeparator = sep; extensionSeparator = ext; } public String extension() { int dot = fullPath.lastIndexOf(extensionSeparator); return fullPath.substring(dot + 1); } // gets filename without extension public String filename() { int dot = fullPath.lastIndexOf(extensionSeparator); int sep = fullPath.lastIndexOf(pathSeparator); return fullPath.substring(sep + 1, dot); } public String path() { int sep = fullPath.lastIndexOf(pathSeparator); return fullPath.substring(0, sep); } } public class RegionMatchesDemo { public static void main(String[] args) { String searchMe = \"Green Eggs and Ham\"; String findMe = \"Eggs\"; int searchMeLength = searchMe.length(); int findMeLength = findMe.length(); boolean foundIt = false; for (int i = 0; i <= (searchMeLength - findMeLength); i++) { if (searchMe.regionMatches(i, findMe, 0, findMeLength)) { foundIt = true; System.out.println(searchMe.substring(i, i + findMeLength)); break; } } if (!foundIt) System.out.println(\"No match found.\"); } } public class StringDemo { public static void main(String[] args) { String palindrome = \"Dot saw I was Tod\"; int len = palindrome.length(); char[] tempCharArray = new char[len]; char[] charArray = new char[len]; // put original string in an // array of chars for (int i = 0; i < len; i++) { tempCharArray[i] = palindrome.charAt(i); } // reverse array of chars for (int j = 0; j < len; j++) { charArray[j] = tempCharArray[len - 1 - j]; } String reversePalindrome = new String(charArray); System.out.println(reversePalindrome); } } Why? \u00b6 String is immutable while stringbuilder ain't Note also that there is StringBuffer in addition to StringBuilder. The difference is that the former has synchronized methods. If you use it as a local variable, use StringBuilder. If it happens that it's possible for it to be accessed by multiple threads, use StringBuffer (that's rarer) autoboxing and unboxing \u00b6 autoboxing -> primitive to object unboxing -> object to primitive","title":"Strings"},{"location":"java/strings.html#strings","text":"Character class is immutable String class is immutable The Java programming language does not permit literal strings to span lines in source files Breaking strings between lines using the + concatenation operator is, once again, very common in print statements. int i; String s1 = \"\" + i; public class Filename { private String fullPath; private char pathSeparator, extensionSeparator; public Filename(String str, char sep, char ext) { fullPath = str; pathSeparator = sep; extensionSeparator = ext; } public String extension() { int dot = fullPath.lastIndexOf(extensionSeparator); return fullPath.substring(dot + 1); } // gets filename without extension public String filename() { int dot = fullPath.lastIndexOf(extensionSeparator); int sep = fullPath.lastIndexOf(pathSeparator); return fullPath.substring(sep + 1, dot); } public String path() { int sep = fullPath.lastIndexOf(pathSeparator); return fullPath.substring(0, sep); } } public class RegionMatchesDemo { public static void main(String[] args) { String searchMe = \"Green Eggs and Ham\"; String findMe = \"Eggs\"; int searchMeLength = searchMe.length(); int findMeLength = findMe.length(); boolean foundIt = false; for (int i = 0; i <= (searchMeLength - findMeLength); i++) { if (searchMe.regionMatches(i, findMe, 0, findMeLength)) { foundIt = true; System.out.println(searchMe.substring(i, i + findMeLength)); break; } } if (!foundIt) System.out.println(\"No match found.\"); } } public class StringDemo { public static void main(String[] args) { String palindrome = \"Dot saw I was Tod\"; int len = palindrome.length(); char[] tempCharArray = new char[len]; char[] charArray = new char[len]; // put original string in an // array of chars for (int i = 0; i < len; i++) { tempCharArray[i] = palindrome.charAt(i); } // reverse array of chars for (int j = 0; j < len; j++) { charArray[j] = tempCharArray[len - 1 - j]; } String reversePalindrome = new String(charArray); System.out.println(reversePalindrome); } }","title":"Strings"},{"location":"java/strings.html#why","text":"String is immutable while stringbuilder ain't Note also that there is StringBuffer in addition to StringBuilder. The difference is that the former has synchronized methods. If you use it as a local variable, use StringBuilder. If it happens that it's possible for it to be accessed by multiple threads, use StringBuffer (that's rarer)","title":"Why?"},{"location":"java/strings.html#autoboxing-and-unboxing","text":"autoboxing -> primitive to object unboxing -> object to primitive","title":"autoboxing and unboxing"},{"location":"java/spring/REST.html","text":"","title":"REST"},{"location":"java/spring/auth.html","text":"","title":"Auth"},{"location":"java/spring/beans.html","text":"","title":"Beans"},{"location":"java/spring/boot.html","text":"","title":"Boot"},{"location":"java/spring/data.html","text":"","title":"Data"},{"location":"java/spring/db.html","text":"Databases \u00b6 MySQL \u00b6 MongoDb \u00b6 Neo4j \u00b6 SQL \u00b6","title":"Databases"},{"location":"java/spring/db.html#databases","text":"","title":"Databases"},{"location":"java/spring/db.html#mysql","text":"","title":"MySQL"},{"location":"java/spring/db.html#mongodb","text":"","title":"MongoDb"},{"location":"java/spring/db.html#neo4j","text":"","title":"Neo4j"},{"location":"java/spring/db.html#sql","text":"","title":"SQL"},{"location":"java/spring/dev-ops.html","text":"","title":"Dev ops"},{"location":"java/spring/intro.html","text":"Spring \u00b6 Spring makes programming Java quicker, easier, and safer for everybody. Spring\u2019s focus on speed, simplicity, and productivity has made it the world's most popular Java framework. Why? \u00b6 Spring is everywhere fast secure productive flexible supportive When \u00b6 Microservicecs Reactive Cloud Web Apps Serverless Event Driven Batch Basic Model \u00b6 At its core, Spring offers a container , often referred to as the Spring application context, that creates and manages application components . These components, or beans , are wired together inside the Spring application context to make a complete application, The act of wiring beans together is based on a pattern known as dependency injection (DI).Rather than have components create and maintain the lifecycle of other beans that they depend on, a dependency-injected application relies on a separate entity (the container) to create and maintain all components and inject those into the beans that need them. This is done typically through constructor arguments or property accessor methods. Confguration \u00b6 by XML <bean id=\"inventoryService\" class=\"com.example.InventoryService\" /> <bean id=\"productService\" class=\"com.example.ProductService\" /> <constructor-arg ref=\"inventoryService\" /> </bean> by Java greater type safety and improved refactorability. @Configuration public class ServiceConfiguration { @Bean public InventoryService inventoryService() { return new InventoryService(); } @Bean public ProductService productService() { return new ProductService(inventoryService()); } } Automatic configuration has its roots in the Spring techniques known as autowiring and component scanning . Spring Boot is an extension of the Spring Framework that offers several productivity enhancements. The most well-known of these enhancements is autoconfiguration , where Spring Boot can make reasonable guesses of what components need to be configured and wired together, based on entries in the classpath, environment variables, and other factors . Spring Initializr \u00b6 (Web App)[http://start.spring.io] curl Spring Boot CLI Spring Tool Suite IntelliJ IDEA NetBeans Resourcess \u00b6 Best learning resources for the Spring Framework (incl. Spring Boot): books, podcasts, YouTube channels, websites and video courses Spring in Action by Crag Walls Official guides","title":"Spring"},{"location":"java/spring/intro.html#spring","text":"Spring makes programming Java quicker, easier, and safer for everybody. Spring\u2019s focus on speed, simplicity, and productivity has made it the world's most popular Java framework.","title":"Spring"},{"location":"java/spring/intro.html#why","text":"Spring is everywhere fast secure productive flexible supportive","title":"Why?"},{"location":"java/spring/intro.html#when","text":"Microservicecs Reactive Cloud Web Apps Serverless Event Driven Batch","title":"When"},{"location":"java/spring/intro.html#basic-model","text":"At its core, Spring offers a container , often referred to as the Spring application context, that creates and manages application components . These components, or beans , are wired together inside the Spring application context to make a complete application, The act of wiring beans together is based on a pattern known as dependency injection (DI).Rather than have components create and maintain the lifecycle of other beans that they depend on, a dependency-injected application relies on a separate entity (the container) to create and maintain all components and inject those into the beans that need them. This is done typically through constructor arguments or property accessor methods.","title":"Basic Model"},{"location":"java/spring/intro.html#confguration","text":"by XML <bean id=\"inventoryService\" class=\"com.example.InventoryService\" /> <bean id=\"productService\" class=\"com.example.ProductService\" /> <constructor-arg ref=\"inventoryService\" /> </bean> by Java greater type safety and improved refactorability. @Configuration public class ServiceConfiguration { @Bean public InventoryService inventoryService() { return new InventoryService(); } @Bean public ProductService productService() { return new ProductService(inventoryService()); } } Automatic configuration has its roots in the Spring techniques known as autowiring and component scanning . Spring Boot is an extension of the Spring Framework that offers several productivity enhancements. The most well-known of these enhancements is autoconfiguration , where Spring Boot can make reasonable guesses of what components need to be configured and wired together, based on entries in the classpath, environment variables, and other factors .","title":"Confguration"},{"location":"java/spring/intro.html#spring-initializr","text":"(Web App)[http://start.spring.io] curl Spring Boot CLI Spring Tool Suite IntelliJ IDEA NetBeans","title":"Spring Initializr"},{"location":"java/spring/intro.html#resourcess","text":"Best learning resources for the Spring Framework (incl. Spring Boot): books, podcasts, YouTube channels, websites and video courses Spring in Action by Crag Walls Official guides","title":"Resourcess"},{"location":"java/spring/kotlin.html","text":"","title":"Kotlin"},{"location":"java/spring/maven.html","text":"","title":"Maven"},{"location":"java/spring/mvc.html","text":"","title":"Mvc"},{"location":"java/spring/profiles.html","text":"","title":"Profiles"},{"location":"java/spring/react.html","text":"","title":"React"},{"location":"java/spring/security.html","text":"","title":"Security"},{"location":"java/spring/soap.html","text":"","title":"Soap"},{"location":"java/spring/telementery.html","text":"","title":"Telementery"},{"location":"java/spring/test.html","text":"","title":"Test"},{"location":"java/spring/vault.html","text":"","title":"Vault"},{"location":"js/async.html","text":"","title":"Async"},{"location":"js/classes.html","text":"","title":"Classes"},{"location":"js/closures.html","text":"","title":"Closures"},{"location":"js/dev-tools.html","text":"Dev Tools \u00b6 reference","title":"Dev Tools"},{"location":"js/dev-tools.html#dev-tools","text":"reference","title":"Dev Tools"},{"location":"js/document.html","text":"","title":"Document"},{"location":"js/error-handling.html","text":"","title":"Error handling"},{"location":"js/events.html","text":"","title":"Events"},{"location":"js/execution-context.html","text":"","title":"Execution context"},{"location":"js/garbage-collection.html","text":"Garbage Collection \u00b6 The main concept of memory management in JavaScript is reachability . Internal algorithms \u00b6 The basic garbage collection algorithm is called mark-and-sweep . The following \u201cgarbage collection\u201d steps are regularly performed: The garbage collector takes roots and \u201cmarks\u201d (remembers) them. Then it visits and \u201cmarks\u201d all references from them. Then it visits marked objects and marks their references. All visited objects are remembered, so as not to visit the same object twice in the future. \u2026And so on until every reachable (from the roots) references are visited. All objects except marked ones are removed. Optimisations \u00b6 Generational collection \u2013 objects are split into two sets: \u201cnew ones\u201d and \u201cold ones\u201d. Many objects appear, do their job and die fast, they can be cleaned up aggressively. Those that survive for long enough, become \u201cold\u201d and are examined less often. Incremental collection \u2013 if there are many objects, and we try to walk and mark the whole object set at once, it may take some time and introduce visible delays in the execution. So the engine tries to split the garbage collection into pieces. Then the pieces are executed one by one, separately. That requires some extra bookkeeping between them to track changes, but we have many tiny delays instead of a big one. Idle-time collection \u2013 the garbage collector tries to run only while the CPU is idle, to reduce the possible effect on the execution.","title":"Garbage Collection"},{"location":"js/garbage-collection.html#garbage-collection","text":"The main concept of memory management in JavaScript is reachability .","title":"Garbage Collection"},{"location":"js/garbage-collection.html#internal-algorithms","text":"The basic garbage collection algorithm is called mark-and-sweep . The following \u201cgarbage collection\u201d steps are regularly performed: The garbage collector takes roots and \u201cmarks\u201d (remembers) them. Then it visits and \u201cmarks\u201d all references from them. Then it visits marked objects and marks their references. All visited objects are remembered, so as not to visit the same object twice in the future. \u2026And so on until every reachable (from the roots) references are visited. All objects except marked ones are removed.","title":"Internal algorithms"},{"location":"js/garbage-collection.html#optimisations","text":"Generational collection \u2013 objects are split into two sets: \u201cnew ones\u201d and \u201cold ones\u201d. Many objects appear, do their job and die fast, they can be cleaned up aggressively. Those that survive for long enough, become \u201cold\u201d and are examined less often. Incremental collection \u2013 if there are many objects, and we try to walk and mark the whole object set at once, it may take some time and introduce visible delays in the execution. So the engine tries to split the garbage collection into pieces. Then the pieces are executed one by one, separately. That requires some extra bookkeeping between them to track changes, but we have many tiny delays instead of a big one. Idle-time collection \u2013 the garbage collector tries to run only while the CPU is idle, to reduce the possible effect on the execution.","title":"Optimisations"},{"location":"js/generators.html","text":"","title":"Generators"},{"location":"js/hoisting.html","text":"","title":"Hoisting"},{"location":"js/intro.html","text":"js \u00b6 Modern JavaScript is a \u201csafe\u201d programming language. It does not provide low-level access to memory or CPU, because it was initially created for browsers which do not require it. Same Origin Policy : Different tabs/windows generally do not know about each other. Full integration with HTML/CSS. Simple things are done simply. Supported by all major browsers and enabled by default. Engines \u00b6 V8 \u2013 Chrome, Opera and Edge. SpiderMonkey \u2013 Firefox. Chakra - IE The engine (embedded if it\u2019s a browser) reads (\u201cparses\u201d) the script. Then it converts (\u201ccompiles\u201d) the script to the machine language. And then the machine code runs, pretty fast. Ajax \u00b6 Comet \u00b6 Languages \u201cover\u201d JavaScript \u00b6 which are transpiled (converted) to JavaScript CoffeeScript is a \u201csyntactic sugar\u201d for JavaScript. (Ruby Devs llike it) TypeScript - concentrated on adding \u201cstrict data typing\u201d, by Microsoft. Flow also adds data typing, but in a different way. Developed by Facebook. Dart is a standalone language that has its own engine that runs in non-browser environments (like mobile apps), but also can be transpiled to JavaScript. Developed by Google. Brython is a Python transpiler to JavaScript that enables the writing of applications in pure Python without JavaScript. Kotlin is a modern, concise and safe programming language that can target the browser or Node. Variables \u00b6 let $ = 1; // declared a variable with the name \"$\" let _ = 2; // and now a variable with the name \"_\" alert($ + _); // 3 let 1a; // cannot start with a digit let my-name; // hyphens '-' aren't allowed in the name Mathematical operations are safe \u00b6 Doing maths is \u201csafe\u201d in JavaScript. We can do anything: divide by zero (Infinity), treat non-numeric strings as numbers, etc. The script will never stop with a fatal error (\u201cdie\u201d). At worst, we\u2019ll get NaN as the result. interaction \u00b6 alert: shows a message. prompt: shows a message asking the user to input text. It returns the text or, if Cancel button or Esc is clicked, null. confirm: shows a message and waits for the user to press \u201cOK\u201d or \u201cCancel\u201d. It returns true for OK and false for Cancel/Esc. null vs undefined \u00b6 Treat any comparison with undefined/null except the strict equality === with exceptional care. Don\u2019t use comparisons >= > < <= with a variable which may be null/undefined, unless you\u2019re really sure of what you\u2019re doing. If a variable can have these values, check for them separately. Non-traditional ? \u00b6 let company = prompt('Which company created JavaScript?', ''); (company == 'Netscape') ? alert('Right!') : alert('Wrong.'); ?? \u00b6 The result of a ?? b is: if a is defined, then a, if a isn\u2019t defined, then b. || returns the first truthy value. ?? returns the first defined value. Using ?? with && or || \u00b6 let x = (1 && 2) ?? 3; // Works Labels \u00b6 outer: for (let i = 0; i < 3; i++) { for (let j = 0; j < 3; j++) { let input = prompt(`Value at coords (${i},${j})`, ''); // if an empty string or canceled, then break out of both loops if (!input) break outer; // (*) // do something with the value... } } alert('Done!'); Type matters in switch Function Declaration \u00b6 // Function Declaration function sum(a, b) { return a + b; } // Function Expression let sum = function(a, b) { return a + b; }; A Function Expression is created when the execution reaches it and is usable only from that moment. A Function Declaration can be called earlier than it is defined. In strict mode, when a Function Declaration is within a code block, it\u2019s visible everywhere inside that block. But not outside of it. Arrow functions \u00b6 let func = (arg1, arg2, ..., argN) => expressionlet func = (arg1, arg2, ..., argN) => expression (...args) => { body } Clean Code \u00b6 Google's js style guide ESLint Comment this: Overall architecture, high-level view. Function usage. Important solutions, especially when not immediately obvious. Avoid comments: That tell \u201chow code works\u201d and \u201cwhat it does\u201d. Put them in only if it\u2019s impossible to make the code so simple and self-descriptive that it doesn\u2019t require them.","title":"js"},{"location":"js/intro.html#js","text":"Modern JavaScript is a \u201csafe\u201d programming language. It does not provide low-level access to memory or CPU, because it was initially created for browsers which do not require it. Same Origin Policy : Different tabs/windows generally do not know about each other. Full integration with HTML/CSS. Simple things are done simply. Supported by all major browsers and enabled by default.","title":"js"},{"location":"js/intro.html#engines","text":"V8 \u2013 Chrome, Opera and Edge. SpiderMonkey \u2013 Firefox. Chakra - IE The engine (embedded if it\u2019s a browser) reads (\u201cparses\u201d) the script. Then it converts (\u201ccompiles\u201d) the script to the machine language. And then the machine code runs, pretty fast.","title":"Engines"},{"location":"js/intro.html#ajax","text":"","title":"Ajax"},{"location":"js/intro.html#comet","text":"","title":"Comet"},{"location":"js/intro.html#languages-over-javascript","text":"which are transpiled (converted) to JavaScript CoffeeScript is a \u201csyntactic sugar\u201d for JavaScript. (Ruby Devs llike it) TypeScript - concentrated on adding \u201cstrict data typing\u201d, by Microsoft. Flow also adds data typing, but in a different way. Developed by Facebook. Dart is a standalone language that has its own engine that runs in non-browser environments (like mobile apps), but also can be transpiled to JavaScript. Developed by Google. Brython is a Python transpiler to JavaScript that enables the writing of applications in pure Python without JavaScript. Kotlin is a modern, concise and safe programming language that can target the browser or Node.","title":"Languages \u201cover\u201d JavaScript"},{"location":"js/intro.html#variables","text":"let $ = 1; // declared a variable with the name \"$\" let _ = 2; // and now a variable with the name \"_\" alert($ + _); // 3 let 1a; // cannot start with a digit let my-name; // hyphens '-' aren't allowed in the name","title":"Variables"},{"location":"js/intro.html#mathematical-operations-are-safe","text":"Doing maths is \u201csafe\u201d in JavaScript. We can do anything: divide by zero (Infinity), treat non-numeric strings as numbers, etc. The script will never stop with a fatal error (\u201cdie\u201d). At worst, we\u2019ll get NaN as the result.","title":"Mathematical operations are safe"},{"location":"js/intro.html#interaction","text":"alert: shows a message. prompt: shows a message asking the user to input text. It returns the text or, if Cancel button or Esc is clicked, null. confirm: shows a message and waits for the user to press \u201cOK\u201d or \u201cCancel\u201d. It returns true for OK and false for Cancel/Esc.","title":"interaction"},{"location":"js/intro.html#null-vs-undefined","text":"Treat any comparison with undefined/null except the strict equality === with exceptional care. Don\u2019t use comparisons >= > < <= with a variable which may be null/undefined, unless you\u2019re really sure of what you\u2019re doing. If a variable can have these values, check for them separately.","title":"null vs undefined"},{"location":"js/intro.html#non-traditional","text":"let company = prompt('Which company created JavaScript?', ''); (company == 'Netscape') ? alert('Right!') : alert('Wrong.');","title":"Non-traditional ?"},{"location":"js/intro.html#_1","text":"The result of a ?? b is: if a is defined, then a, if a isn\u2019t defined, then b. || returns the first truthy value. ?? returns the first defined value.","title":"??"},{"location":"js/intro.html#using-with-or","text":"let x = (1 && 2) ?? 3; // Works","title":"Using ?? with &amp;&amp; or ||"},{"location":"js/intro.html#labels","text":"outer: for (let i = 0; i < 3; i++) { for (let j = 0; j < 3; j++) { let input = prompt(`Value at coords (${i},${j})`, ''); // if an empty string or canceled, then break out of both loops if (!input) break outer; // (*) // do something with the value... } } alert('Done!'); Type matters in switch","title":"Labels"},{"location":"js/intro.html#function-declaration","text":"// Function Declaration function sum(a, b) { return a + b; } // Function Expression let sum = function(a, b) { return a + b; }; A Function Expression is created when the execution reaches it and is usable only from that moment. A Function Declaration can be called earlier than it is defined. In strict mode, when a Function Declaration is within a code block, it\u2019s visible everywhere inside that block. But not outside of it.","title":"Function Declaration"},{"location":"js/intro.html#arrow-functions","text":"let func = (arg1, arg2, ..., argN) => expressionlet func = (arg1, arg2, ..., argN) => expression (...args) => { body }","title":"Arrow functions"},{"location":"js/intro.html#clean-code","text":"Google's js style guide ESLint Comment this: Overall architecture, high-level view. Function usage. Important solutions, especially when not immediately obvious. Avoid comments: That tell \u201chow code works\u201d and \u201cwhat it does\u201d. Put them in only if it\u2019s impossible to make the code so simple and self-descriptive that it doesn\u2019t require them.","title":"Clean Code"},{"location":"js/let-var.html","text":"let vs var \u00b6 var has no block scope var tolerates redeclarations var variables can be declared below their use IIFE : immediately-invoked function expressions (function() { var message = \"Hello\"; alert(message); // Hello })();","title":"let vs var"},{"location":"js/let-var.html#let-vs-var","text":"var has no block scope var tolerates redeclarations var variables can be declared below their use IIFE : immediately-invoked function expressions (function() { var message = \"Hello\"; alert(message); // Hello })();","title":"let vs var"},{"location":"js/links.html","text":"javascript.info Ecma International Standard MDN Web Docs Check Compatibility","title":"Links"},{"location":"js/modules.html","text":"","title":"Modules"},{"location":"js/ninja-code.html","text":"","title":"Ninja code"},{"location":"js/objects.html","text":"Objects \u00b6 An object can be created with figure brackets {\u2026} with an optional list of properties. A property is a \u201ckey: value\u201d pair, where key is a string (also called a \u201cproperty name\u201d), and value can be anything. let obj = { 0: \"test\" // same as \"0\": \"test\" }; let user = { name: \"John\", age: 30, isAdmin: true }; for (let key in user) { // keys alert( key ); // name, age, isAdmin // values for the keys alert( user[key] ); // John, 30, true } A variable assigned to an object stores not the object itself, but its \u201caddress in memory\u201d \u2013 in other words \u201ca reference\u201d to it. When an object variable is copied, the reference is copied, but the object itself is not duplicated. let a = {}; let b = {}; // two independent objects alert( a == b ); // false Cloning \u00b6 Object.assign(dest, [src1, src2, src3...]) const user = { name: \"John\" }; user.name = \"Pete\"; // (*) alert(user.name); // Pete This \u00b6 Method Shorthand \u00b6 // these objects do the same user = { sayHi: function() { alert(\"Hello\"); } }; // method shorthand looks better, right? user = { sayHi() { // same as \"sayHi: function(){...}\" alert(\"Hello\"); } };","title":"Objects"},{"location":"js/objects.html#objects","text":"An object can be created with figure brackets {\u2026} with an optional list of properties. A property is a \u201ckey: value\u201d pair, where key is a string (also called a \u201cproperty name\u201d), and value can be anything. let obj = { 0: \"test\" // same as \"0\": \"test\" }; let user = { name: \"John\", age: 30, isAdmin: true }; for (let key in user) { // keys alert( key ); // name, age, isAdmin // values for the keys alert( user[key] ); // John, 30, true } A variable assigned to an object stores not the object itself, but its \u201caddress in memory\u201d \u2013 in other words \u201ca reference\u201d to it. When an object variable is copied, the reference is copied, but the object itself is not duplicated. let a = {}; let b = {}; // two independent objects alert( a == b ); // false","title":"Objects"},{"location":"js/objects.html#cloning","text":"Object.assign(dest, [src1, src2, src3...]) const user = { name: \"John\" }; user.name = \"Pete\"; // (*) alert(user.name); // Pete","title":"Cloning"},{"location":"js/objects.html#this","text":"","title":"This"},{"location":"js/objects.html#method-shorthand","text":"// these objects do the same user = { sayHi: function() { alert(\"Hello\"); } }; // method shorthand looks better, right? user = { sayHi() { // same as \"sayHi: function(){...}\" alert(\"Hello\"); } };","title":"Method Shorthand"},{"location":"js/prototypes.html","text":"","title":"Prototypes"},{"location":"js/re.html","text":"","title":"Re"},{"location":"js/security.html","text":"","title":"Security"},{"location":"js/shadowing.html","text":"","title":"Shadowing"},{"location":"js/testing.html","text":"","title":"Testing"},{"location":"js/transpilers.html","text":"","title":"Transpilers"},{"location":"js/undefined.html","text":"","title":"Undefined"},{"location":"js/use-strict.html","text":"use strict \u00b6 To keep the old code working, most such modifications are off by default. You need to explicitly enable them with a special directive: \"use strict\". (at the top) Modern JavaScript supports \u201cclasses\u201d and \u201cmodules\u201d \u2013 advanced language structures that enable use strict automatically. So we don\u2019t need to add the \"use strict\" directive, if we use them. \"use strict\"; // this code works the modern way (function() { 'use strict'; // ...your code here... })()","title":"use strict"},{"location":"js/use-strict.html#use-strict","text":"To keep the old code working, most such modifications are off by default. You need to explicitly enable them with a special directive: \"use strict\". (at the top) Modern JavaScript supports \u201cclasses\u201d and \u201cmodules\u201d \u2013 advanced language structures that enable use strict automatically. So we don\u2019t need to add the \"use strict\" directive, if we use them. \"use strict\"; // this code works the modern way (function() { 'use strict'; // ...your code here... })()","title":"use strict"},{"location":"js/v8.html","text":"","title":"V8"},{"location":"js/web-components.html","text":"","title":"Web components"},{"location":"languages/jp.html","text":"Japanese \u00b6","title":"Japanese"},{"location":"languages/jp.html#japanese","text":"","title":"Japanese"},{"location":"linux/setup.html","text":"Arch Installation \u00b6 fdisk -l cfdisk /dev/sda /dev/sda1 - 1G - for /boot /dev/sda2 - 5G - for root /dev/sda3 - 1G - for swap -> make type to swap mkfs.ext4 /dev/sda1 mkfs.ext4 /dev/sda2 mkswap /dev/sda3 swapon /dev/sda3 mount /dev/sda2 /mnt mkdir /mnt/boot /mnt/var /mnt/home mount /dev/sda1 /mnt/boot pacman -Syy pacstrap /mnt base base-devel linux linux-firmware nano dhcpcd net-tools grub genfstab -U /mnt >> /mnt/etc/fstab arch-chroot /mnt nano /etc/locale.gen <uncomment> locale-gen echo LANG=en_US.UTF-8 > /etc/locale.conf export LANG=en_US.UTF-8 ln -s /usr/share/zoneinfo/America/New_York /etc/localtime hwclock --systohc --utc echo arindam-pc > /etc/hostname systemctl enable dhcpcd passwd root useradd -m -g users -G main -s /bin/bash abhinav passwd abhinav nano /etc/sudoers root ALL=(ALL) ALL abhinav ALL=(ALL) ALL grub-install /dev/sda grub-mkconfig -o /boot/grub/grub.cfg mkinitcpio -p linux exit umount /mnt/boot umount /mnt reboot sudo pacman -S --needed xorg sudo pacman -S --needed xfce4 xfce4-goodies parole ristretto thunar-archive-plugin \\\\ thunar-media-tags-plugin file-roller network-manager-applet \\\\ leafpad epdfview galculator lightdm lightdm-gtk-greeter \\\\ lightdm-gtk-greeter-settings capitaine-cursors arc-gtk-theme \\\\ xdg-user-dirs-gtk systemctl enable lightdm systemctl enable NetworkManager reboot VmWare Tools \u00b6 sudo pacman -S open-vm-tools sudo pacman -Su xf86-input-vmmouse xf86-video-vmware mesa gtk2 gtkmm sudo echo needs_root_rights=yes >> /etc/X11/Xwrapper.config sudo systemctl enable vmtoolsd sudo systemctl start vmtoolsd Clipboard \u00b6 vmx file isolation.tools.copy.disable = \"FALSE\" isolation.tools.dnd.disable = \"FALSE\" isolation.tools.paste.disable = \"FALSE\" isolation.tools.hgfs.disable = \"FALSE\" Utils \u00b6 pacman -S firefox-developer-edition yay Desktop going black error \u00b6 Remove $HOME/.cache/sessions and logoff/on. xfdesktop --replace Theming \u00b6 Install from https://www.xfce-look.org/p/1327720/ XFCE themes are available at xfce-look.org. Xfwm themes are stored in /usr/share/themes/theme_name/xfwm4, and set in Settings > Window Manager. GTK themes are stored in /usr/share/themes/theme_name/gtk-2.0 and /usr/share/themes/theme_name/gtk-3.0 and are set in Settings > Appearance. Move the extracted folder containing the icons to either ~/.icons or ~/.local/share/icons (user only) or to /usr/share/icons (systemwide). Optional: run gtk-update-icon-cache -f -t ~/.icons/<theme_name> to update the icon cache. Select the icon theme using the appropriate configuration tool for your desktop environment or window manager. Emacs \u00b6","title":"Arch Installation"},{"location":"linux/setup.html#arch-installation","text":"fdisk -l cfdisk /dev/sda /dev/sda1 - 1G - for /boot /dev/sda2 - 5G - for root /dev/sda3 - 1G - for swap -> make type to swap mkfs.ext4 /dev/sda1 mkfs.ext4 /dev/sda2 mkswap /dev/sda3 swapon /dev/sda3 mount /dev/sda2 /mnt mkdir /mnt/boot /mnt/var /mnt/home mount /dev/sda1 /mnt/boot pacman -Syy pacstrap /mnt base base-devel linux linux-firmware nano dhcpcd net-tools grub genfstab -U /mnt >> /mnt/etc/fstab arch-chroot /mnt nano /etc/locale.gen <uncomment> locale-gen echo LANG=en_US.UTF-8 > /etc/locale.conf export LANG=en_US.UTF-8 ln -s /usr/share/zoneinfo/America/New_York /etc/localtime hwclock --systohc --utc echo arindam-pc > /etc/hostname systemctl enable dhcpcd passwd root useradd -m -g users -G main -s /bin/bash abhinav passwd abhinav nano /etc/sudoers root ALL=(ALL) ALL abhinav ALL=(ALL) ALL grub-install /dev/sda grub-mkconfig -o /boot/grub/grub.cfg mkinitcpio -p linux exit umount /mnt/boot umount /mnt reboot sudo pacman -S --needed xorg sudo pacman -S --needed xfce4 xfce4-goodies parole ristretto thunar-archive-plugin \\\\ thunar-media-tags-plugin file-roller network-manager-applet \\\\ leafpad epdfview galculator lightdm lightdm-gtk-greeter \\\\ lightdm-gtk-greeter-settings capitaine-cursors arc-gtk-theme \\\\ xdg-user-dirs-gtk systemctl enable lightdm systemctl enable NetworkManager reboot","title":"Arch Installation"},{"location":"linux/setup.html#vmware-tools","text":"sudo pacman -S open-vm-tools sudo pacman -Su xf86-input-vmmouse xf86-video-vmware mesa gtk2 gtkmm sudo echo needs_root_rights=yes >> /etc/X11/Xwrapper.config sudo systemctl enable vmtoolsd sudo systemctl start vmtoolsd","title":"VmWare Tools"},{"location":"linux/setup.html#clipboard","text":"vmx file isolation.tools.copy.disable = \"FALSE\" isolation.tools.dnd.disable = \"FALSE\" isolation.tools.paste.disable = \"FALSE\" isolation.tools.hgfs.disable = \"FALSE\"","title":"Clipboard"},{"location":"linux/setup.html#utils","text":"pacman -S firefox-developer-edition yay","title":"Utils"},{"location":"linux/setup.html#desktop-going-black-error","text":"Remove $HOME/.cache/sessions and logoff/on. xfdesktop --replace","title":"Desktop going black error"},{"location":"linux/setup.html#theming","text":"Install from https://www.xfce-look.org/p/1327720/ XFCE themes are available at xfce-look.org. Xfwm themes are stored in /usr/share/themes/theme_name/xfwm4, and set in Settings > Window Manager. GTK themes are stored in /usr/share/themes/theme_name/gtk-2.0 and /usr/share/themes/theme_name/gtk-3.0 and are set in Settings > Appearance. Move the extracted folder containing the icons to either ~/.icons or ~/.local/share/icons (user only) or to /usr/share/icons (systemwide). Optional: run gtk-update-icon-cache -f -t ~/.icons/<theme_name> to update the icon cache. Select the icon theme using the appropriate configuration tool for your desktop environment or window manager.","title":"Theming"},{"location":"linux/setup.html#emacs","text":"","title":"Emacs"},{"location":"macos/vmware.html","text":"VMX \u00b6 smbios.reflectHost = \"TRUE\" hw.model = \"MacBookPro14,3\" board-id = \"Mac-551B86E5744E2388\" smc.version = \"0\"","title":"VMX"},{"location":"macos/vmware.html#vmx","text":"smbios.reflectHost = \"TRUE\" hw.model = \"MacBookPro14,3\" board-id = \"Mac-551B86E5744E2388\" smc.version = \"0\"","title":"VMX"},{"location":"networking/TODO.html","text":"create git repo for configuration files (~/.*)","title":"TODO"},{"location":"networking/flow.html","text":"Flow \u00b6","title":"Flow"},{"location":"networking/flow.html#flow","text":"","title":"Flow"},{"location":"networking/intro.html","text":"(Bible)[https://www.practicalnetworking.net/series/packet-traveling/packet-traveling/]","title":"Intro"},{"location":"networking/osi.html","text":"OSI \u00b6","title":"OSI"},{"location":"networking/osi.html#osi","text":"","title":"OSI"},{"location":"networking/setup.html","text":"Networking Setup \u00b6 System wide TOR \u00b6 Applications can still learn your computer's hostname, MAC address, serial number, timezone, etc. and those with root privileges can disable the firewall entirely. In other words, transparent torification with iptables protects against accidental connections and DNS leaks by misconfigured software, it is not sufficient to protect against malware or software with serious security vulnerabilities. Packages \u00b6 tor nyx - cli monitor torsocks - run commands over tor Setup up system-wide proxy \u00b6 systemctl enable tor.service Transparent Torification configure your system to use 127.0.0.1 as its DNS server Configuration \u00b6 /etc/tor/torrc /etc/iptables/iptables.rules *nat :PREROUTING ACCEPT [6:2126] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [17:6239] :POSTROUTING ACCEPT [6:408] -A PREROUTING ! -i lo -p udp -m udp --dport 53 -j REDIRECT --to-ports 5353 -A PREROUTING ! -i lo -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REDIRECT --to-ports 9040 -A OUTPUT -o lo -j RETURN --ipv4 -A OUTPUT -d 192.168.0.0/16 -j RETURN -A OUTPUT -m owner --uid-owner \"tor\" -j RETURN -A OUTPUT -p udp -m udp --dport 53 -j REDIRECT --to-ports 5353 -A OUTPUT -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REDIRECT --to-ports 9040 COMMIT *filter :INPUT DROP [0:0] :FORWARD DROP [0:0] :OUTPUT DROP [0:0] -A INPUT -i lo -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT --ipv4 -A INPUT -p tcp -j REJECT --reject-with tcp-reset --ipv4 -A INPUT -p udp -j REJECT --reject-with icmp-port-unreachable --ipv4 -A INPUT -j REJECT --reject-with icmp-proto-unreachable --ipv6 -A INPUT -j REJECT --ipv4 -A OUTPUT -d 127.0.0.0/8 -j ACCEPT --ipv4 -A OUTPUT -d 192.168.0.0/16 -j ACCEPT --ipv6 -A OUTPUT -d ::1/8 -j ACCEPT -A OUTPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A OUTPUT -m owner --uid-owner \"tor\" -j ACCEPT --ipv4 -A OUTPUT -j REJECT --reject-with icmp-port-unreachable --ipv6 -A OUTPUT -j REJECT COMMIT TorDNS DNSPort 53 AutomapHostsOnResolve 1 AutomapHostsSuffixes .exit,.onion ControlPort 9051 Whonix, or TorVM (over qubesOS) \u00b6 virtualized torification applications Amnesic solution like Tails \u00b6 Terminal \u00b6 Terminator CTRL+SHIFT+O \u2013 Horizontal Split CTRL+SHIFT+E \u2013 Vertical Split Tmux CTRL+B & C -> new session CTRL+B & <SESSION-NUMBER> -> switch to session ZSH oh my zsh Automatic macchanger configuration \u00b6 /etc/NetworkManager/conf.d/wifi_rand_mac.conf wifi.scan-rand-mac-address=yes ethernet.cloned-mac-address=random wifi.cloned-mac-address=stable Tools \u00b6 BURP suite zaproxy -> owasp web app scanner Wireshark nmap + zenmap docker gnu-netcat -> swiss army networking tool metasploit shodan -> iot search engine wpscan weevely -> remote shell Veil-framework -> obsfcucate exploits // use py2exe sqlmap -> sql injection routersploit -> embedded system exploitation recon-ng rainbowcrack, johnthereaper exploitdb -> find exploits honeypot -> http://canarytokens.org/generate theHarvester, OSRFramework, dmitry -> OSINT https://gtfobins.github.io/ -> linux privelage escalation Evil-Droid, apkwash -> embedding payloads into apk's Quasar -> window's RAT Windows Credentials Editor (WCE) -> post exploitation. LaZagne, Empire -> post exploitation maltego -> open source intelligence and graphical link analysis tool BlackArch \u00b6 smikims-arpspoof Other relevent packages \u00b6 yay -> AUR helper python git macchanger wine -> run windows applications snapd -> snap store Languages \u00b6 ruby / rvm python pip java go c++ / make","title":"Networking Setup"},{"location":"networking/setup.html#networking-setup","text":"","title":"Networking Setup"},{"location":"networking/setup.html#system-wide-tor","text":"Applications can still learn your computer's hostname, MAC address, serial number, timezone, etc. and those with root privileges can disable the firewall entirely. In other words, transparent torification with iptables protects against accidental connections and DNS leaks by misconfigured software, it is not sufficient to protect against malware or software with serious security vulnerabilities.","title":"System wide TOR"},{"location":"networking/setup.html#packages","text":"tor nyx - cli monitor torsocks - run commands over tor","title":"Packages"},{"location":"networking/setup.html#setup-up-system-wide-proxy","text":"systemctl enable tor.service Transparent Torification configure your system to use 127.0.0.1 as its DNS server","title":"Setup up system-wide proxy"},{"location":"networking/setup.html#configuration","text":"/etc/tor/torrc /etc/iptables/iptables.rules *nat :PREROUTING ACCEPT [6:2126] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [17:6239] :POSTROUTING ACCEPT [6:408] -A PREROUTING ! -i lo -p udp -m udp --dport 53 -j REDIRECT --to-ports 5353 -A PREROUTING ! -i lo -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REDIRECT --to-ports 9040 -A OUTPUT -o lo -j RETURN --ipv4 -A OUTPUT -d 192.168.0.0/16 -j RETURN -A OUTPUT -m owner --uid-owner \"tor\" -j RETURN -A OUTPUT -p udp -m udp --dport 53 -j REDIRECT --to-ports 5353 -A OUTPUT -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j REDIRECT --to-ports 9040 COMMIT *filter :INPUT DROP [0:0] :FORWARD DROP [0:0] :OUTPUT DROP [0:0] -A INPUT -i lo -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT --ipv4 -A INPUT -p tcp -j REJECT --reject-with tcp-reset --ipv4 -A INPUT -p udp -j REJECT --reject-with icmp-port-unreachable --ipv4 -A INPUT -j REJECT --reject-with icmp-proto-unreachable --ipv6 -A INPUT -j REJECT --ipv4 -A OUTPUT -d 127.0.0.0/8 -j ACCEPT --ipv4 -A OUTPUT -d 192.168.0.0/16 -j ACCEPT --ipv6 -A OUTPUT -d ::1/8 -j ACCEPT -A OUTPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A OUTPUT -m owner --uid-owner \"tor\" -j ACCEPT --ipv4 -A OUTPUT -j REJECT --reject-with icmp-port-unreachable --ipv6 -A OUTPUT -j REJECT COMMIT TorDNS DNSPort 53 AutomapHostsOnResolve 1 AutomapHostsSuffixes .exit,.onion ControlPort 9051","title":"Configuration"},{"location":"networking/setup.html#whonix-or-torvm-over-qubesos","text":"virtualized torification applications","title":"Whonix, or TorVM (over qubesOS)"},{"location":"networking/setup.html#amnesic-solution-like-tails","text":"","title":"Amnesic solution like Tails"},{"location":"networking/setup.html#terminal","text":"Terminator CTRL+SHIFT+O \u2013 Horizontal Split CTRL+SHIFT+E \u2013 Vertical Split Tmux CTRL+B & C -> new session CTRL+B & <SESSION-NUMBER> -> switch to session ZSH oh my zsh","title":"Terminal"},{"location":"networking/setup.html#automatic-macchanger-configuration","text":"/etc/NetworkManager/conf.d/wifi_rand_mac.conf wifi.scan-rand-mac-address=yes ethernet.cloned-mac-address=random wifi.cloned-mac-address=stable","title":"Automatic macchanger configuration"},{"location":"networking/setup.html#tools","text":"BURP suite zaproxy -> owasp web app scanner Wireshark nmap + zenmap docker gnu-netcat -> swiss army networking tool metasploit shodan -> iot search engine wpscan weevely -> remote shell Veil-framework -> obsfcucate exploits // use py2exe sqlmap -> sql injection routersploit -> embedded system exploitation recon-ng rainbowcrack, johnthereaper exploitdb -> find exploits honeypot -> http://canarytokens.org/generate theHarvester, OSRFramework, dmitry -> OSINT https://gtfobins.github.io/ -> linux privelage escalation Evil-Droid, apkwash -> embedding payloads into apk's Quasar -> window's RAT Windows Credentials Editor (WCE) -> post exploitation. LaZagne, Empire -> post exploitation maltego -> open source intelligence and graphical link analysis tool","title":"Tools"},{"location":"networking/setup.html#blackarch","text":"smikims-arpspoof","title":"BlackArch"},{"location":"networking/setup.html#other-relevent-packages","text":"yay -> AUR helper python git macchanger wine -> run windows applications snapd -> snap store","title":"Other relevent packages"},{"location":"networking/setup.html#languages","text":"ruby / rvm python pip java go c++ / make","title":"Languages"},{"location":"oops/abstraction.html","text":"","title":"Abstraction"},{"location":"oops/aggregation.html","text":"","title":"Aggregation"},{"location":"oops/anti-patterns.html","text":"Anti Patterns \u00b6 ECS (Entity-Component derivatives) \u00b6 The pattern has nothing to do with OOP. It introduces severe limitations, locking developers all the way down to procedural programming. However, it helps to reduce blast radius from inexperienced devs in big teams (reminds me of microservices and Kubernetes somehow). ECS is based on a Service Locator in disguise. Look at the way systems find components to execute on, you'll get the idea. It's very difficult to write strict ECS code. Developers end up breaking the pattern: \"Because purist-ecs is basically fucking unusable.\" - A popular Unity/C# programmer who's name won't be disclosed. MVC \u00b6 Just use an object composition instead. Controller is the especially problematic bit. Nearly all objects ending with -er are wrong: DI Container (or IoC Container) \u00b6 A more descriptive name to this anti-pattern would be a \"Service Locator Container\" or \"Service Locator Injection\". Yet it helps to mitigate limitations of MonoBehaviour design. Inability to use constructors is otherwise a huge problem.","title":"Anti Patterns"},{"location":"oops/anti-patterns.html#anti-patterns","text":"","title":"Anti Patterns"},{"location":"oops/anti-patterns.html#ecs-entity-component-derivatives","text":"The pattern has nothing to do with OOP. It introduces severe limitations, locking developers all the way down to procedural programming. However, it helps to reduce blast radius from inexperienced devs in big teams (reminds me of microservices and Kubernetes somehow). ECS is based on a Service Locator in disguise. Look at the way systems find components to execute on, you'll get the idea. It's very difficult to write strict ECS code. Developers end up breaking the pattern: \"Because purist-ecs is basically fucking unusable.\" - A popular Unity/C# programmer who's name won't be disclosed.","title":"ECS (Entity-Component derivatives)"},{"location":"oops/anti-patterns.html#mvc","text":"Just use an object composition instead. Controller is the especially problematic bit. Nearly all objects ending with -er are wrong:","title":"MVC"},{"location":"oops/anti-patterns.html#di-container-or-ioc-container","text":"A more descriptive name to this anti-pattern would be a \"Service Locator Container\" or \"Service Locator Injection\". Yet it helps to mitigate limitations of MonoBehaviour design. Inability to use constructors is otherwise a huge problem.","title":"DI Container (or IoC Container)"},{"location":"oops/association.html","text":"","title":"Association"},{"location":"oops/class.html","text":"","title":"Class"},{"location":"oops/cohesion.html","text":"","title":"Cohesion"},{"location":"oops/composition.html","text":"","title":"Composition"},{"location":"oops/coupling.html","text":"","title":"Coupling"},{"location":"oops/encapsulation.html","text":"","title":"Encapsulation"},{"location":"oops/inheretance.html","text":"","title":"Inheretance"},{"location":"oops/intro.html","text":"","title":"Intro"},{"location":"oops/object.html","text":"","title":"Object"},{"location":"oops/observer.html","text":"Observer \u00b6 Used for triggering method invocation between decoupled projects.","title":"Observer"},{"location":"oops/observer.html#observer","text":"Used for triggering method invocation between decoupled projects.","title":"Observer"},{"location":"oops/polymorphism.html","text":"","title":"Polymorphism"},{"location":"oops/strategy.html","text":"Strategy \u00b6 Used for injecting dependencies via a constructor (usually an interface instead of concrete implementation). Indirectly helps to avoid polling and empty method calls.","title":"Strategy"},{"location":"oops/strategy.html#strategy","text":"Used for injecting dependencies via a constructor (usually an interface instead of concrete implementation). Indirectly helps to avoid polling and empty method calls.","title":"Strategy"},{"location":"os/bios.html","text":"BIOS \u00b6 UEFI \u00b6","title":"BIOS"},{"location":"os/bios.html#bios","text":"","title":"BIOS"},{"location":"os/bios.html#uefi","text":"","title":"UEFI"},{"location":"os/boot.html","text":"Boot \u00b6 Startup -> master boot record -> bootloader stage 2 -> kernel -> Init","title":"Boot"},{"location":"os/boot.html#boot","text":"Startup -> master boot record -> bootloader stage 2 -> kernel -> Init","title":"Boot"},{"location":"os/deadlock.html","text":"Deadlock \u00b6 Conditions \u00b6 Mutual exclusion Hold and wait No preemption Circular wait","title":"Deadlock"},{"location":"os/deadlock.html#deadlock","text":"","title":"Deadlock"},{"location":"os/deadlock.html#conditions","text":"Mutual exclusion Hold and wait No preemption Circular wait","title":"Conditions"},{"location":"os/intro.html","text":"Operating System \u00b6 manage hardware user <-> os <-> hardware Types \u00b6 single - one at a time, random order batch - batch programs for efficiency (time + space). multiprograming - run one, store many multitasking - run multiple, fast switching time-sharing distributed - one os, multiple (cpu, mem, disk) network - server, client real-time - min response time <apps <-> RTOS/kernel <-> BSP <-> hardware> hard , soft Multi \u00b6 multiprogramming - multiprocessing - >1 processes multitasking - share 1 resource multithreading - (Bible)[https://www.os-book.com/OS9/slide-dir/index.html]","title":"Operating System"},{"location":"os/intro.html#operating-system","text":"manage hardware user <-> os <-> hardware","title":"Operating System"},{"location":"os/intro.html#types","text":"single - one at a time, random order batch - batch programs for efficiency (time + space). multiprograming - run one, store many multitasking - run multiple, fast switching time-sharing distributed - one os, multiple (cpu, mem, disk) network - server, client real-time - min response time <apps <-> RTOS/kernel <-> BSP <-> hardware> hard , soft","title":"Types"},{"location":"os/intro.html#multi","text":"multiprogramming - multiprocessing - >1 processes multitasking - share 1 resource multithreading - (Bible)[https://www.os-book.com/OS9/slide-dir/index.html]","title":"Multi"},{"location":"os/ipc.html","text":"","title":"Ipc"},{"location":"os/kernel.html","text":"Kernel \u00b6 Types \u00b6 monolithic - 1 binary + 1 address space layered \u00b5Kernel -> multiple processes aka servers, services via IPC. kernel + user space","title":"Kernel"},{"location":"os/kernel.html#kernel","text":"","title":"Kernel"},{"location":"os/kernel.html#types","text":"monolithic - 1 binary + 1 address space layered \u00b5Kernel -> multiple processes aka servers, services via IPC. kernel + user space","title":"Types"},{"location":"os/memory.html","text":"Memory \u00b6 RAM - power supply ROM - moving parts Speed Cost Capacity Permanent Management \u00b6","title":"Memory"},{"location":"os/memory.html#memory","text":"RAM - power supply ROM - moving parts Speed Cost Capacity Permanent","title":"Memory"},{"location":"os/memory.html#management","text":"","title":"Management"},{"location":"os/partition.html","text":"GPT \u00b6 MBR \u00b6","title":"GPT"},{"location":"os/partition.html#gpt","text":"","title":"GPT"},{"location":"os/partition.html#mbr","text":"","title":"MBR"},{"location":"os/process.html","text":"Process \u00b6 running program in mem. Structure \u00b6 Stack Heap Data Text States \u00b6 new ready running waiting (for I/O) terminated Types \u00b6 PCB \u00b6 Threads \u00b6 segment of program, shared memory. Scheduling \u00b6 order of processes. Long term -> jobs short term -> cpu allocation medium term -> process swapping Max Zombies? \u00b6 #include<stdio.h> #include<unistd.h> int main() { int count = 0; while (fork() > 0) { count++; printf(\"%d\\t\", count); } }","title":"Process"},{"location":"os/process.html#process","text":"running program in mem.","title":"Process"},{"location":"os/process.html#structure","text":"Stack Heap Data Text","title":"Structure"},{"location":"os/process.html#states","text":"new ready running waiting (for I/O) terminated","title":"States"},{"location":"os/process.html#types","text":"","title":"Types"},{"location":"os/process.html#pcb","text":"","title":"PCB"},{"location":"os/process.html#threads","text":"segment of program, shared memory.","title":"Threads"},{"location":"os/process.html#scheduling","text":"order of processes. Long term -> jobs short term -> cpu allocation medium term -> process swapping","title":"Scheduling"},{"location":"os/process.html#max-zombies","text":"#include<stdio.h> #include<unistd.h> int main() { int count = 0; while (fork() > 0) { count++; printf(\"%d\\t\", count); } }","title":"Max Zombies?"},{"location":"os/sync.html","text":"Synchronization \u00b6 Race condition \u00b6 A semaphore is an integer variable that, apart from initialization, is accessed only through two atomic operations called wait() and signal().","title":"Synchronization"},{"location":"os/sync.html#synchronization","text":"","title":"Synchronization"},{"location":"os/sync.html#race-condition","text":"A semaphore is an integer variable that, apart from initialization, is accessed only through two atomic operations called wait() and signal().","title":"Race condition"},{"location":"os/virtual.html","text":"Virtualization \u00b6 hardware isolation Containerization \u00b6 software isolation","title":"Virtualization"},{"location":"os/virtual.html#virtualization","text":"hardware isolation","title":"Virtualization"},{"location":"os/virtual.html#containerization","text":"software isolation","title":"Containerization"},{"location":"php/atttributes.html","text":"<?php interface ActionHandler { public function execute(); } #[Attribute] class SetUp {} class CopyFile implements ActionHandler { public string $fileName; public string $targetDirectory; #[SetUp] public function fileExists() { if (!file_exists($this->fileName)) { throw new RuntimeException(\"File does not exist\"); } } #[SetUp] public function targetDirectoryExists() { if (!file_exists($this->targetDirectory)) { mkdir($this->targetDirectory); } elseif (!is_dir($this->targetDirectory)) { throw new RuntimeException(\"Target directory $this->targetDirectory is not a directory\"); } } public function execute() { copy($this->fileName, $this->targetDirectory . '/' . basename($this->fileName)); } } function executeAction(ActionHandler $actionHandler) { $reflection = new ReflectionObject($actionHandler); foreach ($reflection->getMethods() as $method) { $attributes = $method->getAttributes(SetUp::class); if (count($attributes) > 0) { $methodName = $method->getName(); $actionHandler->$methodName(); } } $actionHandler->execute(); } $copyAction = new CopyFile(); $copyAction->fileName = \"/tmp/foo.jpg\"; $copyAction->targetDirectory = \"/home/user\"; executeAction($copyAction);","title":"Atttributes"},{"location":"php/basics.html","text":"<?= which is a short-hand to the more verbose <?php echo. If a file contains only PHP code, it is preferable to omit the PHP closing tag at the end of the file. which may cause unwanted effects because PHP will start output buffering Html Escaping \u00b6 <?php if ($expression == true): ?> This will show if the expression is true. <?php else: ?> Otherwise this will show. <?php endif; ?> Comments \u00b6 Notes can come in all sorts of shapes and sizes. They vary, and their uses are completely up to the person writing the code. However, I try to keep things consistent in my code that way it's easy for the next person to read. So something like this might help <?php //====================================================================== // CATEGORY LARGE FONT //====================================================================== //----------------------------------------------------- // Sub-Category Smaller Font //----------------------------------------------------- /* Title Here Notice the First Letters are Capitalized */ # Option 1 # Option 2 # Option 3 /* * This is a detailed explanation * of something that should require * several paragraphs of information. */ // This is a single line quote. ?> HTML comments have no meaning in PHP parser (php-doc)[http://www.phpdoc.org/] types \u00b6 PHP supports 10 primitive types. scalar types: bool int float (floating-point number, aka double) string compound types: array object callable iterable special types: resource NULL double doesn't exist now. php compiler decides type not programmer var_dump() -> type and value gettype() -> human readable is_type functions -> to check type <?php $a_bool = TRUE; // a boolean $a_str = \"foo\"; // a string $a_str2 = 'foo'; // a string $an_int = 12; // an integer echo gettype($a_bool); // prints out: boolean echo gettype($a_str); // prints out: string // If this is an integer, increment it by four if (is_int($an_int)) { $an_int += 4; } // If $a_bool is a string, print it out // (does not print out anything) if (is_string($a_bool)) { echo \"String: $a_bool\"; } ?> boolean \u00b6 <?php var_dump((bool) \"\"); // bool(false) var_dump((bool) \"0\"); // bool(false) var_dump((bool) 1); // bool(true) var_dump((bool) -2); // bool(true) var_dump((bool) \"foo\"); // bool(true) var_dump((bool) 2.3e5); // bool(true) var_dump((bool) array(12)); // bool(true) var_dump((bool) array()); // bool(false) var_dump((bool) \"false\"); // bool(true) ?> $z=TRUE OR FALSE; // FALSE $z=TRUE || FALSE; // TRUE // precedence of operators <?php $var1 = TRUE; $var2 = FALSE; echo $var1; // Will display the number 1 echo $var2; //Will display nothing /* To get it to display the number 0 for a false value you have to typecast it: */ echo (int)$var2; //This will display the number 0 for false. ?> Integers \u00b6 <?php $a = 1234; // decimal number $a = 0123; // octal number (equivalent to 83 decimal) $a = 0x1A; // hexadecimal number (equivalent to 26 decimal) $a = 0b11111111; // binary number (equivalent to 255 decimal) $a = 1_234_567; // decimal number (as of PHP 7.4.0) ?> Float \u00b6 <?php $a = 1.234; $b = 1.2e3; $c = 7E-10; $d = 1_234.567; // as of PHP 7.4.0 ?> Warning Floating point precision Floating point numbers have limited precision. Although it depends on the system, PHP typically uses the IEEE 754 double precision format, which will give a maximum relative error due to rounding in the order of 1.11e-16. Non elementary arithmetic operations may give larger errors, and, of course, error propagation must be considered when several operations are compounded. Additionally, rational numbers that are exactly representable as floating point numbers in base 10, like 0.1 or 0.7, do not have an exact representation as floating point numbers in base 2, which is used internally, no matter the size of the mantissa. Hence, they cannot be converted into their internal binary counterparts without a small loss of precision. This can lead to confusing results: for example, floor((0.1+0.7)*10) will usually return 7 instead of the expected 8, since the internal representation will be something like 7.9999999999999991118.... So never trust floating number results to the last digit, and do not compare floating point numbers directly for equality. If higher precision is necessary, the arbitrary precision math functions and gmp functions are available. For a \"simple\" explanation, see the \u00bb floating point guide that's also titled \"Why don\u2019t my numbers add up?\" String \u00b6 <?php $foo = 1 + \"10.5\"; // $foo is float (11.5) $foo = 1 + \"-1.3e3\"; // $foo is float (-1299) $foo = 1 + \"bob-1.3e3\"; // TypeError as of PHP 8.0.0, $foo is integer (1) previously $foo = 1 + \"bob3\"; // TypeError as of PHP 8.0.0, $foo is integer (1) previously $foo = 1 + \"10 Small Pigs\"; // $foo is integer (11) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously $foo = 4 + \"10.2 Little Piggies\"; // $foo is float (14.2) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously $foo = \"10.0 pigs \" + 1; // $foo is float (11) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously $foo = \"10.0 pigs \" + 1.0; // $foo is float (11) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously ?> Arrays \u00b6 <?php $array = array( \"foo\" => \"bar\", \"bar\" => \"foo\", ); // Using the short array syntax $array = [ \"foo\" => \"bar\", \"bar\" => \"foo\", ]; ?> Iterable \u00b6 <?php function foo(iterable $iterable) { foreach ($iterable as $value) { // ... } } ?> Object \u00b6 <?php class foo { function do_foo() { echo \"Doing foo.\"; } } $bar = new foo; $bar->do_foo(); ?> Reources \u00b6 get_resource_type() Callable \u00b6 // An example callback function function my_callback_function() { echo 'hello world!'; } // An example callback method class MyClass { static function myCallbackMethod() { echo 'Hello World!'; } } // Type 1: Simple callback call_user_func('my_callback_function'); // Type 2: Static class method call call_user_func(array('MyClass', 'myCallbackMethod')); // Type 3: Object method call $obj = new MyClass(); call_user_func(array($obj, 'myCallbackMethod')); // Type 4: Static class method call call_user_func('MyClass::myCallbackMethod'); // Type 5: Relative static class method call class A { public static function who() { echo \"A\\n\"; } } class B extends A { public static function who() { echo \"B\\n\"; } } call_user_func(array('B', 'parent::who')); // A // Type 6: Objects implementing __invoke can be used as callables class C { public function __invoke($name) { echo 'Hello ', $name, \"\\n\"; } } $c = new C(); call_user_func($c, 'PHP!'); ?> Type Juggling and declartion \u00b6 TODO","title":"Basics"},{"location":"php/basics.html#html-escaping","text":"<?php if ($expression == true): ?> This will show if the expression is true. <?php else: ?> Otherwise this will show. <?php endif; ?>","title":"Html Escaping"},{"location":"php/basics.html#comments","text":"Notes can come in all sorts of shapes and sizes. They vary, and their uses are completely up to the person writing the code. However, I try to keep things consistent in my code that way it's easy for the next person to read. So something like this might help <?php //====================================================================== // CATEGORY LARGE FONT //====================================================================== //----------------------------------------------------- // Sub-Category Smaller Font //----------------------------------------------------- /* Title Here Notice the First Letters are Capitalized */ # Option 1 # Option 2 # Option 3 /* * This is a detailed explanation * of something that should require * several paragraphs of information. */ // This is a single line quote. ?> HTML comments have no meaning in PHP parser (php-doc)[http://www.phpdoc.org/]","title":"Comments"},{"location":"php/basics.html#types","text":"PHP supports 10 primitive types. scalar types: bool int float (floating-point number, aka double) string compound types: array object callable iterable special types: resource NULL double doesn't exist now. php compiler decides type not programmer var_dump() -> type and value gettype() -> human readable is_type functions -> to check type <?php $a_bool = TRUE; // a boolean $a_str = \"foo\"; // a string $a_str2 = 'foo'; // a string $an_int = 12; // an integer echo gettype($a_bool); // prints out: boolean echo gettype($a_str); // prints out: string // If this is an integer, increment it by four if (is_int($an_int)) { $an_int += 4; } // If $a_bool is a string, print it out // (does not print out anything) if (is_string($a_bool)) { echo \"String: $a_bool\"; } ?>","title":"types"},{"location":"php/basics.html#boolean","text":"<?php var_dump((bool) \"\"); // bool(false) var_dump((bool) \"0\"); // bool(false) var_dump((bool) 1); // bool(true) var_dump((bool) -2); // bool(true) var_dump((bool) \"foo\"); // bool(true) var_dump((bool) 2.3e5); // bool(true) var_dump((bool) array(12)); // bool(true) var_dump((bool) array()); // bool(false) var_dump((bool) \"false\"); // bool(true) ?> $z=TRUE OR FALSE; // FALSE $z=TRUE || FALSE; // TRUE // precedence of operators <?php $var1 = TRUE; $var2 = FALSE; echo $var1; // Will display the number 1 echo $var2; //Will display nothing /* To get it to display the number 0 for a false value you have to typecast it: */ echo (int)$var2; //This will display the number 0 for false. ?>","title":"boolean"},{"location":"php/basics.html#integers","text":"<?php $a = 1234; // decimal number $a = 0123; // octal number (equivalent to 83 decimal) $a = 0x1A; // hexadecimal number (equivalent to 26 decimal) $a = 0b11111111; // binary number (equivalent to 255 decimal) $a = 1_234_567; // decimal number (as of PHP 7.4.0) ?>","title":"Integers"},{"location":"php/basics.html#float","text":"<?php $a = 1.234; $b = 1.2e3; $c = 7E-10; $d = 1_234.567; // as of PHP 7.4.0 ?> Warning Floating point precision Floating point numbers have limited precision. Although it depends on the system, PHP typically uses the IEEE 754 double precision format, which will give a maximum relative error due to rounding in the order of 1.11e-16. Non elementary arithmetic operations may give larger errors, and, of course, error propagation must be considered when several operations are compounded. Additionally, rational numbers that are exactly representable as floating point numbers in base 10, like 0.1 or 0.7, do not have an exact representation as floating point numbers in base 2, which is used internally, no matter the size of the mantissa. Hence, they cannot be converted into their internal binary counterparts without a small loss of precision. This can lead to confusing results: for example, floor((0.1+0.7)*10) will usually return 7 instead of the expected 8, since the internal representation will be something like 7.9999999999999991118.... So never trust floating number results to the last digit, and do not compare floating point numbers directly for equality. If higher precision is necessary, the arbitrary precision math functions and gmp functions are available. For a \"simple\" explanation, see the \u00bb floating point guide that's also titled \"Why don\u2019t my numbers add up?\"","title":"Float"},{"location":"php/basics.html#string","text":"<?php $foo = 1 + \"10.5\"; // $foo is float (11.5) $foo = 1 + \"-1.3e3\"; // $foo is float (-1299) $foo = 1 + \"bob-1.3e3\"; // TypeError as of PHP 8.0.0, $foo is integer (1) previously $foo = 1 + \"bob3\"; // TypeError as of PHP 8.0.0, $foo is integer (1) previously $foo = 1 + \"10 Small Pigs\"; // $foo is integer (11) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously $foo = 4 + \"10.2 Little Piggies\"; // $foo is float (14.2) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously $foo = \"10.0 pigs \" + 1; // $foo is float (11) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously $foo = \"10.0 pigs \" + 1.0; // $foo is float (11) and an E_WARNING is raised in PHP 8.0.0, E_NOTICE previously ?>","title":"String"},{"location":"php/basics.html#arrays","text":"<?php $array = array( \"foo\" => \"bar\", \"bar\" => \"foo\", ); // Using the short array syntax $array = [ \"foo\" => \"bar\", \"bar\" => \"foo\", ]; ?>","title":"Arrays"},{"location":"php/basics.html#iterable","text":"<?php function foo(iterable $iterable) { foreach ($iterable as $value) { // ... } } ?>","title":"Iterable"},{"location":"php/basics.html#object","text":"<?php class foo { function do_foo() { echo \"Doing foo.\"; } } $bar = new foo; $bar->do_foo(); ?>","title":"Object"},{"location":"php/basics.html#reources","text":"get_resource_type()","title":"Reources"},{"location":"php/basics.html#callable","text":"// An example callback function function my_callback_function() { echo 'hello world!'; } // An example callback method class MyClass { static function myCallbackMethod() { echo 'Hello World!'; } } // Type 1: Simple callback call_user_func('my_callback_function'); // Type 2: Static class method call call_user_func(array('MyClass', 'myCallbackMethod')); // Type 3: Object method call $obj = new MyClass(); call_user_func(array($obj, 'myCallbackMethod')); // Type 4: Static class method call call_user_func('MyClass::myCallbackMethod'); // Type 5: Relative static class method call class A { public static function who() { echo \"A\\n\"; } } class B extends A { public static function who() { echo \"B\\n\"; } } call_user_func(array('B', 'parent::who')); // A // Type 6: Objects implementing __invoke can be used as callables class C { public function __invoke($name) { echo 'Hello ', $name, \"\\n\"; } } $c = new C(); call_user_func($c, 'PHP!'); ?>","title":"Callable"},{"location":"php/basics.html#type-juggling-and-declartion","text":"TODO","title":"Type Juggling and declartion"},{"location":"php/intro.html","text":"In computing, Common Gateway Interface (CGI) is an interface specification that enables web servers to execute an external program, typically to process user requests. PHP (recursive acronym for PHP: Hypertext Preprocessor) is a widely-used open source general-purpose scripting language that is especially suited for web development and can be embedded into HTML. <!DOCTYPE html> <html> <head> <title>Example</title> </head> <body> <?php echo \"Hi, I'm a PHP script!\"; ?> </body> </html> What it can do? \u00b6 server-side scripting Command line scripting Writing desktop applications","title":"Intro"},{"location":"php/intro.html#what-it-can-do","text":"server-side scripting Command line scripting Writing desktop applications","title":"What it can do?"},{"location":"php/security.html","text":"A system is only as good as the weakest link in a chain.","title":"Security"},{"location":"system-design/DRY.html","text":"Y","title":"DRY"},{"location":"system-design/SOLID.html","text":"","title":"SOLID"},{"location":"system-design/YAGMI.html","text":"","title":"YAGMI"},{"location":"system-design/intro.html","text":"https://www.yegor256.com/2016/02/03/design-patterns-and-anti-patterns.html","title":"Intro"},{"location":"system-design/behavioural/chain.html","text":"Chain of Responsibility \u00b6 lets you pass requests along a chain of handlers. Upon receiving a request, each handler decides either to process the request or to pass it to the next handler in the chain. When? \u00b6 Use the Chain of Responsibility pattern when your program is expected to process different kinds of requests in various ways, but the exact types of requests and their sequences are unknown beforehand. Use the pattern when it\u2019s essential to execute several handlers in a particular order. Use the CoR pattern when the set of handlers and their order are supposed to change at runtime. Pros Cons You can control the order of request handling. Some requests may end up unhandled. You avoid tight coupling between concrete products and client code. Single Responsibility Principle : You can decouple classes that invoke operations from classes that perform operations. Open/Closed Principle : You can introduce new handlers into the app without breaking the existing client code.","title":"Chain of Responsibility"},{"location":"system-design/behavioural/chain.html#chain-of-responsibility","text":"lets you pass requests along a chain of handlers. Upon receiving a request, each handler decides either to process the request or to pass it to the next handler in the chain.","title":"Chain of Responsibility"},{"location":"system-design/behavioural/chain.html#when","text":"Use the Chain of Responsibility pattern when your program is expected to process different kinds of requests in various ways, but the exact types of requests and their sequences are unknown beforehand. Use the pattern when it\u2019s essential to execute several handlers in a particular order. Use the CoR pattern when the set of handlers and their order are supposed to change at runtime. Pros Cons You can control the order of request handling. Some requests may end up unhandled. You avoid tight coupling between concrete products and client code. Single Responsibility Principle : You can decouple classes that invoke operations from classes that perform operations. Open/Closed Principle : You can introduce new handlers into the app without breaking the existing client code.","title":"When?"},{"location":"system-design/behavioural/command.html","text":"Command \u00b6 turns a request into a stand-alone object that contains all information about the request. This transformation lets you pass requests as a method arguments, delay or queue a request\u2019s execution, and support undoable operations. When? \u00b6 Use the Command pattern when you want to parametrize objects with operations. Use the Command pattern when you want to queue operations, schedule their execution, or execute them remotely. Use the Command pattern when you want to implement reversible operations. Pros Cons You can implement undo/redo. The code may become more complicated since you\u2019re introducing a whole new layer between senders and receivers. You can implement deferred execution of operations. You can assemble a set of simple commands into a complex one. Single Responsibility Principle : You can decouple classes that invoke operations from classes that perform these operations. Open/Closed Principle : You can introduce new commands into the app without breaking existing client code.","title":"Command"},{"location":"system-design/behavioural/command.html#command","text":"turns a request into a stand-alone object that contains all information about the request. This transformation lets you pass requests as a method arguments, delay or queue a request\u2019s execution, and support undoable operations.","title":"Command"},{"location":"system-design/behavioural/command.html#when","text":"Use the Command pattern when you want to parametrize objects with operations. Use the Command pattern when you want to queue operations, schedule their execution, or execute them remotely. Use the Command pattern when you want to implement reversible operations. Pros Cons You can implement undo/redo. The code may become more complicated since you\u2019re introducing a whole new layer between senders and receivers. You can implement deferred execution of operations. You can assemble a set of simple commands into a complex one. Single Responsibility Principle : You can decouple classes that invoke operations from classes that perform these operations. Open/Closed Principle : You can introduce new commands into the app without breaking existing client code.","title":"When?"},{"location":"system-design/behavioural/iterator.html","text":"Iterator \u00b6 lets you traverse elements of a collection without exposing its underlying representation (list, stack, tree, etc.). When? \u00b6 Use the Iterator pattern when your collection has a complex data structure under the hood, but you want to hide its complexity from clients (either for convenience or security reasons). Use the pattern to reduce duplication of the traversal code across your app. Use the Iterator when you want your code to be able to traverse different data structures or when types of these structures are unknown beforehand.","title":"Iterator"},{"location":"system-design/behavioural/iterator.html#iterator","text":"lets you traverse elements of a collection without exposing its underlying representation (list, stack, tree, etc.).","title":"Iterator"},{"location":"system-design/behavioural/iterator.html#when","text":"Use the Iterator pattern when your collection has a complex data structure under the hood, but you want to hide its complexity from clients (either for convenience or security reasons). Use the pattern to reduce duplication of the traversal code across your app. Use the Iterator when you want your code to be able to traverse different data structures or when types of these structures are unknown beforehand.","title":"When?"},{"location":"system-design/creation/abstract-factory.html","text":"Abstract Factory \u00b6 lets you produce families of related objects without specifying their concrete classes. When? \u00b6 Use the Abstract Factory when your code needs to work with various families of related products, but you don\u2019t want it to depend on the concrete classes of those products\u2014they might be unknown beforehand or you simply want to allow for future extensibility. Pros Cons You can be sure that the products you\u2019re getting from a factory are compatible with each other. The code may become more complicated than it should be, since a lot of new interfaces and classes are introduced along with the pattern. You avoid tight coupling between concrete products and client code. Single Responsibility Principle : You can extract the product creation code into one place, making the code easier to support. Open/Closed Principle : You can introduce new variants of products without breaking existing client code.","title":"Abstract Factory"},{"location":"system-design/creation/abstract-factory.html#abstract-factory","text":"lets you produce families of related objects without specifying their concrete classes.","title":"Abstract Factory"},{"location":"system-design/creation/abstract-factory.html#when","text":"Use the Abstract Factory when your code needs to work with various families of related products, but you don\u2019t want it to depend on the concrete classes of those products\u2014they might be unknown beforehand or you simply want to allow for future extensibility. Pros Cons You can be sure that the products you\u2019re getting from a factory are compatible with each other. The code may become more complicated than it should be, since a lot of new interfaces and classes are introduced along with the pattern. You avoid tight coupling between concrete products and client code. Single Responsibility Principle : You can extract the product creation code into one place, making the code easier to support. Open/Closed Principle : You can introduce new variants of products without breaking existing client code.","title":"When?"},{"location":"system-design/creation/builder.html","text":"Builder \u00b6 lets you construct complex objects step by step. The pattern allows you to produce different types and representations of an object using the same construction code. When? \u00b6 Use the Builder pattern to get rid of a \u201ctelescopic constructor\u201d. Use the Builder pattern when you want your code to be able to create different representations of some product (for example, stone and wooden houses). Use the Builder to construct Composite trees or other complex objects. Pros Cons You can construct objects step-by-step, defer construction steps or run steps recursively. The overall complexity of the code increases since the pattern requires creating multiple new classes. You can reuse the same construction code when building various representations of products. Single Responsibility Principle. You can isolate complex construction code from the business logic of the product.","title":"Builder"},{"location":"system-design/creation/builder.html#builder","text":"lets you construct complex objects step by step. The pattern allows you to produce different types and representations of an object using the same construction code.","title":"Builder"},{"location":"system-design/creation/builder.html#when","text":"Use the Builder pattern to get rid of a \u201ctelescopic constructor\u201d. Use the Builder pattern when you want your code to be able to create different representations of some product (for example, stone and wooden houses). Use the Builder to construct Composite trees or other complex objects. Pros Cons You can construct objects step-by-step, defer construction steps or run steps recursively. The overall complexity of the code increases since the pattern requires creating multiple new classes. You can reuse the same construction code when building various representations of products. Single Responsibility Principle. You can isolate complex construction code from the business logic of the product.","title":"When?"},{"location":"system-design/creation/factory.html","text":"Factory method \u00b6 Virtual Constructor is a creational design pattern that provides an interface for creating objects in a superclass, but allows subclasses to alter the type of objects that will be created. When? \u00b6 Use the Factory Method when you don\u2019t know beforehand the exact types and dependencies of the objects your code should work with. Use the Factory Method when you want to provide users of your library or framework with a way to extend its internal components. Use the Factory Method when you want to save system resources by reusing existing objects instead of rebuilding them each time. Pros Cons You avoid tight coupling between the creator and the concrete products. The code may become more complicated since you need to introduce a lot of new subclasses to implement the pattern. The best case scenario is when you\u2019re introducing the pattern into an existing hierarchy of creator classes. Single Responsibility Principle : You can move the product creation code into one place in the program, making the code easier to support. Open/Closed Principle : You can introduce new types of products into the program without breaking existing client code.","title":"Factory method"},{"location":"system-design/creation/factory.html#factory-method","text":"Virtual Constructor is a creational design pattern that provides an interface for creating objects in a superclass, but allows subclasses to alter the type of objects that will be created.","title":"Factory method"},{"location":"system-design/creation/factory.html#when","text":"Use the Factory Method when you don\u2019t know beforehand the exact types and dependencies of the objects your code should work with. Use the Factory Method when you want to provide users of your library or framework with a way to extend its internal components. Use the Factory Method when you want to save system resources by reusing existing objects instead of rebuilding them each time. Pros Cons You avoid tight coupling between the creator and the concrete products. The code may become more complicated since you need to introduce a lot of new subclasses to implement the pattern. The best case scenario is when you\u2019re introducing the pattern into an existing hierarchy of creator classes. Single Responsibility Principle : You can move the product creation code into one place in the program, making the code easier to support. Open/Closed Principle : You can introduce new types of products into the program without breaking existing client code.","title":"When?"},{"location":"system-design/creation/prototype.html","text":"Prototype \u00b6 lets you copy existing objects without making your code dependent on their classes. When? \u00b6 Use the Prototype pattern when your code shouldn\u2019t depend on the concrete classes of objects that you need to copy. Use the pattern when you want to reduce the number of subclasses that only differ in the way they initialize their respective objects. Somebody could have created these subclasses to be able to create objects with a specific configuration. Pros Cons You can clone objects without coupling to their concrete classes. Cloning complex objects that have circular references might be very tricky. You can get rid of repeated initialization code in favor of cloning pre-built prototypes. You can produce complex objects more conveniently. You get an alternative to inheritance when dealing with configuration presets for complex objects.","title":"Prototype"},{"location":"system-design/creation/prototype.html#prototype","text":"lets you copy existing objects without making your code dependent on their classes.","title":"Prototype"},{"location":"system-design/creation/prototype.html#when","text":"Use the Prototype pattern when your code shouldn\u2019t depend on the concrete classes of objects that you need to copy. Use the pattern when you want to reduce the number of subclasses that only differ in the way they initialize their respective objects. Somebody could have created these subclasses to be able to create objects with a specific configuration. Pros Cons You can clone objects without coupling to their concrete classes. Cloning complex objects that have circular references might be very tricky. You can get rid of repeated initialization code in favor of cloning pre-built prototypes. You can produce complex objects more conveniently. You get an alternative to inheritance when dealing with configuration presets for complex objects.","title":"When?"},{"location":"system-design/creation/singleton.html","text":"Singleton \u00b6 lets you ensure that a class has only one instance, while providing a global access point to this instance. When? \u00b6 Use the Singleton pattern when a class in your program should have just a single instance available to all clients; for example, a single database object shared by different parts of the program. Use the Singleton pattern when you need stricter control over global variables. Pros Cons You can be sure that a class has only a single instance. Violates the Single Responsibility Principle. The pattern solves two problems at the time. You gain a global access point to that instance. The Singleton pattern can mask bad design, for instance, when the components of the program know too much about each other. The singleton object is initialized only when it\u2019s requested for the first time. The pattern requires special treatment in a multithreaded environment so that multiple threads won\u2019t create a singleton object several times. Open/Closed Principle : You can introduce new variants of products without breaking existing client code. It may be difficult to unit test the client code of the Singleton because many test frameworks rely on inheritance when producing mock objects. Since the constructor of the singleton class is private and overriding static methods is impossible in most languages, you will need to think of a creative way to mock the singleton. Or just don\u2019t write the tests. Or don\u2019t use the Singleton pattern.","title":"Singleton"},{"location":"system-design/creation/singleton.html#singleton","text":"lets you ensure that a class has only one instance, while providing a global access point to this instance.","title":"Singleton"},{"location":"system-design/creation/singleton.html#when","text":"Use the Singleton pattern when a class in your program should have just a single instance available to all clients; for example, a single database object shared by different parts of the program. Use the Singleton pattern when you need stricter control over global variables. Pros Cons You can be sure that a class has only a single instance. Violates the Single Responsibility Principle. The pattern solves two problems at the time. You gain a global access point to that instance. The Singleton pattern can mask bad design, for instance, when the components of the program know too much about each other. The singleton object is initialized only when it\u2019s requested for the first time. The pattern requires special treatment in a multithreaded environment so that multiple threads won\u2019t create a singleton object several times. Open/Closed Principle : You can introduce new variants of products without breaking existing client code. It may be difficult to unit test the client code of the Singleton because many test frameworks rely on inheritance when producing mock objects. Since the constructor of the singleton class is private and overriding static methods is impossible in most languages, you will need to think of a creative way to mock the singleton. Or just don\u2019t write the tests. Or don\u2019t use the Singleton pattern.","title":"When?"},{"location":"system-design/structural/adapter.html","text":"Adapter \u00b6 Allows objects with incompatible interfaces to collaborate. When? \u00b6 Use the Adapter class when you want to use some existing class, but its interface isn\u2019t compatible with the rest of your code. Use the pattern when you want to reuse several existing subclasses that lack some common functionality that can\u2019t be added to the superclass. Pros Cons Single Responsibility Principle : You can separate the interface or data conversion code from the primary business logic of the program. The overall complexity of the code increases because you need to introduce a set of new interfaces and classes. Sometimes it\u2019s simpler just to change the service class so that it matches the rest of your code. Open/Closed Principle : You can introduce new types of adapters into the program without breaking the existing client code, as long as they work with the adapters through the client interface.","title":"Adapter"},{"location":"system-design/structural/adapter.html#adapter","text":"Allows objects with incompatible interfaces to collaborate.","title":"Adapter"},{"location":"system-design/structural/adapter.html#when","text":"Use the Adapter class when you want to use some existing class, but its interface isn\u2019t compatible with the rest of your code. Use the pattern when you want to reuse several existing subclasses that lack some common functionality that can\u2019t be added to the superclass. Pros Cons Single Responsibility Principle : You can separate the interface or data conversion code from the primary business logic of the program. The overall complexity of the code increases because you need to introduce a set of new interfaces and classes. Sometimes it\u2019s simpler just to change the service class so that it matches the rest of your code. Open/Closed Principle : You can introduce new types of adapters into the program without breaking the existing client code, as long as they work with the adapters through the client interface.","title":"When?"},{"location":"system-design/structural/bridge.html","text":"Bridge \u00b6 lets you split a large class or a set of closely related classes into two separate hierarchies\u2014abstraction and implementation\u2014which can be developed independently of each other. When? \u00b6 Use the Bridge pattern when you want to divide and organize a monolithic class that has several variants of some functionality (for example, if the class can work with various database servers). Use the pattern when you need to extend a class in several orthogonal (independent) dimensions. Use the Bridge if you need to be able to switch implementations at runtime. Pros Cons You can create platform-independent classes and apps. You might make the code more complicated by applying the pattern to a highly cohesive class. The client code works with high-level abstractions. It isn\u2019t exposed to the platform details. Single Responsibility Principle : You can introduce new abstractions and implementations independently from each other. Open/Closed Principle : You can focus on high-level logic in the abstraction and on platform details in the implementation.","title":"Bridge"},{"location":"system-design/structural/bridge.html#bridge","text":"lets you split a large class or a set of closely related classes into two separate hierarchies\u2014abstraction and implementation\u2014which can be developed independently of each other.","title":"Bridge"},{"location":"system-design/structural/bridge.html#when","text":"Use the Bridge pattern when you want to divide and organize a monolithic class that has several variants of some functionality (for example, if the class can work with various database servers). Use the pattern when you need to extend a class in several orthogonal (independent) dimensions. Use the Bridge if you need to be able to switch implementations at runtime. Pros Cons You can create platform-independent classes and apps. You might make the code more complicated by applying the pattern to a highly cohesive class. The client code works with high-level abstractions. It isn\u2019t exposed to the platform details. Single Responsibility Principle : You can introduce new abstractions and implementations independently from each other. Open/Closed Principle : You can focus on high-level logic in the abstraction and on platform details in the implementation.","title":"When?"},{"location":"system-design/structural/composite.html","text":"Composite \u00b6 lets you compose objects into tree structures and then work with these structures as if they were individual objects. When \u00b6 Use the Composite pattern when you have to implement a tree-like object structure. Use the pattern when you want the client code to treat both simple and complex elements uniformly. Pros Cons You can work with complex tree structures more conveniently: use polymorphism and recursion to your advantage. It might be difficult to provide a common interface for classes whose functionality differs too much. In certain scenarios, you\u2019d need to overgeneralize the component interface, making it harder to comprehend. Open/Closed Principle : You can introduce new element types into the app without breaking the existing code, which now works with the object tree.","title":"Composite"},{"location":"system-design/structural/composite.html#composite","text":"lets you compose objects into tree structures and then work with these structures as if they were individual objects.","title":"Composite"},{"location":"system-design/structural/composite.html#when","text":"Use the Composite pattern when you have to implement a tree-like object structure. Use the pattern when you want the client code to treat both simple and complex elements uniformly. Pros Cons You can work with complex tree structures more conveniently: use polymorphism and recursion to your advantage. It might be difficult to provide a common interface for classes whose functionality differs too much. In certain scenarios, you\u2019d need to overgeneralize the component interface, making it harder to comprehend. Open/Closed Principle : You can introduce new element types into the app without breaking the existing code, which now works with the object tree.","title":"When"},{"location":"system-design/structural/decorator.html","text":"Decorator \u00b6 lets you attach new behaviors to objects by placing these objects inside special wrapper objects that contain the behaviors. Why? \u00b6 Use the Decorator pattern when you need to be able to assign extra behaviors to objects at runtime without breaking the code that uses these objects. Use the pattern when it\u2019s awkward or not possible to extend an object\u2019s behavior using inheritance. Pros Cons You can extend an object\u2019s behavior without making a new subclass. It\u2019s hard to remove a specific wrapper from the wrappers stack. You can add or remove responsibilities from an object at runtime. It\u2019s hard to implement a decorator in such a way that its behavior doesn\u2019t depend on the order in the decorators stack. You can combine several behaviors by wrapping an object into multiple decorators. The initial configuration code of layers might look pretty ugly. Single Responsibility Principle : You can divide a monolithic class that implements many possible variants of behavior into several smaller classes.","title":"Decorator"},{"location":"system-design/structural/decorator.html#decorator","text":"lets you attach new behaviors to objects by placing these objects inside special wrapper objects that contain the behaviors.","title":"Decorator"},{"location":"system-design/structural/decorator.html#why","text":"Use the Decorator pattern when you need to be able to assign extra behaviors to objects at runtime without breaking the code that uses these objects. Use the pattern when it\u2019s awkward or not possible to extend an object\u2019s behavior using inheritance. Pros Cons You can extend an object\u2019s behavior without making a new subclass. It\u2019s hard to remove a specific wrapper from the wrappers stack. You can add or remove responsibilities from an object at runtime. It\u2019s hard to implement a decorator in such a way that its behavior doesn\u2019t depend on the order in the decorators stack. You can combine several behaviors by wrapping an object into multiple decorators. The initial configuration code of layers might look pretty ugly. Single Responsibility Principle : You can divide a monolithic class that implements many possible variants of behavior into several smaller classes.","title":"Why?"},{"location":"system-design/structural/facade.html","text":"Facade \u00b6 Provides a simplified interface to a library, a framework, or any other complex set of classes. When? \u00b6 Use the Facade pattern when you need to have a limited but straightforward interface to a complex subsystem. Use the Facade when you want to structure a subsystem into layers. Pros Cons You can isolate your code from the complexity of a subsystem. A facade can become a god object coupled to all classes of an app.","title":"Facade"},{"location":"system-design/structural/facade.html#facade","text":"Provides a simplified interface to a library, a framework, or any other complex set of classes.","title":"Facade"},{"location":"system-design/structural/facade.html#when","text":"Use the Facade pattern when you need to have a limited but straightforward interface to a complex subsystem. Use the Facade when you want to structure a subsystem into layers. Pros Cons You can isolate your code from the complexity of a subsystem. A facade can become a god object coupled to all classes of an app.","title":"When?"},{"location":"system-design/structural/flyweight.html","text":"FlyWeight \u00b6 lets you fit more objects into the available amount of RAM by sharing common parts of state between multiple objects instead of keeping all of the data in each object. When? \u00b6 Use the Flyweight pattern only when your program must support a huge number of objects which barely fit into available RAM. Pros Cons You can save lots of RAM, assuming your program has tons of similar objects. You might be trading RAM over CPU cycles when some of the context data needs to be recalculated each time somebody calls a flyweight method. The code becomes much more complicated. New team members will always be wondering why the state of an entity was separated in such a way.","title":"FlyWeight"},{"location":"system-design/structural/flyweight.html#flyweight","text":"lets you fit more objects into the available amount of RAM by sharing common parts of state between multiple objects instead of keeping all of the data in each object.","title":"FlyWeight"},{"location":"system-design/structural/flyweight.html#when","text":"Use the Flyweight pattern only when your program must support a huge number of objects which barely fit into available RAM. Pros Cons You can save lots of RAM, assuming your program has tons of similar objects. You might be trading RAM over CPU cycles when some of the context data needs to be recalculated each time somebody calls a flyweight method. The code becomes much more complicated. New team members will always be wondering why the state of an entity was separated in such a way.","title":"When?"},{"location":"system-design/structural/proxy.html","text":"Proxy \u00b6 lets you provide a substitute or placeholder for another object. A proxy controls access to the original object, allowing you to perform something either before or after the request gets through to the original object. When? \u00b6 Lazy initialization (virtual proxy). This is when you have a heavyweight service object that wastes system resources by being always up, even though you only need it from time to time. Access control (protection proxy). This is when you want only specific clients to be able to use the service object; for instance, when your objects are crucial parts of an operating system and clients are various launched applications (including malicious ones). Local execution of a remote service (remote proxy). This is when the service object is located on a remote server. Logging requests (logging proxy). This is when you want to keep a history of requests to the service object. Caching request results (caching proxy). This is when you need to cache results of client requests and manage the life cycle of this cache, especially if results are quite large. Smart reference. This is when you need to be able to dismiss a heavyweight object once there are no clients that use it. Pros Cons You can control the service object without clients knowing about it. The code may become more complicated since you need to introduce a lot of new classes. You can manage the lifecycle of the service object when clients don\u2019t care about it. The response from the service might get delayed. The proxy works even if the service object isn\u2019t ready or is not available. Open/Closed Principle : You can introduce new proxies without changing the service or clients.","title":"Proxy"},{"location":"system-design/structural/proxy.html#proxy","text":"lets you provide a substitute or placeholder for another object. A proxy controls access to the original object, allowing you to perform something either before or after the request gets through to the original object.","title":"Proxy"},{"location":"system-design/structural/proxy.html#when","text":"Lazy initialization (virtual proxy). This is when you have a heavyweight service object that wastes system resources by being always up, even though you only need it from time to time. Access control (protection proxy). This is when you want only specific clients to be able to use the service object; for instance, when your objects are crucial parts of an operating system and clients are various launched applications (including malicious ones). Local execution of a remote service (remote proxy). This is when the service object is located on a remote server. Logging requests (logging proxy). This is when you want to keep a history of requests to the service object. Caching request results (caching proxy). This is when you need to cache results of client requests and manage the life cycle of this cache, especially if results are quite large. Smart reference. This is when you need to be able to dismiss a heavyweight object once there are no clients that use it. Pros Cons You can control the service object without clients knowing about it. The code may become more complicated since you need to introduce a lot of new classes. You can manage the lifecycle of the service object when clients don\u2019t care about it. The response from the service might get delayed. The proxy works even if the service object isn\u2019t ready or is not available. Open/Closed Principle : You can introduce new proxies without changing the service or clients.","title":"When?"},{"location":"toc/automata.html","text":"Automata Theory \u00b6 A study of * The theory of formal languages * The mathematical modeling of systems * The computability of a problem/function * What is a computer Alphabets (\u03a3) \u00b6 A finite set of symbols. Strings \u00b6 A finite sequence of symbols from an alphabet. Empty string: \u03b5 Concatination: \u03c9=abd, \u03b1=ce, \u03c9\u03b1=abdce Exponentiation: \u03c9=abd, \u03c9^3=abdabdabd, \u03c9^0=\u03b5 Reversal: \u03c9=abd, \u03c9^R = dba \u03a3^k = set of all k-length strings formed by the symbols in \u03a3 \u03a3^1 != \u03a3 Kleene Closure: \u03a3* = \u03a3^0 \u222a \u03a3^1 \u222a \u03a3^2 \u222a\u2026 = \u222a^k\u22650 \u03a3^k \u03a3+ = \u03a3* - \u03b5 Languages \u00b6 A set of strings over an alphabet. \u03c6 != {\u03b5}. Finite Automata / Finite State Machines \u00b6 Deterministic Finite Automata (Q, \u03a3, \u03b4, q0, F) Q: finite set of states \u03a3:finite input alphabet \u03b4: transition function (Q \u00d7 \u03a3 -> Q) q0: initial state (only 1) F: set of final states (0 or more) one transition only for each input symbol from each state. Transition Table \u00b6 Transition Diagram \u00b6 Languages Accepted by DFAs \u00b6 The language contains L(M) all input strings accepted by L. There exist languages which are not Regular: There is no DFA that accepts such a language. Nondeterministic Finite Accepter (NFA) \u00b6 The lambda symbol never appears on the input tape NFAs are interesting because we can express languages easier than DFAs NFAs accept the Regular Languages Equivalence \u00b6 M1 = M2 iff L(M1) = L(M2) NFA to DFA \u00b6","title":"Automata Theory"},{"location":"toc/automata.html#automata-theory","text":"A study of * The theory of formal languages * The mathematical modeling of systems * The computability of a problem/function * What is a computer","title":"Automata Theory"},{"location":"toc/automata.html#alphabets","text":"A finite set of symbols.","title":"Alphabets (\u03a3)"},{"location":"toc/automata.html#strings","text":"A finite sequence of symbols from an alphabet. Empty string: \u03b5 Concatination: \u03c9=abd, \u03b1=ce, \u03c9\u03b1=abdce Exponentiation: \u03c9=abd, \u03c9^3=abdabdabd, \u03c9^0=\u03b5 Reversal: \u03c9=abd, \u03c9^R = dba \u03a3^k = set of all k-length strings formed by the symbols in \u03a3 \u03a3^1 != \u03a3 Kleene Closure: \u03a3* = \u03a3^0 \u222a \u03a3^1 \u222a \u03a3^2 \u222a\u2026 = \u222a^k\u22650 \u03a3^k \u03a3+ = \u03a3* - \u03b5","title":"Strings"},{"location":"toc/automata.html#languages","text":"A set of strings over an alphabet. \u03c6 != {\u03b5}.","title":"Languages"},{"location":"toc/automata.html#finite-automata-finite-state-machines","text":"Deterministic Finite Automata (Q, \u03a3, \u03b4, q0, F) Q: finite set of states \u03a3:finite input alphabet \u03b4: transition function (Q \u00d7 \u03a3 -> Q) q0: initial state (only 1) F: set of final states (0 or more) one transition only for each input symbol from each state.","title":"Finite Automata / Finite State Machines"},{"location":"toc/automata.html#transition-table","text":"","title":"Transition Table"},{"location":"toc/automata.html#transition-diagram","text":"","title":"Transition Diagram"},{"location":"toc/automata.html#languages-accepted-by-dfas","text":"The language contains L(M) all input strings accepted by L. There exist languages which are not Regular: There is no DFA that accepts such a language.","title":"Languages Accepted by DFAs"},{"location":"toc/automata.html#nondeterministic-finite-accepter-nfa","text":"The lambda symbol never appears on the input tape NFAs are interesting because we can express languages easier than DFAs NFAs accept the Regular Languages","title":"Nondeterministic Finite Accepter (NFA)"},{"location":"toc/automata.html#equivalence","text":"M1 = M2 iff L(M1) = L(M2)","title":"Equivalence"},{"location":"toc/automata.html#nfa-to-dfa","text":"","title":"NFA to DFA"}]}